{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f36439",
   "metadata": {},
   "source": [
    "# Training the model using image embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637789b1",
   "metadata": {},
   "source": [
    "### Reading the spectrogram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72cadd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_data = pd.read_csv(r'spectrogram_features_google_vite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04ca23d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display max column width\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b1ca411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_758</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/3P66A03R2_spectrogram.png</td>\n",
       "      <td>-0.048340</td>\n",
       "      <td>0.055817</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>-0.373291</td>\n",
       "      <td>-0.008018</td>\n",
       "      <td>-0.236084</td>\n",
       "      <td>0.068909</td>\n",
       "      <td>0.140747</td>\n",
       "      <td>-0.217529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162354</td>\n",
       "      <td>0.164551</td>\n",
       "      <td>-0.133057</td>\n",
       "      <td>-0.170166</td>\n",
       "      <td>-0.205322</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>-0.059418</td>\n",
       "      <td>-0.201294</td>\n",
       "      <td>0.055756</td>\n",
       "      <td>-0.131348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/1P68A01R1_spectrogram.png</td>\n",
       "      <td>0.034607</td>\n",
       "      <td>-0.034241</td>\n",
       "      <td>-0.101257</td>\n",
       "      <td>-0.314209</td>\n",
       "      <td>-0.255371</td>\n",
       "      <td>-0.125732</td>\n",
       "      <td>0.016068</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.256104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.202759</td>\n",
       "      <td>0.092041</td>\n",
       "      <td>-0.255371</td>\n",
       "      <td>0.031555</td>\n",
       "      <td>0.203857</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>-0.272217</td>\n",
       "      <td>0.088928</td>\n",
       "      <td>-0.055908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/5P60A05R1_spectrogram.png</td>\n",
       "      <td>-0.086121</td>\n",
       "      <td>0.089172</td>\n",
       "      <td>-0.174683</td>\n",
       "      <td>-0.426758</td>\n",
       "      <td>-0.082092</td>\n",
       "      <td>-0.114990</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.198975</td>\n",
       "      <td>-0.243286</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085144</td>\n",
       "      <td>0.154419</td>\n",
       "      <td>-0.027039</td>\n",
       "      <td>-0.070984</td>\n",
       "      <td>-0.189209</td>\n",
       "      <td>0.252930</td>\n",
       "      <td>-0.048798</td>\n",
       "      <td>-0.265381</td>\n",
       "      <td>0.128418</td>\n",
       "      <td>-0.162598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/3P65A03R3_spectrogram.png</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>0.034851</td>\n",
       "      <td>-0.234253</td>\n",
       "      <td>-0.427246</td>\n",
       "      <td>-0.048126</td>\n",
       "      <td>-0.329834</td>\n",
       "      <td>0.031830</td>\n",
       "      <td>0.111877</td>\n",
       "      <td>-0.276123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143921</td>\n",
       "      <td>0.160767</td>\n",
       "      <td>-0.096436</td>\n",
       "      <td>-0.269531</td>\n",
       "      <td>-0.185791</td>\n",
       "      <td>0.252930</td>\n",
       "      <td>-0.139282</td>\n",
       "      <td>-0.265625</td>\n",
       "      <td>0.129761</td>\n",
       "      <td>-0.154175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/1P62A01R2_spectrogram.png</td>\n",
       "      <td>0.058044</td>\n",
       "      <td>-0.131958</td>\n",
       "      <td>-0.123047</td>\n",
       "      <td>-0.252197</td>\n",
       "      <td>-0.213013</td>\n",
       "      <td>-0.140991</td>\n",
       "      <td>0.030991</td>\n",
       "      <td>-0.042145</td>\n",
       "      <td>-0.204224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011017</td>\n",
       "      <td>0.183838</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>-0.236328</td>\n",
       "      <td>-0.012367</td>\n",
       "      <td>0.303711</td>\n",
       "      <td>-0.006874</td>\n",
       "      <td>-0.223999</td>\n",
       "      <td>0.184082</td>\n",
       "      <td>-0.125366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/6P56A06R02_spectrogram.png</td>\n",
       "      <td>-0.045807</td>\n",
       "      <td>-0.061523</td>\n",
       "      <td>-0.045197</td>\n",
       "      <td>-0.329102</td>\n",
       "      <td>-0.037079</td>\n",
       "      <td>-0.083801</td>\n",
       "      <td>-0.027954</td>\n",
       "      <td>0.187988</td>\n",
       "      <td>-0.169800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062195</td>\n",
       "      <td>0.154175</td>\n",
       "      <td>0.066467</td>\n",
       "      <td>-0.085999</td>\n",
       "      <td>-0.137573</td>\n",
       "      <td>0.116211</td>\n",
       "      <td>0.149658</td>\n",
       "      <td>-0.290771</td>\n",
       "      <td>0.050995</td>\n",
       "      <td>-0.086914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/1P42A01R03_spectrogram.png</td>\n",
       "      <td>0.025467</td>\n",
       "      <td>-0.013214</td>\n",
       "      <td>-0.195801</td>\n",
       "      <td>-0.453125</td>\n",
       "      <td>-0.275391</td>\n",
       "      <td>-0.149902</td>\n",
       "      <td>-0.021835</td>\n",
       "      <td>0.052521</td>\n",
       "      <td>-0.262939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060883</td>\n",
       "      <td>0.209473</td>\n",
       "      <td>-0.049927</td>\n",
       "      <td>-0.178589</td>\n",
       "      <td>-0.123962</td>\n",
       "      <td>0.248047</td>\n",
       "      <td>-0.003431</td>\n",
       "      <td>-0.199829</td>\n",
       "      <td>0.122925</td>\n",
       "      <td>-0.065186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/6P40A06R03_spectrogram.png</td>\n",
       "      <td>-0.044037</td>\n",
       "      <td>-0.016708</td>\n",
       "      <td>-0.088562</td>\n",
       "      <td>-0.362061</td>\n",
       "      <td>-0.119751</td>\n",
       "      <td>-0.167114</td>\n",
       "      <td>-0.026672</td>\n",
       "      <td>0.211792</td>\n",
       "      <td>-0.144775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124756</td>\n",
       "      <td>0.161621</td>\n",
       "      <td>0.080872</td>\n",
       "      <td>-0.367676</td>\n",
       "      <td>-0.092163</td>\n",
       "      <td>0.234619</td>\n",
       "      <td>0.042145</td>\n",
       "      <td>-0.232544</td>\n",
       "      <td>0.061737</td>\n",
       "      <td>-0.078857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/2P56A02R01_spectrogram.png</td>\n",
       "      <td>0.064026</td>\n",
       "      <td>-0.040649</td>\n",
       "      <td>-0.171021</td>\n",
       "      <td>-0.335693</td>\n",
       "      <td>-0.171387</td>\n",
       "      <td>-0.112244</td>\n",
       "      <td>-0.003649</td>\n",
       "      <td>0.106323</td>\n",
       "      <td>-0.143188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133911</td>\n",
       "      <td>0.197388</td>\n",
       "      <td>-0.149780</td>\n",
       "      <td>-0.167725</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.029999</td>\n",
       "      <td>0.056335</td>\n",
       "      <td>-0.224609</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>-0.127808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>spectrograms/Sample_data_preprocessing+label_extraction/5P56A05R3_spectrogram.png</td>\n",
       "      <td>0.089233</td>\n",
       "      <td>0.015251</td>\n",
       "      <td>-0.126831</td>\n",
       "      <td>-0.284668</td>\n",
       "      <td>-0.037140</td>\n",
       "      <td>-0.150513</td>\n",
       "      <td>0.018188</td>\n",
       "      <td>0.124084</td>\n",
       "      <td>-0.143188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040924</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>0.044037</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.096985</td>\n",
       "      <td>0.154175</td>\n",
       "      <td>0.028397</td>\n",
       "      <td>-0.226929</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>-0.094910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1754 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             image_path  \\\n",
       "0                            spectrograms/4 July 2018 Dataset/3P66A03R2_spectrogram.png   \n",
       "1                            spectrograms/4 July 2018 Dataset/1P68A01R1_spectrogram.png   \n",
       "2                            spectrograms/4 July 2018 Dataset/5P60A05R1_spectrogram.png   \n",
       "3                            spectrograms/4 July 2018 Dataset/3P65A03R3_spectrogram.png   \n",
       "4                            spectrograms/4 July 2018 Dataset/1P62A01R2_spectrogram.png   \n",
       "...                                                                                 ...   \n",
       "1749                    spectrograms/1 December 2017 Dataset/6P56A06R02_spectrogram.png   \n",
       "1750                    spectrograms/1 December 2017 Dataset/1P42A01R03_spectrogram.png   \n",
       "1751                    spectrograms/1 December 2017 Dataset/6P40A06R03_spectrogram.png   \n",
       "1752                    spectrograms/1 December 2017 Dataset/2P56A02R01_spectrogram.png   \n",
       "1753  spectrograms/Sample_data_preprocessing+label_extraction/5P56A05R3_spectrogram.png   \n",
       "\n",
       "      feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0     -0.048340   0.055817  -0.145508  -0.373291  -0.008018  -0.236084   \n",
       "1      0.034607  -0.034241  -0.101257  -0.314209  -0.255371  -0.125732   \n",
       "2     -0.086121   0.089172  -0.174683  -0.426758  -0.082092  -0.114990   \n",
       "3     -0.034821   0.034851  -0.234253  -0.427246  -0.048126  -0.329834   \n",
       "4      0.058044  -0.131958  -0.123047  -0.252197  -0.213013  -0.140991   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1749  -0.045807  -0.061523  -0.045197  -0.329102  -0.037079  -0.083801   \n",
       "1750   0.025467  -0.013214  -0.195801  -0.453125  -0.275391  -0.149902   \n",
       "1751  -0.044037  -0.016708  -0.088562  -0.362061  -0.119751  -0.167114   \n",
       "1752   0.064026  -0.040649  -0.171021  -0.335693  -0.171387  -0.112244   \n",
       "1753   0.089233   0.015251  -0.126831  -0.284668  -0.037140  -0.150513   \n",
       "\n",
       "      feature_6  feature_7  feature_8  ...  feature_758  feature_759  \\\n",
       "0      0.068909   0.140747  -0.217529  ...    -0.162354     0.164551   \n",
       "1      0.016068  -0.000095  -0.256104  ...     0.003653     0.202759   \n",
       "2     -0.052063   0.198975  -0.243286  ...    -0.085144     0.154419   \n",
       "3      0.031830   0.111877  -0.276123  ...    -0.143921     0.160767   \n",
       "4      0.030991  -0.042145  -0.204224  ...    -0.011017     0.183838   \n",
       "...         ...        ...        ...  ...          ...          ...   \n",
       "1749  -0.027954   0.187988  -0.169800  ...    -0.062195     0.154175   \n",
       "1750  -0.021835   0.052521  -0.262939  ...     0.060883     0.209473   \n",
       "1751  -0.026672   0.211792  -0.144775  ...    -0.124756     0.161621   \n",
       "1752  -0.003649   0.106323  -0.143188  ...    -0.133911     0.197388   \n",
       "1753   0.018188   0.124084  -0.143188  ...    -0.040924     0.198242   \n",
       "\n",
       "      feature_760  feature_761  feature_762  feature_763  feature_764  \\\n",
       "0       -0.133057    -0.170166    -0.205322     0.122070    -0.059418   \n",
       "1        0.092041    -0.255371     0.031555     0.203857     0.094727   \n",
       "2       -0.027039    -0.070984    -0.189209     0.252930    -0.048798   \n",
       "3       -0.096436    -0.269531    -0.185791     0.252930    -0.139282   \n",
       "4        0.079102    -0.236328    -0.012367     0.303711    -0.006874   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1749     0.066467    -0.085999    -0.137573     0.116211     0.149658   \n",
       "1750    -0.049927    -0.178589    -0.123962     0.248047    -0.003431   \n",
       "1751     0.080872    -0.367676    -0.092163     0.234619     0.042145   \n",
       "1752    -0.149780    -0.167725    -0.093750     0.029999     0.056335   \n",
       "1753     0.044037    -0.128174    -0.096985     0.154175     0.028397   \n",
       "\n",
       "      feature_765  feature_766  feature_767  \n",
       "0       -0.201294     0.055756    -0.131348  \n",
       "1       -0.272217     0.088928    -0.055908  \n",
       "2       -0.265381     0.128418    -0.162598  \n",
       "3       -0.265625     0.129761    -0.154175  \n",
       "4       -0.223999     0.184082    -0.125366  \n",
       "...           ...          ...          ...  \n",
       "1749    -0.290771     0.050995    -0.086914  \n",
       "1750    -0.199829     0.122925    -0.065186  \n",
       "1751    -0.232544     0.061737    -0.078857  \n",
       "1752    -0.224609     0.089600    -0.127808  \n",
       "1753    -0.226929     0.014641    -0.094910  \n",
       "\n",
       "[1754 rows x 769 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c013e",
   "metadata": {},
   "source": [
    "# Extracting the labels of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8a9ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   image_path  label\n",
      "0  spectrograms/4 July 2018 Dataset/3P66A03R2_spectrogram.png      3\n",
      "1  spectrograms/4 July 2018 Dataset/1P68A01R1_spectrogram.png      1\n",
      "2  spectrograms/4 July 2018 Dataset/5P60A05R1_spectrogram.png      5\n",
      "3  spectrograms/4 July 2018 Dataset/3P65A03R3_spectrogram.png      3\n",
      "4  spectrograms/4 July 2018 Dataset/1P62A01R2_spectrogram.png      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Assuming 'df' is your pandas DataFrame\n",
    "# For example:\n",
    "# df = pd.DataFrame({'image_path': ['/path/to/1P24A01R2_spectrogram.png']})\n",
    "\n",
    "# If already loaded:\n",
    "df = all_data.copy()\n",
    "\n",
    "# Function to extract the first number from the filename only\n",
    "def extract_first_number_from_filename(path):\n",
    "    filename = os.path.basename(path)  # Extracts just the filename\n",
    "    match = re.search(r'(\\d+)', filename)  # Looks for the first digit sequence\n",
    "    if match:\n",
    "        return int(match.group(1))  # Returns the number as an integer\n",
    "    return None  # If no number is found\n",
    "\n",
    "# Apply the function to the 'image_path' column and create the 'label' column\n",
    "df['label'] = df['image_path'].apply(extract_first_number_from_filename)\n",
    "\n",
    "# Display the DataFrame with the new 'label' column\n",
    "print(df[['image_path', 'label']].head())\n",
    "\n",
    "# To see the full DataFrame with the new column:\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cc1c855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/3P66A03R2_spectrogram.png</td>\n",
       "      <td>-0.048340</td>\n",
       "      <td>0.055817</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>-0.373291</td>\n",
       "      <td>-0.008018</td>\n",
       "      <td>-0.236084</td>\n",
       "      <td>0.068909</td>\n",
       "      <td>0.140747</td>\n",
       "      <td>-0.217529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164551</td>\n",
       "      <td>-0.133057</td>\n",
       "      <td>-0.170166</td>\n",
       "      <td>-0.205322</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>-0.059418</td>\n",
       "      <td>-0.201294</td>\n",
       "      <td>0.055756</td>\n",
       "      <td>-0.131348</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/1P68A01R1_spectrogram.png</td>\n",
       "      <td>0.034607</td>\n",
       "      <td>-0.034241</td>\n",
       "      <td>-0.101257</td>\n",
       "      <td>-0.314209</td>\n",
       "      <td>-0.255371</td>\n",
       "      <td>-0.125732</td>\n",
       "      <td>0.016068</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.256104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202759</td>\n",
       "      <td>0.092041</td>\n",
       "      <td>-0.255371</td>\n",
       "      <td>0.031555</td>\n",
       "      <td>0.203857</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>-0.272217</td>\n",
       "      <td>0.088928</td>\n",
       "      <td>-0.055908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/5P60A05R1_spectrogram.png</td>\n",
       "      <td>-0.086121</td>\n",
       "      <td>0.089172</td>\n",
       "      <td>-0.174683</td>\n",
       "      <td>-0.426758</td>\n",
       "      <td>-0.082092</td>\n",
       "      <td>-0.114990</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.198975</td>\n",
       "      <td>-0.243286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154419</td>\n",
       "      <td>-0.027039</td>\n",
       "      <td>-0.070984</td>\n",
       "      <td>-0.189209</td>\n",
       "      <td>0.252930</td>\n",
       "      <td>-0.048798</td>\n",
       "      <td>-0.265381</td>\n",
       "      <td>0.128418</td>\n",
       "      <td>-0.162598</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/3P65A03R3_spectrogram.png</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>0.034851</td>\n",
       "      <td>-0.234253</td>\n",
       "      <td>-0.427246</td>\n",
       "      <td>-0.048126</td>\n",
       "      <td>-0.329834</td>\n",
       "      <td>0.031830</td>\n",
       "      <td>0.111877</td>\n",
       "      <td>-0.276123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160767</td>\n",
       "      <td>-0.096436</td>\n",
       "      <td>-0.269531</td>\n",
       "      <td>-0.185791</td>\n",
       "      <td>0.252930</td>\n",
       "      <td>-0.139282</td>\n",
       "      <td>-0.265625</td>\n",
       "      <td>0.129761</td>\n",
       "      <td>-0.154175</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/1P62A01R2_spectrogram.png</td>\n",
       "      <td>0.058044</td>\n",
       "      <td>-0.131958</td>\n",
       "      <td>-0.123047</td>\n",
       "      <td>-0.252197</td>\n",
       "      <td>-0.213013</td>\n",
       "      <td>-0.140991</td>\n",
       "      <td>0.030991</td>\n",
       "      <td>-0.042145</td>\n",
       "      <td>-0.204224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183838</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>-0.236328</td>\n",
       "      <td>-0.012367</td>\n",
       "      <td>0.303711</td>\n",
       "      <td>-0.006874</td>\n",
       "      <td>-0.223999</td>\n",
       "      <td>0.184082</td>\n",
       "      <td>-0.125366</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/6P56A06R02_spectrogram.png</td>\n",
       "      <td>-0.045807</td>\n",
       "      <td>-0.061523</td>\n",
       "      <td>-0.045197</td>\n",
       "      <td>-0.329102</td>\n",
       "      <td>-0.037079</td>\n",
       "      <td>-0.083801</td>\n",
       "      <td>-0.027954</td>\n",
       "      <td>0.187988</td>\n",
       "      <td>-0.169800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154175</td>\n",
       "      <td>0.066467</td>\n",
       "      <td>-0.085999</td>\n",
       "      <td>-0.137573</td>\n",
       "      <td>0.116211</td>\n",
       "      <td>0.149658</td>\n",
       "      <td>-0.290771</td>\n",
       "      <td>0.050995</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/1P42A01R03_spectrogram.png</td>\n",
       "      <td>0.025467</td>\n",
       "      <td>-0.013214</td>\n",
       "      <td>-0.195801</td>\n",
       "      <td>-0.453125</td>\n",
       "      <td>-0.275391</td>\n",
       "      <td>-0.149902</td>\n",
       "      <td>-0.021835</td>\n",
       "      <td>0.052521</td>\n",
       "      <td>-0.262939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209473</td>\n",
       "      <td>-0.049927</td>\n",
       "      <td>-0.178589</td>\n",
       "      <td>-0.123962</td>\n",
       "      <td>0.248047</td>\n",
       "      <td>-0.003431</td>\n",
       "      <td>-0.199829</td>\n",
       "      <td>0.122925</td>\n",
       "      <td>-0.065186</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/6P40A06R03_spectrogram.png</td>\n",
       "      <td>-0.044037</td>\n",
       "      <td>-0.016708</td>\n",
       "      <td>-0.088562</td>\n",
       "      <td>-0.362061</td>\n",
       "      <td>-0.119751</td>\n",
       "      <td>-0.167114</td>\n",
       "      <td>-0.026672</td>\n",
       "      <td>0.211792</td>\n",
       "      <td>-0.144775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161621</td>\n",
       "      <td>0.080872</td>\n",
       "      <td>-0.367676</td>\n",
       "      <td>-0.092163</td>\n",
       "      <td>0.234619</td>\n",
       "      <td>0.042145</td>\n",
       "      <td>-0.232544</td>\n",
       "      <td>0.061737</td>\n",
       "      <td>-0.078857</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/2P56A02R01_spectrogram.png</td>\n",
       "      <td>0.064026</td>\n",
       "      <td>-0.040649</td>\n",
       "      <td>-0.171021</td>\n",
       "      <td>-0.335693</td>\n",
       "      <td>-0.171387</td>\n",
       "      <td>-0.112244</td>\n",
       "      <td>-0.003649</td>\n",
       "      <td>0.106323</td>\n",
       "      <td>-0.143188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197388</td>\n",
       "      <td>-0.149780</td>\n",
       "      <td>-0.167725</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.029999</td>\n",
       "      <td>0.056335</td>\n",
       "      <td>-0.224609</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>spectrograms/Sample_data_preprocessing+label_extraction/5P56A05R3_spectrogram.png</td>\n",
       "      <td>0.089233</td>\n",
       "      <td>0.015251</td>\n",
       "      <td>-0.126831</td>\n",
       "      <td>-0.284668</td>\n",
       "      <td>-0.037140</td>\n",
       "      <td>-0.150513</td>\n",
       "      <td>0.018188</td>\n",
       "      <td>0.124084</td>\n",
       "      <td>-0.143188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>0.044037</td>\n",
       "      <td>-0.128174</td>\n",
       "      <td>-0.096985</td>\n",
       "      <td>0.154175</td>\n",
       "      <td>0.028397</td>\n",
       "      <td>-0.226929</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>-0.094910</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1754 rows × 770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             image_path  \\\n",
       "0                            spectrograms/4 July 2018 Dataset/3P66A03R2_spectrogram.png   \n",
       "1                            spectrograms/4 July 2018 Dataset/1P68A01R1_spectrogram.png   \n",
       "2                            spectrograms/4 July 2018 Dataset/5P60A05R1_spectrogram.png   \n",
       "3                            spectrograms/4 July 2018 Dataset/3P65A03R3_spectrogram.png   \n",
       "4                            spectrograms/4 July 2018 Dataset/1P62A01R2_spectrogram.png   \n",
       "...                                                                                 ...   \n",
       "1749                    spectrograms/1 December 2017 Dataset/6P56A06R02_spectrogram.png   \n",
       "1750                    spectrograms/1 December 2017 Dataset/1P42A01R03_spectrogram.png   \n",
       "1751                    spectrograms/1 December 2017 Dataset/6P40A06R03_spectrogram.png   \n",
       "1752                    spectrograms/1 December 2017 Dataset/2P56A02R01_spectrogram.png   \n",
       "1753  spectrograms/Sample_data_preprocessing+label_extraction/5P56A05R3_spectrogram.png   \n",
       "\n",
       "      feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0     -0.048340   0.055817  -0.145508  -0.373291  -0.008018  -0.236084   \n",
       "1      0.034607  -0.034241  -0.101257  -0.314209  -0.255371  -0.125732   \n",
       "2     -0.086121   0.089172  -0.174683  -0.426758  -0.082092  -0.114990   \n",
       "3     -0.034821   0.034851  -0.234253  -0.427246  -0.048126  -0.329834   \n",
       "4      0.058044  -0.131958  -0.123047  -0.252197  -0.213013  -0.140991   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1749  -0.045807  -0.061523  -0.045197  -0.329102  -0.037079  -0.083801   \n",
       "1750   0.025467  -0.013214  -0.195801  -0.453125  -0.275391  -0.149902   \n",
       "1751  -0.044037  -0.016708  -0.088562  -0.362061  -0.119751  -0.167114   \n",
       "1752   0.064026  -0.040649  -0.171021  -0.335693  -0.171387  -0.112244   \n",
       "1753   0.089233   0.015251  -0.126831  -0.284668  -0.037140  -0.150513   \n",
       "\n",
       "      feature_6  feature_7  feature_8  ...  feature_759  feature_760  \\\n",
       "0      0.068909   0.140747  -0.217529  ...     0.164551    -0.133057   \n",
       "1      0.016068  -0.000095  -0.256104  ...     0.202759     0.092041   \n",
       "2     -0.052063   0.198975  -0.243286  ...     0.154419    -0.027039   \n",
       "3      0.031830   0.111877  -0.276123  ...     0.160767    -0.096436   \n",
       "4      0.030991  -0.042145  -0.204224  ...     0.183838     0.079102   \n",
       "...         ...        ...        ...  ...          ...          ...   \n",
       "1749  -0.027954   0.187988  -0.169800  ...     0.154175     0.066467   \n",
       "1750  -0.021835   0.052521  -0.262939  ...     0.209473    -0.049927   \n",
       "1751  -0.026672   0.211792  -0.144775  ...     0.161621     0.080872   \n",
       "1752  -0.003649   0.106323  -0.143188  ...     0.197388    -0.149780   \n",
       "1753   0.018188   0.124084  -0.143188  ...     0.198242     0.044037   \n",
       "\n",
       "      feature_761  feature_762  feature_763  feature_764  feature_765  \\\n",
       "0       -0.170166    -0.205322     0.122070    -0.059418    -0.201294   \n",
       "1       -0.255371     0.031555     0.203857     0.094727    -0.272217   \n",
       "2       -0.070984    -0.189209     0.252930    -0.048798    -0.265381   \n",
       "3       -0.269531    -0.185791     0.252930    -0.139282    -0.265625   \n",
       "4       -0.236328    -0.012367     0.303711    -0.006874    -0.223999   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1749    -0.085999    -0.137573     0.116211     0.149658    -0.290771   \n",
       "1750    -0.178589    -0.123962     0.248047    -0.003431    -0.199829   \n",
       "1751    -0.367676    -0.092163     0.234619     0.042145    -0.232544   \n",
       "1752    -0.167725    -0.093750     0.029999     0.056335    -0.224609   \n",
       "1753    -0.128174    -0.096985     0.154175     0.028397    -0.226929   \n",
       "\n",
       "      feature_766  feature_767  label  \n",
       "0        0.055756    -0.131348      3  \n",
       "1        0.088928    -0.055908      1  \n",
       "2        0.128418    -0.162598      5  \n",
       "3        0.129761    -0.154175      3  \n",
       "4        0.184082    -0.125366      1  \n",
       "...           ...          ...    ...  \n",
       "1749     0.050995    -0.086914      6  \n",
       "1750     0.122925    -0.065186      1  \n",
       "1751     0.061737    -0.078857      6  \n",
       "1752     0.089600    -0.127808      2  \n",
       "1753     0.014641    -0.094910      5  \n",
       "\n",
       "[1754 rows x 770 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f7a22",
   "metadata": {},
   "source": [
    "# Splitting the data in train,test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad377719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns = ['image_path', 'label']), df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "\n",
    "#split in validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321b843",
   "metadata": {},
   "source": [
    "## Performing PCA to reduce dimensionality from 1151 down to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88d94115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform PCA on the training data\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=768)  # Reduce to 2 dimensions for visualization\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_val_pca = pca.transform(X_val)\n",
    "# Convert the PCA results back to DataFrame for easier handling\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(X_train_pca.shape[1])])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(X_test_pca.shape[1])])\n",
    "X_val_pca = pd.DataFrame(X_val_pca, columns=[f'PC{i+1}' for i in range(X_val_pca.shape[1])])\n",
    "# Display the PCA results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd5135",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd90d4fd",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5947c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_jobs=8, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_jobs=8, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_jobs=8, random_state=42)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs = 8)\n",
    "# Fit the model on the training data\n",
    "rf_classifier.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976a573",
   "metadata": {},
   "source": [
    "### Calculating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d640c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    63\n",
       "3    62\n",
       "4    62\n",
       "1    62\n",
       "5    62\n",
       "6    40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "450cbae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    200\n",
       "4    199\n",
       "2    199\n",
       "3    199\n",
       "5    198\n",
       "6    127\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94c86a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    50\n",
       "3    50\n",
       "4    50\n",
       "1    50\n",
       "5    50\n",
       "6    31\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71a5757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.97      0.96        62\n",
      "           2       0.65      0.76      0.70        63\n",
      "           3       0.54      0.63      0.58        62\n",
      "           4       0.55      0.58      0.56        62\n",
      "           5       0.55      0.47      0.50        62\n",
      "           6       0.96      0.55      0.70        40\n",
      "\n",
      "    accuracy                           0.67       351\n",
      "   macro avg       0.70      0.66      0.67       351\n",
      "weighted avg       0.68      0.67      0.67       351\n",
      "\n",
      "[[60  0  1  0  1  0]\n",
      " [ 0 48  5  6  3  1]\n",
      " [ 0 11 39  5  7  0]\n",
      " [ 1  8  9 36  8  0]\n",
      " [ 2  4 11 16 29  0]\n",
      " [ 0  3  7  3  5 22]]\n",
      "Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_classifier.predict(X_test_pca)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Generate the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "#accuraccy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5bc7ec",
   "metadata": {},
   "source": [
    "### Plotting the feature importance of the top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d27730c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Get feature importances from the trained model\n",
    "importances = rf_classifier.feature_importances_\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train_pca.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the top 10 feature importances\n",
    "top_n = 10\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_importances['Feature'][:top_n], feature_importances['Importance'][:top_n], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importances from Random Forest Classifier')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9cec9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.79264791\n",
      "Iteration 2, loss = 1.73100044\n",
      "Iteration 3, loss = 1.67376396\n",
      "Iteration 4, loss = 1.61231198\n",
      "Iteration 5, loss = 1.54377810\n",
      "Iteration 6, loss = 1.46402836\n",
      "Iteration 7, loss = 1.36921013\n",
      "Iteration 8, loss = 1.25846391\n",
      "Iteration 9, loss = 1.13214381\n",
      "Iteration 10, loss = 0.99424324\n",
      "Iteration 11, loss = 0.85661418\n",
      "Iteration 12, loss = 0.72524201\n",
      "Iteration 13, loss = 0.60587410\n",
      "Iteration 14, loss = 0.50339419\n",
      "Iteration 15, loss = 0.41669573\n",
      "Iteration 16, loss = 0.34671557\n",
      "Iteration 17, loss = 0.29152527\n",
      "Iteration 18, loss = 0.24772665\n",
      "Iteration 19, loss = 0.21182885\n",
      "Iteration 20, loss = 0.18275578\n",
      "Iteration 21, loss = 0.15938038\n",
      "Iteration 22, loss = 0.13917158\n",
      "Iteration 23, loss = 0.12193086\n",
      "Iteration 24, loss = 0.10720617\n",
      "Iteration 25, loss = 0.09418149\n",
      "Iteration 26, loss = 0.08338698\n",
      "Iteration 27, loss = 0.07323033\n",
      "Iteration 28, loss = 0.06475168\n",
      "Iteration 29, loss = 0.05691881\n",
      "Iteration 30, loss = 0.05068099\n",
      "Iteration 31, loss = 0.04507635\n",
      "Iteration 32, loss = 0.03992968\n",
      "Iteration 33, loss = 0.03566186\n",
      "Iteration 34, loss = 0.03219122\n",
      "Iteration 35, loss = 0.02889948\n",
      "Iteration 36, loss = 0.02616844\n",
      "Iteration 37, loss = 0.02369524\n",
      "Iteration 38, loss = 0.02159169\n",
      "Iteration 39, loss = 0.01968309\n",
      "Iteration 40, loss = 0.01795434\n",
      "Iteration 41, loss = 0.01652426\n",
      "Iteration 42, loss = 0.01520284\n",
      "Iteration 43, loss = 0.01401436\n",
      "Iteration 44, loss = 0.01301741\n",
      "Iteration 45, loss = 0.01208962\n",
      "Iteration 46, loss = 0.01125713\n",
      "Iteration 47, loss = 0.01052080\n",
      "Iteration 48, loss = 0.00984063\n",
      "Iteration 49, loss = 0.00922908\n",
      "Iteration 50, loss = 0.00867247\n",
      "Iteration 51, loss = 0.00819328\n",
      "Iteration 52, loss = 0.00771523\n",
      "Iteration 53, loss = 0.00729297\n",
      "Iteration 54, loss = 0.00690028\n",
      "Iteration 55, loss = 0.00654107\n",
      "Iteration 56, loss = 0.00621811\n",
      "Iteration 57, loss = 0.00591321\n",
      "Iteration 58, loss = 0.00562773\n",
      "Iteration 59, loss = 0.00536856\n",
      "Iteration 60, loss = 0.00512645\n",
      "Iteration 61, loss = 0.00490260\n",
      "Iteration 62, loss = 0.00468956\n",
      "Iteration 63, loss = 0.00450086\n",
      "Iteration 64, loss = 0.00431037\n",
      "Iteration 65, loss = 0.00414005\n",
      "Iteration 66, loss = 0.00397756\n",
      "Iteration 67, loss = 0.00382624\n",
      "Iteration 68, loss = 0.00368479\n",
      "Iteration 69, loss = 0.00355012\n",
      "Iteration 70, loss = 0.00342636\n",
      "Iteration 71, loss = 0.00330279\n",
      "Iteration 72, loss = 0.00319378\n",
      "Iteration 73, loss = 0.00308477\n",
      "Iteration 74, loss = 0.00298873\n",
      "Iteration 75, loss = 0.00289285\n",
      "Iteration 76, loss = 0.00280065\n",
      "Iteration 77, loss = 0.00271453\n",
      "Iteration 78, loss = 0.00263400\n",
      "Iteration 79, loss = 0.00255576\n",
      "Iteration 80, loss = 0.00248110\n",
      "Iteration 81, loss = 0.00241143\n",
      "Iteration 82, loss = 0.00234413\n",
      "Iteration 83, loss = 0.00228052\n",
      "Iteration 84, loss = 0.00221774\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP Validation Score: 0.8932\n",
      "MLP Test Score: 0.8917\n"
     ]
    }
   ],
   "source": [
    "#multi-layer perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Create a Multi-layer Perceptron Classifier and evaluate on validation set\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42, verbose = True)\n",
    "mlp_classifier.fit(X_train_pca, y_train)\n",
    "# Evaluate the model on the validation set\n",
    "mlp_val_score = mlp_classifier.score(X_val_pca, y_val)\n",
    "print(f'MLP Validation Score: {mlp_val_score:.4f}')\n",
    "\n",
    "#test score\n",
    "mlp_test_score = mlp_classifier.score(X_test_pca, y_test)\n",
    "print(f'MLP Test Score: {mlp_test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d49b62cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.98      0.99        62\n",
      "           2       0.92      0.97      0.95        63\n",
      "           3       0.94      0.97      0.95        62\n",
      "           4       0.74      0.77      0.76        62\n",
      "           5       0.80      0.73      0.76        62\n",
      "           6       0.97      0.95      0.96        40\n",
      "\n",
      "    accuracy                           0.89       351\n",
      "   macro avg       0.90      0.89      0.90       351\n",
      "weighted avg       0.89      0.89      0.89       351\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwwAAAK9CAYAAACJnusfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABguklEQVR4nO3dB3wU1fbA8ZMESKgBgjQpIr2DoDQBpYo+qRYQFIGHqIBSRVCqSOyg0hRpD0EEBbvwR0DQR0CKCApEmkZ6BylJINn/51xf4kzIYAIhM5v8vn7mQ3ZmM3t3ZyfeM+fcOwE+n88nAAAAAJCMwORWAgAAAIAiYAAAAADgiIABAAAAgCMCBgAAAACOCBgAAAAAOCJgAAAAAOCIgAEAAACAIwIGAAAAAI4IGAAAAAA4ImAAMpmdO3dKixYtJDQ0VAICAuSTTz5J0/3/9ttvZr+zZs1K0/36szvuuMMsaeXs2bPy73//WwoXLmw+6379+qXZvjMy/U7q56XfUTc4nRtLliyRGjVqSEhIiNl+6tQpefTRR+Wmm25ypZ0AkBQBA+CC3bt3S69eveTmm282nYQ8efJIgwYN5M0335QLFy5c19fu2rWrbN26VV588UWZM2eO1K5dWzIK7WRph0s/z+Q+Rw2WdLsur732Wqr3f+DAARk1apRs3rxZ3DRu3DjT6XziiSfMMXz44Yev6+tpx1U/s2bNmiW7fdq0aYmf64YNGxLX62el644dO+a472+//Tbxd3XJmjWrOS8eeeQR2bNnT4raFxcXJzNnzjRBWf78+SU4ONi0uVu3brb2eNHx48flgQcekOzZs8ukSZPM8cyZM6fbzQIAmyz2hwCuty+//FLuv/9+06nRTlGVKlUkNjZWvv/+exk8eLD88ssv8u67716X19ZOdEREhDz33HPSp0+f6/IaJUuWNK+jHT83ZMmSRc6fPy+ff/656YhZzZ071wRo0dHRV7VvDRhGjx5tOqN6RTil/u///k/S0ooVK6Ru3boycuRISS/6ua1cuVIOHTpkMhtp+bmqp556Sm699Va5ePGibNq0yZwDeq5ocFu0aFHH39PvWvv27c1V+kaNGsmwYcNM0KBX8xcsWCCzZ8+WqKgoKVasmLgtuXNj/fr18ueff8oLL7xgC8g0CIuPj3eppQBgR4YBSEd79+6Vjh07mo7Dtm3bTEahZ8+e0rt3b/nggw/MusqVK1+31z969Kj5N2/evNftNfQqsXYeg4KCxA0aiDVt2tR8nknNmzdP7rnnnnRriwYuKlu2bGZJK0eOHEnTY3jp0iUTtF6JZsBy5colH374oW39vn375Lvvvrvmz7Vhw4bSpUsXkxV4++23TQboxIkTpsN/JRpka7Awfvx4WbVqlQwaNEi6d+8uY8aMMcH3K6+8Il6R3Lmhx1IlPZ4aVOh3OS34fL7rnrkEkLERMADpSDsvWn8+ffp0KVKkyGXby5QpI08//bStI6dXHkuXLp1YZqFXUGNiYmy/p+v/9a9/mSzFbbfdZjolWtbxn//8x1YeooFKQidLOy8JNdJO9dIJJSVWy5Ytk9tvv910cLQDWb58edOmf6rT1qvi2inUcgv93TZt2sj27duTfb1du3aZNunzdKyFdiITOt8p8dBDD8nXX39tasGtV3K1JEm3JaUdU+1oVq1a1bwnLWlq1aqV/PTTT7bSGb0CrrQ9CSU0Ce9Ty2E0W7Rx40ZzpTtHjhyJn0vSMQxaFqbHKOn7b9mypeTLl89kMq5UvqOBp159T2hDQk2+dj579OghhQoVMvuvXr36ZR3uhOOjHfIJEyYkfrc0WL0S3Z9eydegy0oDM22ztj0tNWnSxPyr79WJBivvvPOONG/ePNlxHNox1+N6pezCp59+aoIdzWLo56Cfh55zWuZkpd+dDh06mOyKfha6Tw3+T58+fdXnhn4n9Lug9Lul2/R773ROasZBj5leVNA26HHW0saTJ08m+/dg6dKlpuRQy530cwKAq0VJEpCOtExGO/L169dP0fN1YKt2+O677z4ZOHCgrFu3TsLDw01Hc/Hixbbnaidbn6cdRu2EzJgxw3Q6atWqZToY2tnTjkz//v2lU6dOcvfdd5tOTWroFVvtiFSrVs1cwdUOlr7uf//73yv+3jfffGM64PreNSjQq516FVmvWmv5SdKOkZYSlSpVyrxX3f7ee+9JwYIF5eWXX05RO/W9Pv7447Jo0SJztVlpR7dChQpyyy23XPZ8rZXXwd9aKqave/jwYdPBaty4selIa2eyYsWK5j2PGDFCHnvsMRP8KOux1Hp0fZ/akdSr5dqhS45mljSA0uOkJWLasdXX09IlrWF3KsHRNuh2PYbaYdXvhLrhhhvMZ6odUD0eWm6m72PhwoXmO6CBkzUQVVrzryVE+l70OGoZzz/RYEsHzOsYHO1YJ3yu+r1L6xI0fQ0VFhbm+BwNCjWovpYxHNp51/NgwIAB5l89LnqMz5w5I6+++qp5jmZfNCDSQL1v374maNi/f7988cUX5rPVoPZqzg0tDdSgQsuv9Hf0mCV8rsnR4EDbqwGrlnBpMDVx4kT58ccfzetYj0FkZKQ5z/V3NIuprwMAV80HIF2cPn3ap6dcmzZtUvT8zZs3m+f/+9//tq0fNGiQWb9ixYrEdSVLljTrVq9enbjuyJEjvuDgYN/AgQMT1+3du9c879VXX7Xts2vXrmYfSY0cOdI8P8H48ePN46NHjzq2O+E1Zs6cmbiuRo0avoIFC/qOHz+euO6nn37yBQYG+h555JHLXq979+62fbZr184XFhbm+JrW95EzZ07z83333edr2rSp+TkuLs5XuHBh3+jRo5P9DKKjo81zkr4P/fzGjBmTuG79+vWXvbcEjRs3NtumTp2a7DZdrJYuXWqeP3bsWN+ePXt8uXLl8rVt29aXEnqs7rnnHtu6CRMmmP29//77ietiY2N99erVM/s+c+ZM4vvS5+XJk8d8R1LzepcuXTKf4wsvvGDWb9u2zexr1apV5jPRn/UzSno8r/R9WblypXnOjBkzzPMOHDjg+/LLL3033XSTLyAgwLa/pPr3729+98cff0zR+0hoo34GCc6fP3/Z83r16uXLkSOH+V4o3b/+3sKFCx33fbXnRnKfW3Ln5HfffWeeN3fuXNvzlixZctn6hL8Hug0A0gIlSUA60SuWKnfu3Cl6/ldffWX+1SufVglXlbUkxapSpUqJV70TrjrrVcWUzjSTEgl11lrGkdIBmQcPHjSzCumVbutVbL0Sq6UkCe/TSrMDVvq+9Op9wmeYEno1XEt4dJCuXjXWf5MrR1J6NTgw8K8/h1qKoq+VUFKiGY6U0v3o1d+U0Cv1evVXryxrRkRLTK6lbEQ/R73yrVeVE+gVZ70SrWVwWt9vpeU1+h1JDc2EaPYnYXyIDnYuXry47Xt3tTQTpO3R7IqWCJ07d85k1640i1dqz6nkaLlOAh18rDM66fvRErgdO3aY9ZpBUFri41QadzXnRmpotkjboeeMtjFh0Qyifld1QLqVZivSukwMQOZFwACkE62LT+iUpMTvv/9uOrE6rsFKO4XaOdHtViVKlLhsH1pbnrS++Vo8+OCDpoxIS6W03EZLb3Qmmit1kBLamVxJhJbYaKdHO4dXei/6PlRq3ouWXGlHUgfpasdWa8STfpYJtP06aLZs2bKm01+gQAHTed2yZYutRv2f3Hjjjaka3KzjCDSI0oDqrbfeMmVXV0s/Z21/QuBj/YwTtiftUF4NDbq0TEvHd2g5kn4Hko5zuRpaBqRjADS4089dx3H8U6lRas+p5GgpUbt27UxnXPenx13LyVTCsdfPSgN3LY3T74Z2xHUKVOt342rOjdTQMRT6evod0TZaFw0IEwZPX+vxBYDkMIYBSCfaGdGrpz///HOqfi+lnTGnWYl0hpSrfY2kAz/1auzq1avN1UzNcOjsNNoh1wGqWn+fVjMjXct7SaAdf71yr1epNcuiYyeudF+D4cOHm6vcOuBVO/Ha8daBtKnp8FmvVqeE1p4ndPR0+lBrduB6S21bE9SpU8fU2etnozX0Tlmb1NIB5073eXCiY1ISPrvUTHObQMcf6DgVPTc106PvSzM9mlUaMmSI7di//vrrJkumGQT9rmvmRsfYrF271ownud7nhrZFgwUNfpOTNFt0tccXAJJDhgFIRzooUgdz6kDXf6IzGmknQa8sWumAXO3oJMx4lBb0Cr51RqEESa9KK+1I67Slb7zxhrnSrDeA06vCSUsirO8jYRBmUlryoVdsr9eNqrQzq51yvQKtV3ydfPTRR3LnnXea2av0eVoupJ3XpJ9JWlxJT6BZFS1f0lIyHXisM2jpTE5XSz9n/a4kDXASymrS8vuigY2We2n24mo66mlFB5hrR/z999+/qt/X96DlZzqQWAeF6/mpxz0ho5VcUPP888+bwECnktWBz1OnTr3qcyM1NJjRtmoWQ9uYdNEZsQDgeiFgANLRM888YzrHWragHf+kNJjQGXQSSmqUTqNopZ0RlZb3E9DOiJY7aCmIdexB0pmYdPrRpBI6jEmnek2g08fqc/RKv7UDrpkWvfKa8D6vBw0CNGOgM8kkvdmYlXY6k2YvtGZcO4RWCYFNcsFVaukVbL2hmH4uekx1piidNcnpc/wn+jnqOA3rfRJ0BiGdjUpr3PVKelrR76/eNE6vurtJx0/oDED6PdL3mZQGT9pGnX41OQlX/a3HXmdEmjx58mVjJfSzTBo8aICQcLyu5txIDR07ohk//T4npW1Li+8kADihJAlIR9ox17pvrXfWq7PWOz2vWbMmcRpMpVcMtQOpUy4mlE788MMPpoPZtm1b0xlOK3pVXTuwWsutpRY6sHPKlClSrlw526BfLdvQq6sarOgVay2n0c6VlmTo/PNOdHpKvRpcr149M+1rwrSqWjd+pVKha6UdOr0i/E/0yrK+N73ir9OkaomLln7oNLBJj5+OH9Gryjo+QgMILdFJbb24XnXWz0073QnTvOo0pzotqpZGXc3NxjRLoYOm9fuj94LQAEQzJzrdpgad1zIwOCk99qk5bhoQ6X0pkh4b6z0KrpYGBBpo6/dWp9HVY6kZAg3G9HzSDItTdkmPtT5XzzP9fc0g6bS1SYNHPV46Va1Ou6vnhHbQ9XkacOjg8Ws5N1JKz38dJK9lUDrmRbNgOqhds0r6PvVCg05vCwDXAwEDkM5at25truRrJ1rrobVjrvX2OmuQdn70imkCHWSpnVYtmdCr/XqVfOjQoaajmZZ0rnvdvw7s1CxIwj0QtDNiDRi07XrzKb3Hgw5W1nIi7ciMHj06cSaZ5GjJhNZ0a7t1cKt2dPT39L4KXhicqR1XLRHSYE6v0GsnXuvQn332WdvztN0asOkx0JmctOOoHf3UvActj9KxEjVr1jTz8CfQmXm0LEa/Azr2om7duql6D1qzriU22mZto14V14Hm2r6EINQt+l1KSjvbaREwaCCi92PQc0Tft16B14BXxwvp+AEN/HQwutP3Xu+loDOPaWCpwYMOeNayIusMQxq862O9j4pmnfQ1dZ2+bsJxutpzIzU0UNVZkTQw1M8uS5YsJjDUNmupEgBcLwE6t+p12zsAAAAAv8YYBgAAAACOCBgAAAAAOCJgAAAAAOCIgAEAAACAIwIGAAAAAI4IGAAAAAA4ImAAAAAAkLlu3Ja9Zh+3m4ArOLbubbebAAdBgQFuNwEAkEmEeLgX6mZf8sKPE8VryDAAAAAAcOTh2A4AAABwQQDX1K34NAAAAAA4ImAAAAAA4IiSJAAAAMAqgElArMgwAAAAAH5q//790qVLFwkLC5Ps2bNL1apVZcOGDYnbfT6fjBgxQooUKWK2N2vWTHbu3Jmq1yBgAAAAAJIOenZrSYWTJ09KgwYNJGvWrPL111/Ltm3b5PXXX5d8+fIlPueVV16Rt956S6ZOnSrr1q2TnDlzSsuWLSU6OjrFr0NJEgAAAOCHXn75ZSlevLjMnDkzcV2pUqVs2YUJEybI888/L23atDHr/vOf/0ihQoXkk08+kY4dO6bodcgwAAAAAFY6hsGlJSYmRs6cOWNbdF1yPvvsM6ldu7bcf//9UrBgQalZs6ZMmzYtcfvevXvl0KFDpgwpQWhoqNSpU0ciIiJS/HEQMAAAAAAeER4ebjr11kXXJWfPnj0yZcoUKVu2rCxdulSeeOIJeeqpp2T27NlmuwYLSjMKVvo4YVtKUJIEAAAAeMTQoUNlwIABtnXBwcHJPjc+Pt5kGMaNG2cea4bh559/NuMVunbtmmZtIsMAAAAAeGTQc3BwsOTJk8e2OAUMOvNRpUqVbOsqVqwoUVFR5ufChQubfw8fPmx7jj5O2JYSBAwAAACAH2rQoIFERkba1v36669SsmTJxAHQGhgsX748cbuOidDZkurVq5fi16EkCQAAAPDDG7f1799f6tevb0qSHnjgAfnhhx/k3XffNYsKCAiQfv36ydixY804Bw0ghg8fLkWLFpW2bdum+HUIGAAAAAA/dOutt8rixYvNuIcxY8aYgECnUe3cuXPic5555hk5d+6cPPbYY3Lq1Cm5/fbbZcmSJRISEpLi1wnw6QStGUz2mn3cbgKu4Ni6t91uAhwEBfrHFRUAgP8L8fBl6+x1Brv22hfWvSpe4+FDBQAAALgglXdczuj4NAAAAAA4IsMAAAAA+OGg5/RChgEAAACAIzIMAAAAgBVjGGz4NAAAAAA4ImAAAAAA4IiSJAAAAMCKQc82ZBgAAAAAOCLDAAAAAFgx6NmGTwMAAACAIwIGAAAAAI4oSQIAAACsGPRsQ4YBAAAAgCMyDAAAAIAVg55t+DQAAAAAOCLDAAAAAFiRYbDh0wAAAADgiIABAAAAgCNKkgAAAACrQKZVtSLDAAAAAMARGQYAAADAikHPNnwaAAAAABwRMAAAAABwREkSAAAAYBXAoGcrMgwAAAAAHJFhAAAAAKwY9GzDpwEAAADAERkGAAAAwIoxDDZkGDyi6A2hMmPsI7Jv5ctyIuINWb9gmNxSqUTi9jZNqsvnk3ub7Rd+nCjVyt3oanszs40b1svTfR6XFk0ayi1VK8jK5d+43SQkMX/eXGnVvIncWrOqdO54v2zdssXtJuF/ODbexvHxLo4N3ETA4AF5c2eXFbMGyMVL8dK2z2Sp2eFFefaNRXLyzPnE5+TInk3WbN4tz7/1iatthUj0hQtSrlwFefa5EW43BclY8vVX8tor4dLryd4yf+FiKV++gjzRq4ccP37c7aZlehwbb+P4eBfHBm4jYPCAgd2ay75DJ6XXqPdlwy+/y+8HjsvytTtk775jic/54Mv1Ev7uElmxNtLVtkKkQcNG0vupftKkaXO3m4JkzJk9U9rf94C0bddBSpcpI8+PHC0hISHyyaKP3W5apsex8TaOj3dxbFwa9OzW4kHebFUmc0/jqrJpW5TMfaW7/L48XCI+GCLd2tV3u1mA37kYGyvbt/0idev9ff4EBgZK3br1ZctPP7ratsyOY+NtHB/v4tjACzwdMPzxxx/SvXv3Kz4nJiZGzpw5Y1t88XHiT0rdWEB63t9QdkUdldZPTpJpC7+X15+5TzrfW8ftpgF+5eSpkxIXFydhYWG29fr42LG/M3ZIfxwbb+P4eBfHxsVBz24tHuTpgOHEiRMye/bsKz4nPDxcQkNDbculwxvFnwQGBsjmHX/IyImfy0+R+2TGov/KzMVrpOd9t7vdNAAAAGRyrk6r+tlnn11x+549e/5xH0OHDpUBAwbY1hVsOET8yaFjZ2T7nkO2dTv2HpK2TWu41ibAH+XLm0+CgoIuGwiojwsUKOBau8Cx8TqOj3dxbCCZPWBo27atBAQEiM/nc3yObr+S4OBgs9h+JzBI/EnE5j1SrmRB27qyJQpK1METrrUJ8EdZs2WTipUqy7q1EdKkaTOzLj4+Xtati5COnbq43bxMjWPjbRwf7+LYuMSjg4/d4uqnUaRIEVm0aJH54ie3bNq0STKDt99fIbdVLSWDu7eQm4sXkAfvqi3dOzSQdz5cnficfHlymHsvVCxd2Dwud1Mh87hQWG4XW545nT9/TiJ3bDeL2r9/n/n54MEDbjcNIvJw126y6KMF8tkni2XP7t0ydswouXDhgrRt197tpmV6HBtv4/h4F8cGmTrDUKtWLdm4caO0adMm2e3/lH3IKDZui5IHB06TMX1by7DHWslv+4/L4Fc/lvlfb7DNpDRtzMOJj+e8/Ndg8LFTv5IX3/nKlXZnVtt++Vke69418fEbr75k/r23dVsZ/eJfP8M9d7W6W06eOCGTJ74lx44dlfIVKsrkd96TMFL3ruPYeBvHx7s4Ni7w6OBjtwT4XOyRf/fdd3Lu3Dm56667kt2u2zZs2CCNGzdO1X6z1+yTRi3E9XBs3dtuNwEOggL5AwkASB8hrl62vrLsrca79toXvu4vXuPqoWrYsOEVt+fMmTPVwQIAAABwTRjDYMOnAQAAAMARAQMAAAAARx6uHgMAAABcwKBnGzIMAAAAAByRYQAAAACsGPRsw6cBAAAAwBEBAwAAAABHlCQBAAAAVpQk2fBpAAAAAHBEhgEAAACwYlpVGzIMAAAAABwRMAAAAABwREkSAAAAYMWgZxs+DQAAAACOyDAAAAAAVgx6tiHDAAAAAMARGQYAAADAijEMNnwaAAAAABwRMAAAAABwREkSAAAAYMWgZxsyDAAAAAAckWEAAAAALALIMNiQYQAAAADgiIABAAAAgCNKkgAAAAALSpLsyDAAAAAAcESGAQAAALAiwWBDhgEAAACAIzIMAAAAgAVjGOzIMAAAAABwRMAAAAAAwBElSQAAAIAFJUl2ZBgAAAAAOCLDAAAAAFiQYbAjwwAAAADAEQEDAAAAAEeUJAEAAAAWlCTZkWEAAAAA4IgMAwAAAGBFgsGGDAMAAAAAR2QYAAAAAAvGMNiRYQAAAADgiIABAAAAgCNKkgAAAAALSpIyQcBwcv1Et5uAK8h3ax+3mwAHnDvA1Ym9FO92E+AgWxaKKYBrlSEDBgAAAOBqkWGwI+wGAAAA4IiAAQAAAIAjSpIAAAAAC0qS7MgwAAAAAH5o1KhRJrixLhUqVEjcHh0dLb1795awsDDJlSuXdOjQQQ4fPpzq1yFgAAAAAKwCXFxSqXLlynLw4MHE5fvvv0/c1r9/f/n8889l4cKFsmrVKjlw4IC0b98+tS9BSRIAAADgr7JkySKFCxe+bP3p06dl+vTpMm/ePGnSpIlZN3PmTKlYsaKsXbtW6tatm+LXIMMAAAAAWCQt8wlIxyUmJkbOnDljW3Sdk507d0rRokXl5ptvls6dO0tUVJRZv3HjRrl48aI0a9Ys8blarlSiRAmJiIhI1edBwAAAAAB4RHh4uISGhtoWXZecOnXqyKxZs2TJkiUyZcoU2bt3rzRs2FD+/PNPOXTokGTLlk3y5s1r+51ChQqZbalBSRIAAADgEUOHDpUBAwbY1gUHByf73FatWiX+XK1aNRNAlCxZUhYsWCDZs2dPszYRMAAAAAAemVY1ODjYMUD4J5pNKFeunOzatUuaN28usbGxcurUKVuWQWdJSm7Mw5VQkgQAAABkAGfPnpXdu3dLkSJFpFatWpI1a1ZZvnx54vbIyEgzxqFevXqp2i8ZBgAAAMAPb9w2aNAguffee00Zkk6ZOnLkSAkKCpJOnTqZsQ89evQw5U358+eXPHnySN++fU2wkJoZkhQBAwAAAOCH9u3bZ4KD48ePyw033CC33367mTJVf1bjx4+XwMBAc8M2nWmpZcuWMnny5FS/ToDP5/NJBhN9ye0W4Ery3drH7SbAwcn1E91uAuCXYi/Fu90EOMiWheprrwrx8GXrgt0XuPbaR2Y8IF7j4UMFAAAAuMA/KpLSDWE3AAAAAEdkGAAAAAA/HPScXsgwAAAAAHBEhgEAAACwIMNgR4YBAAAAgCMCBgAAAACOKEkCAAAALChJsiPDAAAAAMARGQYAAADAggyDHRkGAAAAAI4IGAAAAAA4oiQJAAAAsKIiyYYMAwAAAABHZBgAAAAACwY925FhAAAAAOCIDAMAAABgQYbBjgwDAAAAAEcEDAAAAAAcUZIEAAAAWFCSZEeGAQAAAIAjMgwAAACAFQkGGzIMAAAAABwRMAAAAABwREkSAAAAYMGgZzsyDAAAAAAckWEAAAAALMgw2JFhAAAAAOCIgAEAAACAI0qSAAAAAAtKkuzIMHjY/HlzpVXzJnJrzarSueP9snXLFreblCkVvSFUZox9RPatfFlORLwh6xcMk1sqlUjc3qZJdfl8cm+z/cKPE6VauRtdbS84d7yMY+NNM6e/K488dL80rldLWtzRQAb16yO//bbX7WbBgnMHbiJg8KglX38lr70SLr2e7C3zFy6W8uUryBO9esjx48fdblqmkjd3dlkxa4BcvBQvbftMlpodXpRn31gkJ8+cT3xOjuzZZM3m3fL8W5+42lb8hXPHuzg23rVpw3q5/8GHZMac+TLxnely6dJF6ft4D7lw/u+/dXAP5447GQa3Fi8K8Pl8Pslgoi+J39OrB5WrVJVhz48wj+Pj46VF08bS6aGHpUfPx8Sf5bu1j/iLF55qLfWq3yzNekz4x+eWKJJfIr8aI3UeDJctv+4Xf3Ry/UTxdxn53PF3GfnYxF6Kl4zk5IkT0uLOBvLOjP/ILbVuFX+WLYv/XxvNqOdOiIcL40v1+9K119474R7xGv8/izKgi7Gxsn3bL1K3Xv3EdYGBgVK3bn3Z8tOPrrYts7mncVXZtC1K5r7SXX5fHi4RHwyRbu3+Pi7wFs4d7+LY+JezZ/80/+bJE+p2UzI9zh2XBLi4eBABgwedPHVS4uLiJCwszLZeHx87dsy1dmVGpW4sID3vbyi7oo5K6ycnybSF38vrz9wnne+t43bTkAzOHe/i2PgPvXr9xivhUr3GLVKmbDm3m5Ppce7AC1xPBl24cEE2btwo+fPnl0qVKtm2RUdHy4IFC+SRRx5x/P2YmBizWPmCgiU4OPi6tRmZR2BggMkwjJz4uXn8U+Q+qVymiPS873aZ+/k6t5sHAGnulXFjZPfunTJt1ly3mwLAI1zNMPz6669SsWJFadSokVStWlUaN24sBw8eTNx++vRp6dat2xX3ER4eLqGhobbl1ZfDxZ/ly5tPgoKCLhvMpI8LFCjgWrsyo0PHzsj2PYds63bsPSTFC+dzrU1wxrnjXRwb//DKuBfku9WrZMq02VKoUGG3mwPOHdcw6NlDAcOQIUOkSpUqcuTIEYmMjJTcuXNLgwYNJCoqKsX7GDp0qAksrMvgIUPFn2XNlk0qVqos69ZG2FLE69ZFSLXqNV1tW2YTsXmPlCtZ0LaubImCEnXwhGttgjPOHe/i2Hibzn+iwcK3K76RKdNmyo3FirndJPwP5w4ks5ckrVmzRr755hsTIevy+eefy5NPPikNGzaUlStXSs6cOf9xH1p6lLT8KCPMkvRw124yfNgQqVy5ilSpWk3enzPblG+1bdfe7aZlKm+/v0JWzhoog7u3kI+XbZJbK98k3Ts0kD4vfJD4nHx5cpiMQ5GCfw0OLHdTIfPv4eNn5PDxvwYOIv1w7ngXx8a7Xh43RpZ+/aW8NmGi5MiZU44dO2rW58qVW0JCQtxuXqbHuZP+vHqlP1MGDPplz5Ili+3gTJkyRfr06WPKk+bNmyeZ1V2t7jbT2k2e+Jb5w12+QkWZ/M57Ekb6MV1t3BYlDw6cJmP6tpZhj7WS3/Yfl8Gvfizzv95gm0lp2piHEx/Pebm7+Xfs1K/kxXe+cqXdmRnnjndxbLzr4wXzzb+P9+hqWz9izDi5t007l1qFBJw7yNT3Ybjtttukb9++8vDDf3e2EmjQMHfuXDlz5oyZHSA1MkKGISPzp/swZDYZ4T4MgBsy2n0YMpKMcB+GjMrL92EoPfBr11579+utxGtcPYvatWsnH3zwd2mH1cSJE6VTp06mrhIAAABIL1qR5NbiRdzpGemODIN3kWEArg4ZBu8iw+BdXs4wlBnkXoZh12veyzB4+FABAAAA6Y9Bz3aE3QAAAAAckWEAAAAALEgw2JFhAAAAAOCIgAEAAACAI0qSAAAAAAsGPduRYQAAAADgiAwDAAAAYEGCwY4MAwAAAABHBAwAAAAAHFGSBAAAAFgEBlKTZEWGAQAAAIAjMgwAAACABYOe7cgwAAAAAHBEhgEAAACw4MZtdmQYAAAAADgiYAAAAADgiJIkAAAAwIKKJDsyDAAAAAAckWEAAAAALBj0bEeGAQAAAIAjAgYAAAAAjihJAgAAACwoSbIjwwAAAADAERkGAAAAwIIEgx0ZBgAAAACOyDAAAAAAFoxhsCPDAAAAAMARAQMAAAAAR5QkAQAAABZUJNmRYQAAAADgiAwDAAAAYMGgZzsyDAAAAAAcETAAAAAAcERJEgAAAGBBRZIdGQYAAAAAjsgwAAAAABYMerYjwwAAAADAERkGAAAAwIIEgx0ZBgAAAACOCBgAAAAAOKIkCQAAALBg0LMdGQYAAAAAjsgwAAAAABYkGOwIGJDuTq6f6HYT4CBfvQFuNwEOjv33dbebgCvIloWEvVfFxfvcbgIc0Sv3F/yFAwAAAOCIDAMAAABgwaBnOzIMAAAAgJ976aWXTKDTr1+/xHXR0dHSu3dvCQsLk1y5ckmHDh3k8OHDqd43AQMAAABgoQkGt5arsX79ennnnXekWrVqtvX9+/eXzz//XBYuXCirVq2SAwcOSPv27VO9fwIGAAAAwE+dPXtWOnfuLNOmTZN8+fIlrj99+rRMnz5d3njjDWnSpInUqlVLZs6cKWvWrJG1a9em6jUIGAAAAAALLe1xa4mJiZEzZ87YFl3nREuO7rnnHmnWrJlt/caNG+XixYu29RUqVJASJUpIREREqj4PAgYAAADAI8LDwyU0NNS26LrkzJ8/XzZt2pTs9kOHDkm2bNkkb968tvWFChUy21KDWZIAAAAAjxg6dKgMGGC/L1JwcPBlz/vjjz/k6aeflmXLlklISMh1bRMBAwAAAGDh5qyqwcHByQYISWnJ0ZEjR+SWW25JXBcXFyerV6+WiRMnytKlSyU2NlZOnTplyzLoLEmFCxdOVZsIGAAAAAA/07RpU9m6dattXbdu3cw4hSFDhkjx4sUla9assnz5cjOdqoqMjJSoqCipV69eql6LgAEAAADwsxu35c6dW6pUqWJblzNnTnPPhYT1PXr0MOVN+fPnlzx58kjfvn1NsFC3bt1UvRYBAwAAAJABjR8/XgIDA02GQWdaatmypUyePDnV+yFgAAAAADKAb7/91vZYB0NPmjTJLNeCgAEAAADws5Kk9MR9GAAAAAA4IsMAAAAAWJBgsCPDAAAAAMARAQMAAAAAR5QkAQAAABYMerYjwwAAAADAERkGAAAAwIIEgx0ZBgAAAACOyDAAAAAAFoxhsCPDAAAAAMARAQMAAAAAR5QkAQAAABZUJNmRYQAAAADgiAwDAAAAYBFIisGGDAMAAAAARwQMAAAAABxRkgQAAABYUJFkR4YBAAAAgCMyDAAAAIAFd3q2I8MAAAAAwBEZBgAAAMAikASDDRkGAAAAAI4IGAAAAAA4oiQJAAAAsGDQsx0ZBgAAAACOyDAAAAAAFiQY7MgwAAAAAHBEwAAAAADAESVJAAAAgEWAUJNkRYYBAAAAgCMyDAAAAIAFd3q2I8PgYfPnzZVWzZvIrTWrSueO98vWLVvcbhIsOD7eUPSGUJkxprPsW/aCnPjuZVn/wWC5pWIx23OG97pL9nw9ymz/ctLjUrp4Adfam5lt3LBenu7zuLRo0lBuqVpBVi7/xu0mIQn+rnkT5w7cRsDgUUu+/kpeeyVcej3ZW+YvXCzly1eQJ3r1kOPHj7vdNHB8PCNv7uyy4r2+cvFSnLR9eprUfPBleXbCp3LyzIXE5wx8pIk8+WBDeSp8oTTqNkHOXYiVz9/uJcHZSLCmt+gLF6RcuQry7HMj3G4KksHfNe/i3HHnxm1uLV5EwOBRc2bPlPb3PSBt23WQ0mXKyPMjR0tISIh8suhjt5sGjo9nDOzaRPYdPiW9xsyXDdui5PcDJ2T5ul9l7/6/Ozi9OzWSl2csky9W/yI/7zoo/x45T4oUyCOtG1dxte2ZUYOGjaT3U/2kSdPmbjcFyeDvmndx7sBtBAwedDE2VrZv+0Xq1qufuC4wMFDq1q0vW3760dW2gePjJfc0rCybtv8hc8Mfkd+XjpaI9wdIt7Z1E7ffdGN+Exys+OHXxHVnzkXL+l+ipE61m1xqNeA9/F0D4OmAYfv27TJz5kzZsWOHeaz/PvHEE9K9e3dZsWLFP/5+TEyMnDlzxrboOn928tRJiYuLk7CwMNt6fXzs2DHX2oW/cHy8o9SNYdKzQ33Z9ccxad33XZn28Rp5fWA76XxPbbO9cFge8++R43/afk8fFwrL7UqbAS/i7xpgp5VBbi1e5GrAsGTJEqlRo4YMGjRIatasaR43atRIdu3aJb///ru0aNHiH4OG8PBwCQ0NtS2vvhyebu8BgHsCAwNkc+Q+GTn5K/np1/0yY/FamfnJWunZ/u+rpAAAwI8DhjFjxsjgwYPNgCrNMjz00EPSs2dPWbZsmSxfvtxse+mll664j6FDh8rp06dty+AhQ8Wf5cubT4KCgi4baKaPCxRgdhe3cXy849CxM7J9z2Hbuh2/HZbihfP9tf34GfNvwSTZBH18OEnWAcjM+LsG2AUGBLi2eJGrAcMvv/wijz76qPn5gQcekD///FPuu+++xO2dO3eWLf8wpVtwcLDkyZPHtug6f5Y1WzapWKmyrFsbkbguPj5e1q2LkGrVa7raNnB8vCTip9+kXMmCtnVlS9wgUYdOmJ9/239CDh47I3feWjZxe+6cwXJr5RKybstv6d5ewKv4uwbgSlyfVzBh+igdXKWzMWhJUYLcuXObjEFm9HDXbjJ82BCpXLmKVKlaTd6fM1suXLggbdu1d7tp4Ph4xtsfrJKV05+SwY82lY+/+ckEAt3b1ZU+4xYmPmfSB6tlSPfmZpyDBhAjH7/LBBGfrfrZ1bZnRufPn5M/oqISH+/fv08id2yXPKGhUqRIUVfbBv6ueRnnDjJ1wHDTTTfJzp07pXTp0uZxRESElChRInF7VFSUFClSRDKju1rdLSdPnJDJE9+SY8eOSvkKFWXyO+9JGKlhT+D4eMPGbX/Ig4Nnypje98iwf7eQ3w6ckMFvfCrzl2xKfM7r/1khObJnk4nD7pe8ubLLmp/2Suun3pWY2Euutj0z2vbLz/JY966Jj9949a+S03tbt5XRL165/BTXH3/XvItzJ/15tDLINQE+n8/n1otPnTpVihcvLvfcc0+y24cNGyZHjhyR9957L1X7jaYfAFyVfPUGuN0EODj239fdbgKuICiQ3oVXxcW71s3BP8iZzbvnTYcZG1177Y+71xKvcTXD8Pjjj19x+7hx49KtLQAAAIDy6h2XM+19GAAAAAB4l+uDngEAAAAvIcFgR4YBAAAAgCMCBgAAAACOKEkCAAAALLx6x2W3kGEAAAAA4IgMAwAAAGBBfsGODAMAAAAARwQMAAAAABxRkgQAAABYcKdnOzIMAAAAAK4tw7BlyxZJqWrVqqX4uQAAAIDXBJJgSH3AUKNGDZOa8fl8yW5P2Kb/xsXFpWSXAAAAADJKwLB3797r3xIAAADAAxjDcBUBQ8mSJVPyNAAAAAAZzFUNep4zZ440aNBAihYtKr///rtZN2HCBPn000/Tun0AAAAA/ClgmDJligwYMEDuvvtuOXXqVOKYhbx585qgAQAAAPBnWpHk1pIhAoa3335bpk2bJs8995wEBQUlrq9du7Zs3bo1rdsHAAAAwJ9u3KYDoGvWrHnZ+uDgYDl37lxatQsAAABwBYOerzHDUKpUKdm8efNl65csWSIVK1ZM7e4AAAAAZKQMg45f6N27t0RHR5t7L/zwww/ywQcfSHh4uLz33nvXp5UAAAAA/CNg+Pe//y3Zs2eX559/Xs6fPy8PPfSQmS3pzTfflI4dO16fVgIAAADphDs9X2PAoDp37mwWDRjOnj0rBQsWvJrdAAAAAMiIAYM6cuSIREZGJg4MueGGG9KyXQAAAIArGPR8jYOe//zzT3n44YdNGVLjxo3Noj936dJFTp8+ndrdAQAAAMhIAYOOYVi3bp18+eWX5sZtunzxxReyYcMG6dWr1/VpJQAAAJBOAlxcMkRJkgYHS5culdtvvz1xXcuWLc3N3O666660bh8AAAAAf8owhIWFSWho6GXrdV2+fPnSql0AAAAA/DFg0OlU9V4Mhw4dSlynPw8ePFiGDx+e1u0DAAAA0lVgQIBri9+WJNWsWdM2Wnznzp1SokQJs6ioqCgJDg6Wo0ePMo4BAAAAyEBSFDC0bdv2+rcEAAAA8ACPXuj3dsAwcuTI698SAAAAAP4/hgEAAABA5pHqaVXj4uJk/PjxsmDBAjN2ITY21rb9xIkTadk+AAAAIF1xp+drzDCMHj1a3njjDXnwwQfNnZ11xqT27dtLYGCgjBo1KrW7AwAAAJCRAoa5c+eam7QNHDhQsmTJIp06dZL33ntPRowYIWvXrr0+rQQAAADSiSYY3FoyRMCg91yoWrWq+TlXrlwmy6D+9a9/yZdffpn2LQQAAADgPwFDsWLF5ODBg+bn0qVLy//93/+Zn9evX2/uxQAAAAAgEw96bteunSxfvlzq1Kkjffv2lS5dusj06dPNAOj+/ftfn1YCAAAA6cSrd1z2m4DhpZdeSvxZBz6XLFlS1qxZI2XLlpV77703rdsHAAAAwJ/vw1C3bl0zU5JmHMaNG5c2rQIAAABcwqDn63TjNh3XMHz48LTaHQAAAAB/LEkCAAAAMjJu3HadMgwAAAAAMh4CBgAAAADXXpKkA5uv5OjRoyndFTK5C7FxbjcBDg6setXtJsBB2acWu90EXMH3L9ztdhPgoGi+ELebAD/EFfWrDBh+/PHHf3xOo0aNUro7AAAAANdgypQpZvntt9/M48qVK8uIESOkVatW5nF0dLQMHDhQ5s+fLzExMdKyZUuZPHmyFCpU6PoEDCtXrkztewAAAAD8jr8Mei5WrJi5R5reD83n88ns2bOlTZs25kK/Bg96U+Uvv/xSFi5cKKGhodKnTx9p3769/Pe//03V6zBLEgAAAOCH7k1y0+QXX3zRZBzWrl1rgonp06fLvHnzpEmTJmb7zJkzpWLFima73kstpSjRAgAAADwiJiZGzpw5Y1t03T+Ji4szpUfnzp2TevXqycaNG+XixYvSrFmzxOdUqFBBSpQoIREREalqEwEDAAAAYBEY4N4SHh5uyoesi65zsnXrVsmVK5cEBwfL448/LosXL5ZKlSrJoUOHJFu2bJI3b17b83X8gm5LDUqSAAAAAI8YOnToZbOTajDgpHz58rJ582Y5ffq0fPTRR9K1a1dZtWpVmraJgAEAAACw0Cv9bgkODr5igJCUZhHKlCljfq5Vq5asX79e3nzzTXnwwQclNjZWTp06ZcsyHD58WAoXLnz9S5K+++476dKli6mP2r9/v1k3Z84c+f77769mdwAAAADSQHx8vBnzoMFD1qxZZfny5YnbIiMjJSoqyvThr2vA8PHHH5s5XLNnz26mbEoYhKFpkHHjxqV2dwAAAIDnplV1a0lt+dLq1avNfRh0LIM+/vbbb6Vz585m7EOPHj1MeZPeHkEHQXfr1s0EC6mZIemqAoaxY8fK1KlTZdq0aSZqSdCgQQPZtGlTancHAAAA4CocOXJEHnnkETOOoWnTpqYcaenSpdK8eXOzffz48fKvf/1LOnToYG6wrKVIixYtuv5jGDSVkdwdnTWK0RopAAAAANef3mfhSkJCQmTSpElmuRapzjBoZLJr167L1uv4hZtvvvmaGgMAAABk5mlVvSjVAUPPnj3l6aeflnXr1pk6qwMHDsjcuXNl0KBB8sQTT1yfVgIAAABwRapLkp599lkz+lrrpM6fP2/Kk3TqJw0Y+vbte31aCQAAAKSTVI49zvBSHTBoVuG5556TwYMHm9Kks2fPmrvJ6R3mAAAAAGQsV33jNr1JhAYKAAAAADKuVAcMd9555xXniF2xYsW1tgkAAABwTSA1SdcWMNSoUcP2+OLFi7J582b5+eefpWvXrqndHQAAAICMFDDoDSCSM2rUKDOeAQAAAPBnqZ5GNINLs8+jS5cuMmPGjLTaHQAAAAB/HvScVEREhLmbHAAAAODPGMJwjQFD+/btbY99Pp8cPHhQNmzYIMOHD0/t7gAAAABkpIAhNDTU9jgwMFDKly8vY8aMkRYtWqRl2wAAAAD4U8AQFxcn3bp1k6pVq0q+fPmuX6sAAAAAlzCt6jUMeg4KCjJZhFOnTqXm1wAAAABkllmSqlSpInv27Lk+rQEAAABcpgkGt5YMETCMHTtWBg0aJF988YUZ7HzmzBnbAgAAACATjmHQQc0DBw6Uu+++2zxu3bq1BFjCIJ0tSR/rOAcAAAAAmSxgGD16tDz++OOycuXK69siAAAAwEWBHi0N8nzAoBkE1bhx4+vZHgAAAAD+Oq2qtQQJAAAAyIiYVvUaAoZy5cr9Y9Bw4sSJ1OwSAAAAQEYJGHQcQ9I7PQMAAAAZCQmGawgYOnbsKAULFkzNrwAAAADwYym+DwPjFwAAAIDMJ9WzJAEAAAAZGdOqXmXAEB8fn9KnAgAAAMiMYxgAAACAjC5ASDFc1RgGAAAAAJkPAQMAAAAAR5QkAQAAABYMerYjwwAAAADAERkGAAAAwIIMgx0ZBgAAAACOyDAAAAAAFgEBpBisCBg8bP68uTJ75nQ5duyolCtfQZ4dNlyqVqvmdrMyvY8XzJdFH82Xgwf2m8c331xGuj/2hNS/vZHbTcv0ODbe1adlORnWropMW75LRi7cYtbdkCdYhrevKo0qFpRcIVlk9+Gz8ubXO+SrHw+43dxMJy4uTubOmCIr/u9LOXn8uOQvcIM0v7u1dOr6GB0nj6BPADdRkuRRS77+Sl57JVx6Pdlb5i9cLOXLV5AnevWQ48ePu920TK9goULSu29/mTV3oVlq3VZHnunfR/bs3ul20zI9jo03VS+ZT7o0LCW/7DtlW//Wo7WldOFc8uiUCGnywjfy1Y/75Z2edaRK8VDX2ppZLZw7U778ZKE82X+ovDt3sXR/op98NHeWfPbRPLebBvoE8ADPBQw+n8/tJnjCnNkzpf19D0jbdh2kdJky8vzI0RISEiKfLPrY7aZleg0b3yn1GzaWEiVvMssTffpJjhw55Octf101hXs4Nt6TIzhIJnavLYPf3ySnz1+0bat9c5jMWLlbNv92UqKOnZc3v46U0+djpVqJfK61N7Pa/vNmqXv7HXJb/UZSqMiN0vDO5nLLbfUkcvvPbjcN9AlcG/Ts1uJFngsYgoODZfv27ZKZXYyNle3bfpG69eonrgsMDJS6devLlp9+dLVtuDyNv2zJV3LhwgWpWq26282BBcfGG8Z1rCHLfz4k3+04etm2DXuOS+taxSRvjqyiVS9taheTkKxBsubXy5+L66tilRqyeeMPsi/qN/N4z85I+WXLj1K77u1uNy3To0+ATD2GYcCAAY7/k3/ppZckLCzMPH7jjTeuuJ+YmBizWPmCgk3g4a9OnjppPoeEzyCBPt67d49r7cLfdu38VXp27SSxsbGSPXsOefn1t6RU6TJuNwscG0/RAKBqibxyd/jKZLf3mvaDTP33bbLtjXvlYly8XIiNkx5T18pvR8+le1szuwe6dJfz587KY53bSmBgkMTHx0nXx/pKkxb3uN20TI8+gTsYuuORgGHChAlSvXp1yZs372UlSZphyJkzZ4oGWoWHh8vo0aNt654bPlKeHzEqzdsMJCh5003yn/mL5NzZs7Lim6UyZsQwmfLebDqmHsCx8Yai+bLLmAeqScc3v5eYS/HJPueZ1pUkT46s8sD47+TE2Vi5q0YRmdrzNmn32mrZceBMurc5M1u9YqmsXPaVPDMyXEqWKiN7du6Qd9569a/Bz61au908AJk1YBg3bpy8++678vrrr0uTJk0S12fNmlVmzZollSpVStF+hg4delm2QjMM/ixf3nwSFBR02WAmfVygQAHX2oW/Zc2aTYqXKGl+rlCpsmz75Wf58IM58uzz9uAV6Y9j4w3VSuSVG/KEyNJhf/99zxIUKHXLFJBud9wsDUcuk+53lpY7Ri+TXw/+abZv239a6pQpII/ecbM8O2+zi63PfKZPHi8PdO4udzRrZR6XKl1Wjhw6KAvmTCdgcBl9AmTqgOHZZ5+Vpk2bSpcuXeTee+81mQINFlJLS4+Slh9FXxK/ljVbNqlYqbKsWxshTZo2M+vi4+Nl3boI6dipi9vNQzI0MxYbax/QCW/g2LhDxyzcOeYb27rxj9SSXYf+lEn/96tkzxZk1sUnmeciLt4ngdQCpLuY6GgJCLQPawwMChJffPLZIaQf+gTu4O+Qh+7DcOutt8rGjRuld+/eUrt2bZk7dy7zPf/Pw127yfBhQ6Ry5SpSpWo1eX/ObDN4s2279m43LdOb/NYbUq+BziRSRM6fOyf/9/UXsmnDDzJh8jS3m5bpcWy841zMJYlMUlZ0PvaSnDwXa9ZnCQyQPUfOyiuda8qYj7fKyf+VJOk9GR6ZvMa1dmdWdRo0lvn/mSYFCxWWkqVKy65fd8iiD+dIi7vbuN000CeAB7h+47ZcuXLJ7NmzZf78+dKsWTMzsAcid7W6W06eOCGTJ75lbtJSvkJFmfzOexJG+tF1elxGD39Wjh87Krly5ZbSZcuZDmmdun/PYAF3cGz8x6V4nzw88b8yrG0Vmf1kPckZnEX2Hj0rT8/eICt+Pux28zKdJ/o/K/+ZNkkmvT5OTp08YcYu3N36PnmoWy+3mwb6BK7w6vSmbgnweejGB/v27TMZBw0cdNDz1fL3kqSMTmdCAZA6lQd86nYTcAXfv3C3202Ag6L5QtxuAhyEuH7Z2tlb3+917bWfur2UeI2nDlWxYsXMAgAAALiFCnmP37gNAAAAgHcQMAAAAADwj5IkAAAAwG2BQk2SFRkGAAAAAI7IMAAAAAAWDHq2I8MAAAAAwBEBAwAAAABHlCQBAAAAFtzp2Y4MAwAAAABHZBgAAAAAi0BGPduQYQAAAADgiIABAAAAgCNKkgAAAAALKpLsyDAAAAAAcESGAQAAALBg0LMdGQYAAAAAjsgwAAAAABYkGOzIMAAAAABwRMAAAAAAwBElSQAAAIAFV9Tt+DwAAAAAOCLDAAAAAFgEMOrZhgwDAAAAAEcEDAAAAAAcUZIEAAAAWFCQZEeGAQAAAIAjMgwAAACARSCDnm3IMAAAAABwRIYBAAAAsCC/YEeGAQAAAIAjAgYAAAAAjihJAgAAACwY82xHhgEAAACAIzIMAAAAgEUAKQYbMgwAAAAAHBEwAAAAAHBESRIAAABgwRV1Oz4PAAAAAI7IMAAAAAAWDHq2I8MAAAAAwBEZBgAAAMCC/IIdGQYAAAAAjggYAAAAADiiJAkAAACwYNCzHQED0l22LCS2vCookD+QXvXhgDvdbgKuoOUr37rdBDjYGn6X200Arpvw8HBZtGiR7NixQ7Jnzy7169eXl19+WcqXL5/4nOjoaBk4cKDMnz9fYmJipGXLljJ58mQpVKhQil+HnhsAAACQpIPs1pIaq1atkt69e8vatWtl2bJlcvHiRWnRooWcO3cu8Tn9+/eXzz//XBYuXGief+DAAWnfvn2qXocMAwAAAOCHlixZYns8a9YsKViwoGzcuFEaNWokp0+flunTp8u8efOkSZMm5jkzZ86UihUrmiCjbt26KXodMgwAAACAR8TExMiZM2dsi65LCQ0QVP78+c2/Gjho1qFZs2aJz6lQoYKUKFFCIiIiUtwmAgYAAAAgyaBnt5bw8HAJDQ21Lbrun8THx0u/fv2kQYMGUqVKFbPu0KFDki1bNsmbN6/tuTp+QbelFCVJAAAAgEcMHTpUBgwYYFsXHBz8j7+nYxl+/vln+f7779O8TQQMAAAAgIWbcwYGBwenKECw6tOnj3zxxReyevVqKVasWOL6woULS2xsrJw6dcqWZTh8+LDZllKUJAEAAAB+yOfzmWBh8eLFsmLFCilVqpRte61atSRr1qyyfPnyxHWRkZESFRUl9erVS/HrkGEAAAAALPzlvm29e/c2MyB9+umnkjt37sRxCTruQe/LoP/26NHDlDjpQOg8efJI3759TbCQ0hmSFAEDAAAA4IemTJli/r3jjjts63Xq1EcffdT8PH78eAkMDJQOHTrYbtyWGgQMAAAAgJ+WJP2TkJAQmTRpklmuFgEDAAAAYBHo6rBn72HQMwAAAABHZBgAAAAAPxz0nF7IMAAAAABwRMAAAAAAwBElSQAAAIBFAIOebcgwAAAAAHBEhgEAAACwYNCzHRkGAAAAAI7IMAAAAAAW3LjNjgwDAAAAAEcEDAAAAAAcUZIEAAAAWDDo2Y4MAwAAAABHZBgAAAAACzIMdmQYAAAAADgiYAAAAADgiJIkAAAAwCKA+zDYkGEAAAAA4IgMAwAAAGARSILBhgwDAAAAAEdkGAAAAAALxjDYkWEAAAAA4IiAAQAAAIAjSpIAAAAAC+70bEeGAQAAAIAjMgwAAACABYOe7cgwAAAAAHBEwAAAAADAESVJAAAAgAV3erYjwwAAAADAERkGAAAAwIJBz3ZkGAAAAAA4ImAAAAAA4IiSJAAAAMCCOz3bETB42Px5c2X2zOly7NhRKVe+gjw7bLhUrVbN7WZlehs3rJf/zJou27f9IseOHpXXJ0yUO5s2c7tZsODc8YZff/5Rlix6X37fHSmnTxyT3sNelpr1Gtuec+CPvfLxrEnmuXFxcVK0eCl5Ymi4hBUs7Fq7M5vH7iwlg+8uL7O++01e/GyHWff+47dJndL5bc/7ICJKRiza5lIrwd81uImSJI9a8vVX8tor4dLryd4yf+FiKV++gjzRq4ccP37c7aZletEXLki5chXk2edGuN0UJINzxztioi9I8VJlpfPjg5LdfuTgPnl5SC8pXKykDB43WUa9/b78q2M3yZotW7q3NbOqWiyPdKxbXLYfOHPZtvlr/5B6Y1YkLq98GelKG8HfNTcEuLh4EQGDR82ZPVPa3/eAtG3XQUqXKSPPjxwtISEh8smij91uWqbXoGEj6f1UP2nStLnbTUEyOHe8o2rt+tLu4cfllnp3JLt98ZypUrVWfbm/W18pUbq8FCxSTGrUaSR58tqvbOP6yJEtSF5/qLo8/9EvcubCpcu2R1+Mk2N/xiYuZ2PiXGkn+LsG9xEweNDF2FhT7lK3Xv3EdYGBgVK3bn3Z8tOPrrYN8DLOHf8RHx8vWzaskUI3lpDxI56W/l1ayYsDu8uPEavcblqmMbJdJfl2+1FZszP5q9StaxaVdaOayJcDG8jAVuUkJCtdBjfwd80dgQEBri1exNnvQSdPnTS1vGFhYbb1+vjYsWOutQvwOs4d//Hn6ZMSc+G8fP3Rf6TyLXWl/5g3pWbdO2Ry+LMSuXWT283L8O6pXlgq35hHXvv612S3f/7jARn4wRZ5eOoP8s6KPdL2lqLyeifq5d3A3zV4gacGPZ87d04WLFggu3btkiJFikinTp0uO0GSiomJMYuVLyhYgoODr3NrAQBXyxcfb/7VEqQWbTuZn0vcXE5279giq5YslvJVb3G5hRlX4dAQeb5NRXl02nqJvfTXcUjqw3X7En/+9dBZOXImRuY8fpuUCMsuUccvpGNrAUhmzzBUqlRJTpw4YX7+448/pEqVKtK/f39ZtmyZjBw50mzfu3fvFfcRHh4uoaGhtuXVl8PFn+XLm0+CgoIuG8ykjwsUKOBauwCv49zxH7ny5DXHqmiJm2zrixS/SY4fPeRauzKDKsXySIHcwfLJ0/Vl+0stzKIzIj3SoKT5OTCZioifok6bf0uE5Uz/Bmdy/F1zB4OePRQw7NixQy5d+mug1dChQ6Vo0aLy+++/yw8//GD+rVatmjz33HNX3If+3unTp23L4CFDxZ/pDCEVK1WWdWsjbPW+69ZFSLXqNV1tG+BlnDv+I0vWrHJT2UpyaF+Ubf3h/X9I2A1FXGtXZhCx67jc/dr30nr8msRlyx+n5bMfD5if432X/07FG3Obf4/+GZ3+Dc7k+LsGL/BMSVJERIRMnTrVZAhUrly5ZPTo0dKxY8cr/p6WHiUtP4q+fLIHv/Nw124yfNgQqVy5ilSpWk3enzNbLly4IG3btXe7aZne+fPn5I+ovzs5+/fvk8gd2yVPaKgUKVLU1baBc8dLoi+cN1OnJjh6+IBE7flVcubKY+6z0LJ9Z3nnleelXJUaUr5qLfll01r56YfvZfC4Sa62O6M7FxMnOw+fta27EBsnp85fNOu17OjemkXNgGhdV75ILnmudUX5YfcJiTxo/z2kD/6uucCrl/oza8AQ8L/R4NHR0WbcgtWNN94oR48elczorlZ3y8kTJ2TyxLfMTVrKV6gok995T8JIP7pu2y8/y2PduyY+fuPVl8y/97ZuK6Nf/OtnuIdzxzt+27VdXhvWO/Hxgulvmn/rN7lbuvcfYaZbffjJIfLVwtnywbvjpfCNJcxN28pWruFiqxF7ySf1y4RJ19tLmqlXD56KlqVbD8nkb3a73bRMi79rcFuAz+dLJvmYPnRaMB23kCVLFtm5c6fMmjVLOnTokLh99erV8tBDD8m+fX9foUqJjJBhyMjikst3wxOCkitehies33PS7SbgCrpPW+d2E+Bga/hdbjcBDkJcv2ztbO3uU669dt3SecVrXD1UOrDZSsuQrD7//HNp2LBhOrcKAAAAmVkANUneDRiSevXVV9OtLQAAAAAu5+FkEAAAAJD+PHrDZddwp2cAAAAAjsgwAAAAABYkGOzIMAAAAABwRMAAAAAAwBElSQAAAIAVNUk2ZBgAAAAAOCLDAAAAAFhw4zY7MgwAAAAAHBEwAAAAAHBESRIAAABgwZ2e7cgwAAAAAHBEhgEAAACwIMFgR4YBAAAAgCMyDAAAAIAVKQYbMgwAAAAAHBEwAAAAAHBESRIAAABgwZ2e7cgwAAAAAHBEhgEAAACw4MZtdmQYAAAAADgiYAAAAADgiJIkAAAAwIKKJDsyDAAAAAAckWEAAAAArEgx2JBhAAAAAOCIDAMAAABgwY3b7MgwAAAAAHBEwAAAAADAESVJAAAAgAV3erYjwwAAAADAERkGAAAAwIIEgx0ZBgAAAACOCBgAAAAAOKIkCQAAALCiJsmGDAMAAAAAR2QYAAAAAAvu9GxHhgEAAACAIzIMAAAAgAU3brMjwwAAAADAEQEDAAAAAEeUJAEAAAAWVCTZkWEAAAAA/NDq1avl3nvvlaJFi0pAQIB88skntu0+n09GjBghRYoUkezZs0uzZs1k586dqX4dAgYAAAAgaYrBrSUVzp07J9WrV5dJkyYlu/2VV16Rt956S6ZOnSrr1q2TnDlzSsuWLSU6Ojo1LyMBPg09MpjoS263APBPcfEZ7s9BhhEUSILcyzh3vKv1O2vdbgIcLO9bz+0mONp+8Jxrr12xSM6r+j3NMCxevFjatm1rHmsXXzMPAwcOlEGDBpl1p0+flkKFCsmsWbOkY8eOKd43GQYAAADAI2JiYuTMmTO2Rdel1t69e+XQoUOmDClBaGio1KlTRyIiIlK1LwIGAAAAIMmdnt36Lzw83HTsrYuuSy0NFpRmFKz0ccK2lGKWJAAAAMAjhg4dKgMGDLCtCw4OFjcRMAAAAAAeudNzcHBwmgQIhQsXNv8ePnzYzJKUQB/XqFEjVfuiJAkAAADIYEqVKmWChuXLlyeu0/EQOltSvXqpG3BOhgEAAACw8Jd56c6ePSu7du2yDXTevHmz5M+fX0qUKCH9+vWTsWPHStmyZU0AMXz4cDNzUsJMSilFwAAAAAD4oQ0bNsidd96Z+Dhh7EPXrl3N1KnPPPOMuVfDY489JqdOnZLbb79dlixZIiEhIal6He7DACARc8l7F/dh8DbOHe/iPgze5eX7MPx66Lxrr12ucA7xGjIMAAAAgBXXaGwY9AwAAADAERkGAAAAwEJvoIa/kWEAAAAA4IiAAQAAAIAjSpIAAAAAj9zp2YvIMAAAAABwRIYBAAAAsCDBYEeGAQAAAIAjAgYAAAAAjihJAgAAAKyoSbIhwwAAAADAERkGAAAAwII7PduRYQAAAADgiAwDAAAAYMGN2+zIMAAAAABwRMAAAAAAwBElSQAAAIAFFUl2ZBgAAAAAOCLDAAAAAFiRYrAhwwAAAADAEQEDAAAAAEeUJAEAAAAW3OnZjgwDAAAAAEdkGAAAAAAL7vRsR4YBAAAAgCMyDAAAAIAFCQY7MgwAAAAAHBEwAAAAAHBESRIAAABgwaBnOzIMAAAAAByRYQAAAABsSDFYkWEAAAAA4IiAAQAAAIAjSpIAAAAACwY925FhAAAAAOCIDAMAAABgQYLBjgyDh82fN1daNW8it9asKp073i9bt2xxu0mw4Ph408YN6+XpPo9LiyYN5ZaqFWTl8m/cbhIsOG+8ifPGO+6tUkimdaomn/W61Sxv31dFbiuZN3F7vhxZ5dnmZWRh91ryxeO3ydQHq0rD0vldbTMyPgIGj1ry9Vfy2ivh0uvJ3jJ/4WIpX76CPNGrhxw/ftztpoHj42nRFy5IuXIV5NnnRrjdFCTBeeNdnDfecexsrExbEyVPzN8qT364VX7cd1rG3FNeSubPbrZrsFA8X3Z5/stI6TnvJ/lu9wkZflc5KVMgh9tNz3BjGNxavIiAwaPmzJ4p7e97QNq26yCly5SR50eOlpCQEPlk0cduNw0cH09r0LCR9H6qnzRp2tztpiAJzhvv4rzxjojfTsoPv5+S/aejZd+paJmx9g+5cDFeKhXObbZXLpxbFv90UCIPn5WDZ2Jk7ob9ci7mkpQrmMvtpiMDI2DwoIuxsbJ92y9St179xHWBgYFSt2592fLTj662DRwf4Gpw3gCpFxggcmfZMAnJGijbDv5p1v1y6E+5s2wByR2cxdTZ6/asWQJl8/4zbjcXGZirg543bdok+fLlk1KlSpnHc+bMkalTp0pUVJSULFlS+vTpIx07drziPmJiYsxi5QsKluDgYPFXJ0+dlLi4OAkLC7Ot18d79+5xrV34C8cHSD3OGyDlSoXlMGMXsmUJlAsX42Tkl5Hy+8kLZtuYr381JUifPHarXIqLl+hL8Wb7gdPRbjc7Qwlg2LN3MgzdunWT3bt3m5/fe+896dWrl9SuXVuee+45ufXWW6Vnz54yY8aMK+4jPDxcQkNDbcurL4en0zsAAABIW3+cvCCPzd8ivRdslc+2HpYhzctIyXx/jWHoVre45AoOkkGLf5EnFmyVjzYflBGtypkgA8iQGYadO3dK2bJlzc+TJ0+WN9980wQJCTRoePHFF6V79+6O+xg6dKgMGDDgsgyDP8uXN58EBQVdNhBQHxcoUMC1duEvHB8g9ThvgJS7FO9LzBjsPHpOyhfKKe1rFJH5G/dLu+pFpPvczfL7ib8yDnuOnZeqRXNLm6qFZMK3e11ueQZCgsE7GYYcOXLIsWPHzM/79++X2267zba9Tp06snfvlb/8WnqUJ08e2+LP5Ugqa7ZsUrFSZVm3NiJxXXx8vKxbFyHVqtd0tW3g+ABXg/MGuHqBEiBZgwIkJGuQeezz2bfHx+vsOvRwkUEDhlatWsmUKVPMz40bN5aPPvrItn3BggVSpkwZyYwe7tpNFn20QD77ZLHs2b1bxo4ZJRcuXJC27dq73TRwfDzt/PlzErlju1nU/v37zM8HDx5wu2mZHueNd3HeeEePeiVMxqBQ7mBTZqSPqxfLI8sjj0nUyQuy79QF6X/nzVK+UC4pkidY7q9ZRGqVCJX/7jnhdtORgQX4fEnj1PRz4MABadCggZQoUcKMXdDgoVatWlKxYkWJjIyUtWvXyuLFi+Xuu+9O1X6jL0mG8MHc92X2zOly7NhRKV+hogwZ9rxUq1bd7WYhAx+fuHjX/hykmQ3r18lj3btetv7e1m1l9Isvib8K0ulSMoCMeN5khHMno543qvU7a8WfDGpSWmoWzyP5c2aTczFxsuf4Oflw4wHZ+Mdps/3G0BD5d/2/ggrNOGjp0oJNB+SbyL8qNvzJ8r71xKsOn7no2msXypNVvMbVgEGdOnVKXnrpJfn8889lz549JkVdpEgRE0j079/fBBKplVECBiC9+XunJyPLKAFDRsW5413+FjBkJgQMySNgSCcEDMDVodPjXQQM3sa5410EDN7l5YDhyJ/uBQwFc3svYODGbQAAAAC8Oa0qAAAA4DXcuM2ODAMAAAAARwQMAAAAABxRkgQAAABYUZFkQ4YBAAAAgCMyDAAAAIAFCQY7MgwAAAAAHBEwAAAAAHBESRIAAABgEUBNkg0ZBgAAAACOyDAAAAAAFtzp2Y4MAwAAAABHZBgAAAAAC8Yw2JFhAAAAAOCIgAEAAACAIwIGAAAAAI4IGAAAAAA4YtAzAAAAYMGgZzsyDAAAAAAcETAAAAAAcERJEgAAAGDBnZ7tyDAAAAAAcESGAQAAALBg0LMdGQYAAAAAjsgwAAAAABYkGOzIMAAAAABwRMAAAAAAwBElSQAAAIAVNUk2ZBgAAAAAOCLDAAAAAFhw4zY7MgwAAAAAHBEwAAAAAHBESRIAAABgwZ2e7cgwAAAAAHBEhgEAAACwIMFgR4YBAAAAgCMCBgAAAACOKEkCAAAArKhJsiHDAAAAAMARGQYAAADAgjs925FhAAAAAPzUpEmT5KabbpKQkBCpU6eO/PDDD2n+GgQMAAAAQJIbt7m1pMaHH34oAwYMkJEjR8qmTZukevXq0rJlSzly5IikJQIGAAAAwA+98cYb0rNnT+nWrZtUqlRJpk6dKjly5JAZM2ak6esQMAAAAAAeERMTI2fOnLEtui6p2NhY2bhxozRr1ixxXWBgoHkcERGRpm3KkIOeQzLQu9IvSHh4uAwdOlSCg4Pdbg4y/LHJOIO8MubxyRgy5rHJGOdORjw2y/vWk4wiIx4fr3KzLzlqbLiMHj3atk5LjkaNGmVbd+zYMYmLi5NChQrZ1uvjHTt2pGmbAnw+ny9N94g0pVFlaGionD59WvLkyeN2c2DBsfE2jo93cWy8i2PjbRyfzCEmJuayjIIGiEmDxAMHDsiNN94oa9askXr1/g6Mn3nmGVm1apWsW7cuzdqUga7FAwAAAP4tOJngIDkFChSQoKAgOXz4sG29Pi5cuHCatokxDAAAAICfyZYtm9SqVUuWL1+euC4+Pt48tmYc0gIZBgAAAMAPDRgwQLp27Sq1a9eW2267TSZMmCDnzp0zsyalJQIGj9OUlA50YXCT93BsvI3j410cG+/i2HgbxwdJPfjgg3L06FEZMWKEHDp0SGrUqCFLliy5bCD0tWLQMwAAAABHjGEAAAAA4IiAAQAAAIAjAgYAAAAAjggYAAAAADgiYPCo1atXy7333itFixaVgIAA+eSTT9xuEv4nPDxcbr31VsmdO7cULFhQ2rZtK5GRkW43CyIyZcoUqVatmrkDqi46D/XXX3/tdrOQjJdeesn8bevXr5/bTYGIjBo1yhwP61KhQgW3m4X/2b9/v3Tp0kXCwsIke/bsUrVqVdmwYYPbzUImQsDgUTqHbvXq1WXSpEluNwVJ6O3We/fuLWvXrpVly5bJxYsXpUWLFuaYwV3FihUzHdGNGzea/5k2adJE2rRpI7/88ovbTYPF+vXr5Z133jHBHbyjcuXKcvDgwcTl+++/d7tJEJGTJ09KgwYNJGvWrOYCyLZt2+T111+XfPnyud00ZCLch8GjWrVqZRZ4j85vbDVr1iyTadBOaqNGjVxrF8Rk5axefPFFk3XQ4E47Q3Df2bNnpXPnzjJt2jQZO3as282BRZYsWaRw4cJuNwNJvPzyy1K8eHGZOXNm4rpSpUq52iZkPmQYgGt0+vRp82/+/Pndbgos4uLiZP78+Sbzo6VJ8AbNzt1zzz3SrFkzt5uCJHbu3GnKYG+++WYT1EVFRbndJIjIZ599Zu7ie//995uLUzVr1jQBN5CeyDAA1yA+Pt7UYGu6uEqVKm43ByKydetWEyBER0dLrly5ZPHixVKpUiW3mwURE8Bt2rTJlCTBW+rUqWOypeXLlzflSKNHj5aGDRvKzz//bMZrwT179uwxmdIBAwbIsGHDzPnz1FNPSbZs2aRr165uNw+ZBAEDcI1XS/V/qNT6eod2eDZv3mwyPx999JH5H6qOOyFocNcff/whTz/9tBn3ExIS4nZzkIS1BFbHlmgAUbJkSVmwYIH06NHD1bZldnphSjMM48aNM481w6D/35k6dSoBA9INJUnAVerTp4988cUXsnLlSjPYFt6gV93KlCkjtWrVMjNa6eQBb775ptvNyvR0jM+RI0fklltuMbXyumgg99Zbb5mftYQM3pE3b14pV66c7Nq1y+2mZHpFihS57IJHxYoVKRlDuiLDAKSSz+eTvn37mlKXb7/9lsFnfnB1LiYmxu1mZHpNmzY15WJW3bp1M1N3DhkyRIKCglxrG5IfnL579255+OGH3W5Kpqclr0mn7v71119NBghILwQMHv5jbb2ys3fvXlNmoQNrS5Qo4WrbMjstQ5o3b558+umnprb30KFDZn1oaKiZHxvuGTp0qCmt0HPkzz//NMdJg7qlS5e63bRMT8+VpON8cubMaeaVZ/yP+wYNGmRmGdNO6IEDB2TkyJEmiOvUqZPbTcv0+vfvL/Xr1zclSQ888ID88MMP8u6775oFSC8EDB6lc8jfeeediY91sJPSekUdmAb36OAzdccdd9jW65R3jz76qEutgtKSl0ceecQM2tQATmuxNVho3ry5200DPG3fvn0mODh+/LjccMMNcvvtt5vpiPVnuEtvFKoZbb0gMmbMGJPVnjBhgpnJCkgvAT6trwAAAACAZDDoGQAAAIAjAgYAAAAAjggYAAAAADgiYAAAAADgiIABAAAAgCMCBgAAAACOCBgAAAAAOCJgAAAAAOCIgAEArpHe4btt27aJj/Uu4P369Uv3dnz77bcSEBAgp06dSrf36tV2AgDSDgEDgAxJO7baKdUlW7ZsUqZMGRkzZoxcunTpur/2okWL5IUXXvBk5/mmm26SCRMmpMtrAQAyhixuNwAArpe77rpLZs6cKTExMfLVV19J7969JWvWrDJ06NDLnhsbG2sCi7SQP3/+NNkPAABeQIYBQIYVHBwshQsXlpIlS8oTTzwhzZo1k88++8xWWvPiiy9K0aJFpXz58mb9H3/8IQ888IDkzZvXdPzbtGkjv/32W+I+4+LiZMCAAWZ7WFiYPPPMM+Lz+Wyvm7QkSQOWIUOGSPHixU2bNNsxffp0s98777zTPCdfvnwm06DtUvHx8RIeHi6lSpWS7NmzS/Xq1eWjjz6yvY4GQeXKlTPbdT/Wdl4NfW89evRIfE39TN58881knzt69Gi54YYbJE+ePPL444+bgCtBStoOAPAfZBgAZBraeT1+/Hji4+XLl5sO77Jly8zjixcvSsuWLaVevXry3XffSZYsWWTs2LEmU7FlyxaTgXj99ddl1qxZMmPGDKlYsaJ5vHjxYmnSpInj6z7yyCMSEREhb731luk87927V44dO2YCiI8//lg6dOggkZGRpi3aRqUd7vfff1+mTp0qZcuWldWrV0uXLl1MJ71x48YmsGnfvr3Jmjz22GOyYcMGGThw4DV9PtrRL1asmCxcuNAEQ2vWrDH7LlKkiAmirJ9bSEiIKafSIKVbt27m+Rp8paTtAAA/4wOADKhr166+Nm3amJ/j4+N9y5Yt8wUHB/sGDRqUuL1QoUK+mJiYxN+ZM2eOr3z58ub5CXR79uzZfUuXLjWPixQp4nvllVcSt1+8eNFXrFixxNdSjRs39j399NPm58jISE0/mNdPzsqVK832kydPJq6Ljo725ciRw7dmzRrbc3v06OHr1KmT+Xno0KG+SpUq2bYPGTLksn0lVbJkSd/48eN9KdW7d29fhw4dEh/r55Y/f37fuXPnEtdNmTLFlytXLl9cXFyK2p7cewYAeBcZBgAZ1hdffCG5cuUymQO9ev7QQw/JqFGjErdXrVrVNm7hp59+kl27dknu3Llt+4mOjpbdu3fL6dOn5eDBg1KnTp3EbZqFqF279mVlSQk2b94sQUFBqbqyrm04f/68NG/e3LZey35q1qxpft6+fbutHUozI9dq0qRJJnsSFRUlFy5cMK9Zo0YN23M0S5IjRw7b6549e9ZkPfTff2o7AMC/EDAAyLC0rn/KlCkmKNBxCtq5t8qZM6ftsXZ2a9WqJXPnzr1sX1pOczUSSoxSQ9uhvvzyS7nxxhtt23QMxPUyf/58GTRokCmz0iBAA6dXX31V1q1b5/m2AwCuHwIGABmWBgQ6wDilbrnlFvnwww+lYMGCZjxBcrSeXzvQjRo1Mo91mtaNGzea302OZjE0u7Fq1Soz6DqphAyHDjhOUKlSJdO51qv8TpkJHT+RMIA7wdq1a+Va/Pe//5X69evLk08+mbhOMytJaSZGsw8JwZC+rmZydEyGDhT/p7YDAPwLsyQBwP907txZChQoYGZG0kHPOjhZB/Y+9dRTsm/fPvOcp59+Wl566SX55JNPZMeOHaZzfaV7KOh9D7p27Srdu3c3v5OwzwULFpjtOoOTzo6k5VNHjx41V+j1yr5e6e/fv7/Mnj3bdNo3bdokb7/9tnmsdGainTt3yuDBg82A6Xnz5pnB2Cmxf/9+UyplXU6ePGkGKOvg6aVLl8qvv/4qw4cPl/Xr11/2+1pepLMpbdu2zczUNHLkSOnTp48EBgamqO0AAP9CwAAA/6N1+TqjT4kSJcwMRHoVXzvGOoYhIeOgMxE9/PDDJghIKNtp167dFferZVH33XefCS4qVKggPXv2lHPnzpltWrajU5Q+++yzUqhQIdPxVnrjN+2w64xD2g6dqUnLfHSqUqVt1BmWNAjRMQU6I9G4ceNS9D5fe+01M57Auui+e/XqZd73gw8+aMZH6IxS1mxDgqZNm5rgQrMs+tzWrVvbxob8U9sBAP4lQEc+u90IAAAAAN5EhgEAAACAIwIGAAAAAI4IGAAAAAA4ImAAAAAA4IiAAQAAAIAjAgYAAAAAjggYAAAAADgiYAAAAADgiIABAAAAgCMCBgAAAACOCBgAAAAAiJP/BxMKiWg60m3MAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Generate predictions on the test set\n",
    "y_pred_mlp = mlp_classifier.predict(X_test_pca)\n",
    "# Generate the classification report\n",
    "print(classification_report(y_test, y_pred_mlp))\n",
    "# Generate the confusion matrix\n",
    "conf_matrix_mlp = confusion_matrix(y_test, y_pred_mlp)\n",
    "#make a confusion matrix plot\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_mlp, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for MLP Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5976919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best hyperparameters: lr=0.005, dropout=0.5, best_val_loss=0.2737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c83a9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_train_tensor: tensor([0, 1, 2, 3, 4, 5])\n",
      "Unique values in y_val_tensor: tensor([0, 1, 2, 3, 4, 5])\n",
      "Unique values in y_test_tensor: tensor([0, 1, 2, 3, 4, 5])\n",
      "num_classes: 6\n",
      "X_train_tensor has NaNs: tensor(False)\n",
      "X_train_tensor has Infs: tensor(False)\n",
      "X_val_tensor has NaNs: tensor(False)\n",
      "X_val_tensor has Infs: tensor(False)\n",
      "X_test_tensor has NaNs: tensor(False)\n",
      "X_test_tensor has Infs: tensor(False)\n",
      "Using device: cuda\n",
      "Epoch [1/5000], Loss: 1.2158, Val Loss: 0.5063\n",
      "Epoch [2/5000], Loss: 0.4251, Val Loss: 0.3979\n",
      "Epoch [3/5000], Loss: 0.2297, Val Loss: 0.3364\n",
      "Epoch [4/5000], Loss: 0.1554, Val Loss: 0.3384\n",
      "Epoch [5/5000], Loss: 0.0855, Val Loss: 0.3684\n",
      "Epoch [6/5000], Loss: 0.0714, Val Loss: 0.3975\n",
      "Epoch [7/5000], Loss: 0.0576, Val Loss: 0.4253\n",
      "Epoch [8/5000], Loss: 0.0583, Val Loss: 0.5391\n",
      "Epoch [9/5000], Loss: 0.0299, Val Loss: 0.4995\n",
      "Epoch [10/5000], Loss: 0.0337, Val Loss: 0.4890\n",
      "Epoch [11/5000], Loss: 0.0233, Val Loss: 0.5469\n",
      "Epoch [12/5000], Loss: 0.0246, Val Loss: 0.6012\n",
      "Epoch [13/5000], Loss: 0.0182, Val Loss: 0.6572\n",
      "Epoch [14/5000], Loss: 0.0232, Val Loss: 0.6421\n",
      "Epoch [15/5000], Loss: 0.0264, Val Loss: 0.5998\n",
      "Epoch [16/5000], Loss: 0.0113, Val Loss: 0.6007\n",
      "Epoch [17/5000], Loss: 0.0114, Val Loss: 0.6067\n",
      "Epoch [18/5000], Loss: 0.0189, Val Loss: 0.6469\n",
      "Epoch [19/5000], Loss: 0.0198, Val Loss: 0.7053\n",
      "Epoch [20/5000], Loss: 0.0188, Val Loss: 0.7919\n",
      "Epoch [21/5000], Loss: 0.0129, Val Loss: 0.7879\n",
      "Epoch [22/5000], Loss: 0.0116, Val Loss: 0.7261\n",
      "Epoch [23/5000], Loss: 0.0171, Val Loss: 0.7894\n",
      "Epoch [24/5000], Loss: 0.0115, Val Loss: 0.7991\n",
      "Epoch [25/5000], Loss: 0.0173, Val Loss: 0.8145\n",
      "Epoch [26/5000], Loss: 0.0155, Val Loss: 0.7554\n",
      "Epoch [27/5000], Loss: 0.0172, Val Loss: 0.7117\n",
      "Epoch [28/5000], Loss: 0.0374, Val Loss: 0.7530\n",
      "Epoch [29/5000], Loss: 0.0201, Val Loss: 0.6978\n",
      "Epoch [30/5000], Loss: 0.0165, Val Loss: 0.7489\n",
      "Epoch [31/5000], Loss: 0.0112, Val Loss: 0.7713\n",
      "Epoch [32/5000], Loss: 0.0251, Val Loss: 0.8806\n",
      "Epoch [33/5000], Loss: 0.0290, Val Loss: 0.7762\n",
      "Epoch [34/5000], Loss: 0.0205, Val Loss: 0.8376\n",
      "Epoch [35/5000], Loss: 0.0188, Val Loss: 0.8613\n",
      "Epoch [36/5000], Loss: 0.0200, Val Loss: 0.8707\n",
      "Epoch [37/5000], Loss: 0.0308, Val Loss: 0.8213\n",
      "Epoch [38/5000], Loss: 0.0248, Val Loss: 0.8539\n",
      "Epoch [39/5000], Loss: 0.0092, Val Loss: 0.9134\n",
      "Epoch [40/5000], Loss: 0.0079, Val Loss: 0.8642\n",
      "Epoch [41/5000], Loss: 0.0081, Val Loss: 0.8476\n",
      "Epoch [42/5000], Loss: 0.0025, Val Loss: 0.8725\n",
      "Epoch [43/5000], Loss: 0.0090, Val Loss: 0.9546\n",
      "Epoch [44/5000], Loss: 0.0152, Val Loss: 1.1086\n",
      "Epoch [45/5000], Loss: 0.0155, Val Loss: 1.0179\n",
      "Epoch [46/5000], Loss: 0.0449, Val Loss: 0.9572\n",
      "Epoch [47/5000], Loss: 0.0181, Val Loss: 0.9996\n",
      "Epoch [48/5000], Loss: 0.0209, Val Loss: 1.1243\n",
      "Epoch [49/5000], Loss: 0.0362, Val Loss: 1.0253\n",
      "Epoch [50/5000], Loss: 0.0314, Val Loss: 1.2138\n",
      "Epoch [51/5000], Loss: 0.0321, Val Loss: 1.1271\n",
      "Epoch [52/5000], Loss: 0.0357, Val Loss: 0.9555\n",
      "Epoch [53/5000], Loss: 0.0138, Val Loss: 1.0672\n",
      "Epoch [54/5000], Loss: 0.0205, Val Loss: 1.0610\n",
      "Epoch [55/5000], Loss: 0.0143, Val Loss: 1.0941\n",
      "Epoch [56/5000], Loss: 0.0087, Val Loss: 1.0415\n",
      "Epoch [57/5000], Loss: 0.0183, Val Loss: 1.1092\n",
      "Epoch [58/5000], Loss: 0.0146, Val Loss: 1.0148\n",
      "Epoch [59/5000], Loss: 0.0144, Val Loss: 1.0700\n",
      "Epoch [60/5000], Loss: 0.0089, Val Loss: 1.1402\n",
      "Epoch [61/5000], Loss: 0.0198, Val Loss: 1.1530\n",
      "Epoch [62/5000], Loss: 0.0168, Val Loss: 1.4373\n",
      "Epoch [63/5000], Loss: 0.0253, Val Loss: 1.3666\n",
      "Epoch [64/5000], Loss: 0.0144, Val Loss: 1.3103\n",
      "Epoch [65/5000], Loss: 0.0278, Val Loss: 1.2684\n",
      "Epoch [66/5000], Loss: 0.0173, Val Loss: 1.4582\n",
      "Epoch [67/5000], Loss: 0.0104, Val Loss: 1.3941\n",
      "Epoch [68/5000], Loss: 0.0295, Val Loss: 1.5806\n",
      "Epoch [69/5000], Loss: 0.0349, Val Loss: 1.5424\n",
      "Epoch [70/5000], Loss: 0.0159, Val Loss: 1.4194\n",
      "Epoch [71/5000], Loss: 0.0098, Val Loss: 1.4211\n",
      "Epoch [72/5000], Loss: 0.0181, Val Loss: 1.3554\n",
      "Epoch [73/5000], Loss: 0.0492, Val Loss: 1.4520\n",
      "Epoch [74/5000], Loss: 0.0682, Val Loss: 1.6253\n",
      "Epoch [75/5000], Loss: 0.0422, Val Loss: 1.5547\n",
      "Epoch [76/5000], Loss: 0.0416, Val Loss: 1.4439\n",
      "Epoch [77/5000], Loss: 0.0382, Val Loss: 1.3645\n",
      "Epoch [78/5000], Loss: 0.0242, Val Loss: 1.3327\n",
      "Epoch [79/5000], Loss: 0.0296, Val Loss: 1.1857\n",
      "Epoch [80/5000], Loss: 0.0129, Val Loss: 1.3086\n",
      "Epoch [81/5000], Loss: 0.0346, Val Loss: 1.2603\n",
      "Epoch [82/5000], Loss: 0.0531, Val Loss: 1.1604\n",
      "Epoch [83/5000], Loss: 0.0529, Val Loss: 1.3176\n",
      "Epoch [84/5000], Loss: 0.0187, Val Loss: 1.3524\n",
      "Epoch [85/5000], Loss: 0.0159, Val Loss: 1.2534\n",
      "Epoch [86/5000], Loss: 0.0322, Val Loss: 1.3936\n",
      "Epoch [87/5000], Loss: 0.0165, Val Loss: 1.4286\n",
      "Epoch [88/5000], Loss: 0.0269, Val Loss: 1.3791\n",
      "Epoch [89/5000], Loss: 0.0104, Val Loss: 1.3353\n",
      "Epoch [90/5000], Loss: 0.0063, Val Loss: 1.3626\n",
      "Epoch [91/5000], Loss: 0.0169, Val Loss: 1.2797\n",
      "Epoch [92/5000], Loss: 0.0400, Val Loss: 1.2811\n",
      "Epoch [93/5000], Loss: 0.0244, Val Loss: 1.2710\n",
      "Epoch [94/5000], Loss: 0.0284, Val Loss: 1.1832\n",
      "Epoch [95/5000], Loss: 0.0257, Val Loss: 1.1693\n",
      "Epoch [96/5000], Loss: 0.0104, Val Loss: 1.2201\n",
      "Epoch [97/5000], Loss: 0.0102, Val Loss: 1.2606\n",
      "Epoch [98/5000], Loss: 0.0014, Val Loss: 1.3099\n",
      "Epoch [99/5000], Loss: 0.0073, Val Loss: 1.3523\n",
      "Epoch [100/5000], Loss: 0.0094, Val Loss: 1.3951\n",
      "Epoch [101/5000], Loss: 0.0142, Val Loss: 1.3166\n",
      "Epoch [102/5000], Loss: 0.0222, Val Loss: 1.3458\n",
      "Epoch [103/5000], Loss: 0.0029, Val Loss: 1.4042\n",
      "Epoch [104/5000], Loss: 0.0113, Val Loss: 1.4544\n",
      "Epoch [105/5000], Loss: 0.0072, Val Loss: 1.5376\n",
      "Epoch [106/5000], Loss: 0.0118, Val Loss: 1.5575\n",
      "Epoch [107/5000], Loss: 0.0012, Val Loss: 1.5779\n",
      "Epoch [108/5000], Loss: 0.0148, Val Loss: 1.5797\n",
      "Epoch [109/5000], Loss: 0.0236, Val Loss: 1.5336\n",
      "Epoch [110/5000], Loss: 0.0042, Val Loss: 1.4932\n",
      "Epoch [111/5000], Loss: 0.0350, Val Loss: 1.4966\n",
      "Epoch [112/5000], Loss: 0.0229, Val Loss: 1.4104\n",
      "Epoch [113/5000], Loss: 0.0130, Val Loss: 1.5795\n",
      "Epoch [114/5000], Loss: 0.0216, Val Loss: 1.7411\n",
      "Epoch [115/5000], Loss: 0.0470, Val Loss: 1.4870\n",
      "Epoch [116/5000], Loss: 0.0155, Val Loss: 1.4060\n",
      "Epoch [117/5000], Loss: 0.0067, Val Loss: 1.4804\n",
      "Epoch [118/5000], Loss: 0.0084, Val Loss: 1.6445\n",
      "Epoch [119/5000], Loss: 0.0192, Val Loss: 1.6425\n",
      "Epoch [120/5000], Loss: 0.0024, Val Loss: 1.6981\n",
      "Epoch [121/5000], Loss: 0.0217, Val Loss: 1.5827\n",
      "Epoch [122/5000], Loss: 0.0185, Val Loss: 1.5755\n",
      "Epoch [123/5000], Loss: 0.0068, Val Loss: 1.5778\n",
      "Epoch [124/5000], Loss: 0.0175, Val Loss: 1.4217\n",
      "Epoch [125/5000], Loss: 0.0059, Val Loss: 1.4603\n",
      "Epoch [126/5000], Loss: 0.0043, Val Loss: 1.4401\n",
      "Epoch [127/5000], Loss: 0.0021, Val Loss: 1.5648\n",
      "Epoch [128/5000], Loss: 0.0113, Val Loss: 1.5277\n",
      "Epoch [129/5000], Loss: 0.0036, Val Loss: 1.5886\n",
      "Epoch [130/5000], Loss: 0.0053, Val Loss: 1.6159\n",
      "Epoch [131/5000], Loss: 0.0115, Val Loss: 1.5746\n",
      "Epoch [132/5000], Loss: 0.0089, Val Loss: 1.5795\n",
      "Epoch [133/5000], Loss: 0.0044, Val Loss: 1.5650\n",
      "Epoch [134/5000], Loss: 0.0048, Val Loss: 1.5990\n",
      "Epoch [135/5000], Loss: 0.0220, Val Loss: 1.9387\n",
      "Epoch [136/5000], Loss: 0.0134, Val Loss: 1.9367\n",
      "Epoch [137/5000], Loss: 0.0145, Val Loss: 2.0357\n",
      "Epoch [138/5000], Loss: 0.0090, Val Loss: 1.9940\n",
      "Epoch [139/5000], Loss: 0.0057, Val Loss: 1.9915\n",
      "Epoch [140/5000], Loss: 0.0050, Val Loss: 2.1247\n",
      "Epoch [141/5000], Loss: 0.0203, Val Loss: 1.8151\n",
      "Epoch [142/5000], Loss: 0.0223, Val Loss: 1.8713\n",
      "Epoch [143/5000], Loss: 0.0071, Val Loss: 1.9690\n",
      "Epoch [144/5000], Loss: 0.0181, Val Loss: 1.9069\n",
      "Epoch [145/5000], Loss: 0.0018, Val Loss: 1.7982\n",
      "Epoch [146/5000], Loss: 0.0140, Val Loss: 1.8909\n",
      "Epoch [147/5000], Loss: 0.0029, Val Loss: 2.1592\n",
      "Epoch [148/5000], Loss: 0.0181, Val Loss: 1.9320\n",
      "Epoch [149/5000], Loss: 0.0063, Val Loss: 1.7770\n",
      "Epoch [150/5000], Loss: 0.0026, Val Loss: 1.8022\n",
      "Epoch [151/5000], Loss: 0.0016, Val Loss: 1.8054\n",
      "Epoch [152/5000], Loss: 0.0081, Val Loss: 1.7402\n",
      "Epoch [153/5000], Loss: 0.0010, Val Loss: 1.6576\n",
      "Epoch [154/5000], Loss: 0.0011, Val Loss: 1.6380\n",
      "Epoch [155/5000], Loss: 0.0164, Val Loss: 1.6435\n",
      "Epoch [156/5000], Loss: 0.0069, Val Loss: 1.7225\n",
      "Epoch [157/5000], Loss: 0.0152, Val Loss: 1.9712\n",
      "Epoch [158/5000], Loss: 0.0095, Val Loss: 1.9015\n",
      "Epoch [159/5000], Loss: 0.0121, Val Loss: 1.8569\n",
      "Epoch [160/5000], Loss: 0.0148, Val Loss: 1.8080\n",
      "Epoch [161/5000], Loss: 0.0079, Val Loss: 1.8858\n",
      "Epoch [162/5000], Loss: 0.0165, Val Loss: 1.6672\n",
      "Epoch [163/5000], Loss: 0.0072, Val Loss: 1.7309\n",
      "Epoch [164/5000], Loss: 0.0062, Val Loss: 1.7029\n",
      "Epoch [165/5000], Loss: 0.0071, Val Loss: 1.7434\n",
      "Epoch [166/5000], Loss: 0.0031, Val Loss: 1.7816\n",
      "Epoch [167/5000], Loss: 0.0052, Val Loss: 1.7065\n",
      "Epoch [168/5000], Loss: 0.0186, Val Loss: 1.6583\n",
      "Epoch [169/5000], Loss: 0.0073, Val Loss: 1.5577\n",
      "Epoch [170/5000], Loss: 0.0210, Val Loss: 1.5726\n",
      "Epoch [171/5000], Loss: 0.0010, Val Loss: 1.6615\n",
      "Epoch [172/5000], Loss: 0.0073, Val Loss: 1.7142\n",
      "Epoch [173/5000], Loss: 0.0108, Val Loss: 1.7530\n",
      "Epoch [174/5000], Loss: 0.0128, Val Loss: 1.6419\n",
      "Epoch [175/5000], Loss: 0.0064, Val Loss: 1.6513\n",
      "Epoch [176/5000], Loss: 0.0131, Val Loss: 1.7721\n",
      "Epoch [177/5000], Loss: 0.0124, Val Loss: 1.9494\n",
      "Epoch [178/5000], Loss: 0.0179, Val Loss: 1.8611\n",
      "Epoch [179/5000], Loss: 0.0030, Val Loss: 2.0014\n",
      "Epoch [180/5000], Loss: 0.0146, Val Loss: 1.9860\n",
      "Epoch [181/5000], Loss: 0.0097, Val Loss: 1.9046\n",
      "Epoch [182/5000], Loss: 0.0017, Val Loss: 2.0064\n",
      "Epoch [183/5000], Loss: 0.0415, Val Loss: 1.7593\n",
      "Epoch [184/5000], Loss: 0.0028, Val Loss: 1.6389\n",
      "Epoch [185/5000], Loss: 0.0028, Val Loss: 1.5827\n",
      "Epoch [186/5000], Loss: 0.0184, Val Loss: 1.5679\n",
      "Epoch [187/5000], Loss: 0.0108, Val Loss: 1.5341\n",
      "Epoch [188/5000], Loss: 0.0284, Val Loss: 1.5371\n",
      "Epoch [189/5000], Loss: 0.0023, Val Loss: 1.5369\n",
      "Epoch [190/5000], Loss: 0.0275, Val Loss: 1.5485\n",
      "Epoch [191/5000], Loss: 0.0059, Val Loss: 1.5566\n",
      "Epoch [192/5000], Loss: 0.0071, Val Loss: 1.5586\n",
      "Epoch [193/5000], Loss: 0.0074, Val Loss: 1.6113\n",
      "Epoch [194/5000], Loss: 0.0414, Val Loss: 1.6692\n",
      "Epoch [195/5000], Loss: 0.0040, Val Loss: 1.5400\n",
      "Epoch [196/5000], Loss: 0.0099, Val Loss: 1.7431\n",
      "Epoch [197/5000], Loss: 0.0019, Val Loss: 1.7645\n",
      "Epoch [198/5000], Loss: 0.0071, Val Loss: 1.7911\n",
      "Epoch [199/5000], Loss: 0.0105, Val Loss: 1.8152\n",
      "Epoch [200/5000], Loss: 0.0046, Val Loss: 1.8057\n",
      "Epoch [201/5000], Loss: 0.0003, Val Loss: 1.8255\n",
      "Epoch [202/5000], Loss: 0.0015, Val Loss: 1.8340\n",
      "Epoch [203/5000], Loss: 0.0102, Val Loss: 2.0580\n",
      "Early stopping triggered\n",
      "Loading best model from best_model.pth\n",
      "Test Loss: 0.3520, Test Accuracy: 0.8661\n",
      "Final model state saved to final_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\AppData\\Local\\Temp\\ipykernel_3512\\3230057143.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd # Assuming X_train_pca is a pandas DataFrame\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Encode labels to start from 0 using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Fit on all unique labels across all sets to ensure consistency\n",
    "le.fit(np.unique(np.concatenate([y_train, y_val, y_test])))\n",
    "\n",
    "# Transform all label sets\n",
    "y_train_tensor = torch.tensor(le.transform(y_train), dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(le.transform(y_val), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(le.transform(y_test), dtype=torch.long)\n",
    "\n",
    "# Define number of classes\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Debug: Check label ranges\n",
    "print(\"Unique values in y_train_tensor:\", torch.unique(y_train_tensor))\n",
    "print(\"Unique values in y_val_tensor:\", torch.unique(y_val_tensor))\n",
    "print(\"Unique values in y_test_tensor:\", torch.unique(y_test_tensor))\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "# Assertions for label range (already present, good practice)\n",
    "assert torch.all(y_train_tensor >= 0) and torch.all(y_train_tensor < num_classes), \\\n",
    "    f\"y_train_tensor has out-of-range labels. Min: {y_train_tensor.min()}, Max: {y_train_tensor.max()}, Expected range [0, {num_classes-1}]\"\n",
    "assert torch.all(y_val_tensor >= 0) and torch.all(y_val_tensor < num_classes), \\\n",
    "    f\"y_val_tensor has out-of-range labels. Min: {y_val_tensor.min()}, Max: {y_val_tensor.max()}, Expected range [0, {num_classes-1}]\"\n",
    "assert torch.all(y_test_tensor >= 0) and torch.all(y_test_tensor < num_classes), \\\n",
    "    f\"y_test_tensor has out-of-range labels. Min: {y_test_tensor.min()}, Max: {y_test_tensor.max()}, Expected range [0, {num_classes-1}]\"\n",
    "\n",
    "\n",
    "# Convert PCA features to PyTorch tensors\n",
    "# Ensure .values is used if X_pca is a pandas DataFrame\n",
    "X_train_tensor = torch.tensor(X_train_pca.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_pca.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_pca.values, dtype=torch.float32)\n",
    "\n",
    "# Check for NaNs/Infs in input tensors\n",
    "print(\"X_train_tensor has NaNs:\", torch.isnan(X_train_tensor).any())\n",
    "print(\"X_train_tensor has Infs:\", torch.isinf(X_train_tensor).any())\n",
    "print(\"X_val_tensor has NaNs:\", torch.isnan(X_val_tensor).any())\n",
    "print(\"X_val_tensor has Infs:\", torch.isinf(X_val_tensor).any())\n",
    "print(\"X_test_tensor has NaNs:\", torch.isnan(X_test_tensor).any())\n",
    "print(\"X_test_tensor has Infs:\", torch.isinf(X_test_tensor).any())\n",
    "\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x) # No activation here, as CrossEntropyLoss expects logits\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = X_train_pca.shape[1]\n",
    "model = SimpleNN(input_size, num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 5000\n",
    "patience = 200\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "best_model_path = 'best_model.pth' # Define path for saving best model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)  # Save best model\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Load best model for evaluation\n",
    "print(f\"Loading best model from {best_model_path}\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "with torch.no_grad():\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(X_test_tensor)\n",
    "test_accuracy = test_correct / len(X_test_tensor)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Save final model (optional, often you'd just use the best_model.pth for deployment)\n",
    "final_model_path = 'final_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model state saved to {final_model_path}\")\n",
    "\n",
    "# Load model for inference (example)\n",
    "# loaded_model = SimpleNN(input_size, num_classes)\n",
    "# loaded_model.load_state_dict(torch.load('final_model.pth'))\n",
    "# loaded_model.to(device) # Don't forget to move it to device if you want to use it on GPU\n",
    "# loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f37002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_train_tensor: tensor([0, 1, 2, 3, 4, 5])\n",
      "Unique values in y_val_tensor: tensor([0, 1, 2, 3, 4, 5])\n",
      "Unique values in y_test_tensor: tensor([0, 1, 2, 3, 4, 5])\n",
      "num_classes: 6\n",
      "X_train_tensor has NaNs: tensor(False)\n",
      "X_train_tensor has Infs: tensor(False)\n",
      "X_val_tensor has NaNs: tensor(False)\n",
      "X_val_tensor has Infs: tensor(False)\n",
      "X_test_tensor has NaNs: tensor(False)\n",
      "X_test_tensor has Infs: tensor(False)\n",
      "Using device: cuda\n",
      "Epoch [1/5000], Loss: 1.5371, Val Loss: 1.5953\n",
      "Epoch [2/5000], Loss: 1.0769, Val Loss: 0.8391\n",
      "Epoch [3/5000], Loss: 0.7855, Val Loss: 0.5948\n",
      "Epoch [4/5000], Loss: 0.5814, Val Loss: 0.4689\n",
      "Epoch [5/5000], Loss: 0.4356, Val Loss: 0.3963\n",
      "Epoch [6/5000], Loss: 0.3784, Val Loss: 0.4084\n",
      "Epoch [7/5000], Loss: 0.3008, Val Loss: 0.3599\n",
      "Epoch [8/5000], Loss: 0.2241, Val Loss: 0.4148\n",
      "Epoch [9/5000], Loss: 0.2110, Val Loss: 0.3493\n",
      "Epoch [10/5000], Loss: 0.1736, Val Loss: 0.3445\n",
      "Epoch [11/5000], Loss: 0.1475, Val Loss: 0.3579\n",
      "Epoch [12/5000], Loss: 0.1785, Val Loss: 0.3572\n",
      "Epoch [13/5000], Loss: 0.1932, Val Loss: 0.3812\n",
      "Epoch [14/5000], Loss: 0.1521, Val Loss: 0.3695\n",
      "Epoch [15/5000], Loss: 0.1313, Val Loss: 0.3749\n",
      "Epoch [16/5000], Loss: 0.1077, Val Loss: 0.3978\n",
      "Epoch [17/5000], Loss: 0.1077, Val Loss: 0.4056\n",
      "Epoch [18/5000], Loss: 0.1226, Val Loss: 0.3992\n",
      "Epoch [19/5000], Loss: 0.1041, Val Loss: 0.3721\n",
      "Epoch [20/5000], Loss: 0.0956, Val Loss: 0.4338\n",
      "Epoch [21/5000], Loss: 0.0930, Val Loss: 0.4093\n",
      "Epoch [22/5000], Loss: 0.0930, Val Loss: 0.3869\n",
      "Epoch [23/5000], Loss: 0.0775, Val Loss: 0.3956\n",
      "Epoch [24/5000], Loss: 0.0743, Val Loss: 0.4074\n",
      "Epoch [25/5000], Loss: 0.0866, Val Loss: 0.4298\n",
      "Epoch [26/5000], Loss: 0.0634, Val Loss: 0.4557\n",
      "Epoch [27/5000], Loss: 0.0686, Val Loss: 0.4433\n",
      "Epoch [28/5000], Loss: 0.0770, Val Loss: 0.3707\n",
      "Epoch [29/5000], Loss: 0.0720, Val Loss: 0.3935\n",
      "Epoch [30/5000], Loss: 0.1224, Val Loss: 0.4293\n",
      "Epoch [31/5000], Loss: 0.1428, Val Loss: 0.4167\n",
      "Epoch [32/5000], Loss: 0.1166, Val Loss: 0.4009\n",
      "Epoch [33/5000], Loss: 0.0860, Val Loss: 0.3915\n",
      "Epoch [34/5000], Loss: 0.0689, Val Loss: 0.4000\n",
      "Epoch [35/5000], Loss: 0.0713, Val Loss: 0.4220\n",
      "Epoch [36/5000], Loss: 0.0581, Val Loss: 0.4683\n",
      "Epoch [37/5000], Loss: 0.0658, Val Loss: 0.4920\n",
      "Epoch [38/5000], Loss: 0.0556, Val Loss: 0.4595\n",
      "Epoch [39/5000], Loss: 0.0700, Val Loss: 0.5196\n",
      "Epoch [40/5000], Loss: 0.0739, Val Loss: 0.5172\n",
      "Epoch [41/5000], Loss: 0.0922, Val Loss: 0.4920\n",
      "Epoch [42/5000], Loss: 0.0706, Val Loss: 0.5365\n",
      "Epoch [43/5000], Loss: 0.0669, Val Loss: 0.4951\n",
      "Epoch [44/5000], Loss: 0.0735, Val Loss: 0.5054\n",
      "Epoch [45/5000], Loss: 0.0750, Val Loss: 0.5041\n",
      "Epoch [46/5000], Loss: 0.0782, Val Loss: 0.4614\n",
      "Epoch [47/5000], Loss: 0.0656, Val Loss: 0.4806\n",
      "Epoch [48/5000], Loss: 0.0620, Val Loss: 0.4937\n",
      "Epoch [49/5000], Loss: 0.0379, Val Loss: 0.4620\n",
      "Epoch [50/5000], Loss: 0.0464, Val Loss: 0.4778\n",
      "Epoch [51/5000], Loss: 0.0480, Val Loss: 0.4751\n",
      "Epoch [52/5000], Loss: 0.0621, Val Loss: 0.4638\n",
      "Epoch [53/5000], Loss: 0.0581, Val Loss: 0.4627\n",
      "Epoch [54/5000], Loss: 0.0600, Val Loss: 0.5364\n",
      "Epoch [55/5000], Loss: 0.0687, Val Loss: 0.5119\n",
      "Epoch [56/5000], Loss: 0.0356, Val Loss: 0.4848\n",
      "Epoch [57/5000], Loss: 0.0416, Val Loss: 0.4936\n",
      "Epoch [58/5000], Loss: 0.0413, Val Loss: 0.5254\n",
      "Epoch [59/5000], Loss: 0.0536, Val Loss: 0.4573\n",
      "Epoch [60/5000], Loss: 0.0744, Val Loss: 0.4783\n",
      "Early stopping triggered\n",
      "Loading best model from best_model.pth\n",
      "Test Loss: 0.3427, Test Accuracy: 0.8889\n",
      "Final model state saved to final_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\AppData\\Local\\Temp\\ipykernel_3512\\457460344.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode labels to start from 0 using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(np.unique(np.concatenate([y_train, y_val, y_test])))\n",
    "y_train_tensor = torch.tensor(le.transform(y_train), dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(le.transform(y_val), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(le.transform(y_test), dtype=torch.long)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Debug: Check label ranges\n",
    "print(\"Unique values in y_train_tensor:\", torch.unique(y_train_tensor))\n",
    "print(\"Unique values in y_val_tensor:\", torch.unique(y_val_tensor))\n",
    "print(\"Unique values in y_test_tensor:\", torch.unique(y_test_tensor))\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "# Assertions for label range\n",
    "assert torch.all(y_train_tensor >= 0) and torch.all(y_train_tensor < num_classes), \\\n",
    "    f\"y_train_tensor has out-of-range labels. Min: {y_train_tensor.min()}, Max: {y_train_tensor.max()}, Expected range [0, {num_classes-1}]\"\n",
    "assert torch.all(y_val_tensor >= 0) and torch.all(y_val_tensor < num_classes), \\\n",
    "    f\"y_val_tensor has out-of-range labels. Min: {y_val_tensor.min()}, Max: {y_val_tensor.max()}, Expected range [0, {num_classes-1}]\"\n",
    "assert torch.all(y_test_tensor >= 0) and torch.all(y_test_tensor < num_classes), \\\n",
    "    f\"y_test_tensor has out-of-range labels. Min: {y_test_tensor.min()}, Max: {y_test_tensor.max()}, Expected range [0, {num_classes-1}]\"\n",
    "\n",
    "# Convert PCA features to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_pca.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_pca.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_pca.values, dtype=torch.float32)\n",
    "\n",
    "# Check for NaNs/Infs in input tensors\n",
    "print(\"X_train_tensor has NaNs:\", torch.isnan(X_train_tensor).any())\n",
    "print(\"X_train_tensor has Infs:\", torch.isinf(X_train_tensor).any())\n",
    "print(\"X_val_tensor has NaNs:\", torch.isnan(X_val_tensor).any())\n",
    "print(\"X_val_tensor has Infs:\", torch.isinf(X_val_tensor).any())\n",
    "print(\"X_test_tensor has NaNs:\", torch.isnan(X_test_tensor).any())\n",
    "print(\"X_test_tensor has Infs:\", torch.isinf(X_test_tensor).any())\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the improved neural network architecture\n",
    "class ImprovedNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)  # Additional layer\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)  # Increased dropout rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)  # No activation here, as CrossEntropyLoss expects logits\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = X_train_pca.shape[1]\n",
    "model = ImprovedNN(input_size, num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 5000\n",
    "patience = 50\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Load best model for evaluation\n",
    "print(f\"Loading best model from {best_model_path}\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "with torch.no_grad():\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(X_test_tensor)\n",
    "test_accuracy = test_correct / len(X_test_tensor)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Save final model (optional)\n",
    "final_model_path = 'final_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model state saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36e84466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "|   iter    |  target   | batch_... | dropou... | layer1... | layer2... | layer3... | learni... | optimi... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-0.3309  \u001b[39m | \u001b[39m201.8    \u001b[39m | \u001b[39m0.8556   \u001b[39m | \u001b[39m1.133e+03\u001b[39m | \u001b[39m932.4    \u001b[39m | \u001b[39m266.7    \u001b[39m | \u001b[39m0.0156   \u001b[39m | \u001b[39m0.1162   \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m-0.573   \u001b[39m | \u001b[39m445.6    \u001b[39m | \u001b[39m0.541    \u001b[39m | \u001b[39m1.097e+03\u001b[39m | \u001b[39m62.96    \u001b[39m | \u001b[39m1.491e+03\u001b[39m | \u001b[39m0.08324  \u001b[39m | \u001b[39m0.4247   \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m-0.4129  \u001b[39m | \u001b[39m106.2    \u001b[39m | \u001b[39m0.1651   \u001b[39m | \u001b[39m489.6    \u001b[39m | \u001b[39m821.2    \u001b[39m | \u001b[39m681.6    \u001b[39m | \u001b[39m0.02912  \u001b[39m | \u001b[39m1.224    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-0.3777  \u001b[39m | \u001b[39m85.19    \u001b[39m | \u001b[39m0.2629   \u001b[39m | \u001b[39m583.0    \u001b[39m | \u001b[39m717.9    \u001b[39m | \u001b[39m1.213e+03\u001b[39m | \u001b[39m0.01997  \u001b[39m | \u001b[39m1.028    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-0.4025  \u001b[39m | \u001b[39m309.8    \u001b[39m | \u001b[39m0.04181  \u001b[39m | \u001b[39m945.7    \u001b[39m | \u001b[39m288.5    \u001b[39m | \u001b[39m129.8    \u001b[39m | \u001b[39m0.09489  \u001b[39m | \u001b[39m1.931    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m-0.4884  \u001b[39m | \u001b[39m417.0    \u001b[39m | \u001b[39m0.2742   \u001b[39m | \u001b[39m178.9    \u001b[39m | \u001b[39m1.061e+03\u001b[39m | \u001b[39m694.0    \u001b[39m | \u001b[39m0.0122   \u001b[39m | \u001b[39m0.9904   \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-0.3322  \u001b[39m | \u001b[39m33.06    \u001b[39m | \u001b[39m0.8184   \u001b[39m | \u001b[39m421.2    \u001b[39m | \u001b[39m1.028e+03\u001b[39m | \u001b[39m500.8    \u001b[39m | \u001b[39m0.05201  \u001b[39m | \u001b[39m1.093    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m-0.3857  \u001b[39m | \u001b[39m107.7    \u001b[39m | \u001b[39m0.8726   \u001b[39m | \u001b[39m1.198e+03\u001b[39m | \u001b[39m1.445e+03\u001b[39m | \u001b[39m1.378e+03\u001b[39m | \u001b[39m0.05979  \u001b[39m | \u001b[39m1.844    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m-0.3602  \u001b[39m | \u001b[39m59.89    \u001b[39m | \u001b[39m0.1764   \u001b[39m | \u001b[39m100.0    \u001b[39m | \u001b[39m521.3    \u001b[39m | \u001b[39m616.6    \u001b[39m | \u001b[39m0.02713  \u001b[39m | \u001b[39m1.657    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m-0.3791  \u001b[39m | \u001b[39m192.9    \u001b[39m | \u001b[39m0.2528   \u001b[39m | \u001b[39m848.2    \u001b[39m | \u001b[39m244.0    \u001b[39m | \u001b[39m1.239e+03\u001b[39m | \u001b[39m0.007455 \u001b[39m | \u001b[39m1.974    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m-0.3933  \u001b[39m | \u001b[39m62.42    \u001b[39m | \u001b[39m0.3553   \u001b[39m | \u001b[39m751.1    \u001b[39m | \u001b[39m1.043e+03\u001b[39m | \u001b[39m1.016e+03\u001b[39m | \u001b[39m0.09063  \u001b[39m | \u001b[39m1.677    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m-0.4619  \u001b[39m | \u001b[39m47.93    \u001b[39m | \u001b[39m0.09807  \u001b[39m | \u001b[39m1.38e+03 \u001b[39m | \u001b[39m1.31e+03 \u001b[39m | \u001b[39m1.163e+03\u001b[39m | \u001b[39m0.02789  \u001b[39m | \u001b[39m0.9956   \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m-0.3675  \u001b[39m | \u001b[39m227.1    \u001b[39m | \u001b[39m0.3067   \u001b[39m | \u001b[39m1.393e+03\u001b[39m | \u001b[39m742.7    \u001b[39m | \u001b[39m1.45e+03 \u001b[39m | \u001b[39m0.02972  \u001b[39m | \u001b[39m1.361    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m-0.637   \u001b[39m | \u001b[39m511.3    \u001b[39m | \u001b[39m0.04605  \u001b[39m | \u001b[39m1.209e+03\u001b[39m | \u001b[39m1.148e+03\u001b[39m | \u001b[39m1.224e+03\u001b[39m | \u001b[39m0.03493  \u001b[39m | \u001b[39m0.9288   \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m-0.3735  \u001b[39m | \u001b[39m178.1    \u001b[39m | \u001b[39m0.5175   \u001b[39m | \u001b[39m1.17e+03 \u001b[39m | \u001b[39m990.1    \u001b[39m | \u001b[39m219.6    \u001b[39m | \u001b[39m0.09636  \u001b[39m | \u001b[39m0.8472   \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m-0.6238  \u001b[39m | \u001b[39m417.3    \u001b[39m | \u001b[39m0.1735   \u001b[39m | \u001b[39m1.312e+03\u001b[39m | \u001b[39m1.011e+03\u001b[39m | \u001b[39m1.029e+03\u001b[39m | \u001b[39m0.00512  \u001b[39m | \u001b[39m0.8376   \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m-1.793   \u001b[39m | \u001b[39m154.3    \u001b[39m | \u001b[39m0.9      \u001b[39m | \u001b[39m799.4    \u001b[39m | \u001b[39m930.2    \u001b[39m | \u001b[39m327.1    \u001b[39m | \u001b[39m1e-08    \u001b[39m | \u001b[39m0.0      \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m-0.509   \u001b[39m | \u001b[39m210.2    \u001b[39m | \u001b[39m0.4973   \u001b[39m | \u001b[39m1.324e+03\u001b[39m | \u001b[39m899.5    \u001b[39m | \u001b[39m330.1    \u001b[39m | \u001b[39m0.08239  \u001b[39m | \u001b[39m0.06642  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m-0.3755  \u001b[39m | \u001b[39m500.2    \u001b[39m | \u001b[39m0.6299   \u001b[39m | \u001b[39m1.39e+03 \u001b[39m | \u001b[39m700.8    \u001b[39m | \u001b[39m645.6    \u001b[39m | \u001b[39m0.02404  \u001b[39m | \u001b[39m0.3581   \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m-0.3626  \u001b[39m | \u001b[39m28.62    \u001b[39m | \u001b[39m0.5611   \u001b[39m | \u001b[39m394.8    \u001b[39m | \u001b[39m1.108e+03\u001b[39m | \u001b[39m701.1    \u001b[39m | \u001b[39m0.006405 \u001b[39m | \u001b[39m0.3609   \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m-0.4376  \u001b[39m | \u001b[39m182.4    \u001b[39m | \u001b[39m0.269    \u001b[39m | \u001b[39m868.2    \u001b[39m | \u001b[39m216.7    \u001b[39m | \u001b[39m1.224e+03\u001b[39m | \u001b[39m0.05586  \u001b[39m | \u001b[39m0.7962   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m-0.3342  \u001b[39m | \u001b[39m16.0     \u001b[39m | \u001b[39m0.5416   \u001b[39m | \u001b[39m219.6    \u001b[39m | \u001b[39m892.0    \u001b[39m | \u001b[39m595.0    \u001b[39m | \u001b[39m0.05321  \u001b[39m | \u001b[39m1.436    \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m-0.3852  \u001b[39m | \u001b[39m277.0    \u001b[39m | \u001b[39m0.7871   \u001b[39m | \u001b[39m746.9    \u001b[39m | \u001b[39m502.0    \u001b[39m | \u001b[39m1.3e+03  \u001b[39m | \u001b[39m0.03417  \u001b[39m | \u001b[39m1.573    \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m-0.426   \u001b[39m | \u001b[39m70.39    \u001b[39m | \u001b[39m0.8114   \u001b[39m | \u001b[39m207.7    \u001b[39m | \u001b[39m1.181e+03\u001b[39m | \u001b[39m475.6    \u001b[39m | \u001b[39m0.04807  \u001b[39m | \u001b[39m0.9519   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m-1.761   \u001b[39m | \u001b[39m58.24    \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m619.0    \u001b[39m | \u001b[39m348.4    \u001b[39m | \u001b[39m1.3e+03  \u001b[39m | \u001b[39m1e-08    \u001b[39m | \u001b[39m2.0      \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m-0.3686  \u001b[39m | \u001b[39m481.5    \u001b[39m | \u001b[39m0.7116   \u001b[39m | \u001b[39m414.3    \u001b[39m | \u001b[39m1.077e+03\u001b[39m | \u001b[39m364.2    \u001b[39m | \u001b[39m0.06485  \u001b[39m | \u001b[39m1.473    \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m-0.3922  \u001b[39m | \u001b[39m94.71    \u001b[39m | \u001b[39m0.251    \u001b[39m | \u001b[39m572.5    \u001b[39m | \u001b[39m714.0    \u001b[39m | \u001b[39m1.215e+03\u001b[39m | \u001b[39m0.06712  \u001b[39m | \u001b[39m1.618    \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m-0.3524  \u001b[39m | \u001b[39m379.8    \u001b[39m | \u001b[39m0.2042   \u001b[39m | \u001b[39m929.9    \u001b[39m | \u001b[39m341.3    \u001b[39m | \u001b[39m1.342e+03\u001b[39m | \u001b[39m0.09104  \u001b[39m | \u001b[39m1.644    \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m-0.3754  \u001b[39m | \u001b[39m431.6    \u001b[39m | \u001b[39m0.5334   \u001b[39m | \u001b[39m594.6    \u001b[39m | \u001b[39m450.8    \u001b[39m | \u001b[39m1.515e+03\u001b[39m | \u001b[39m0.04177  \u001b[39m | \u001b[39m1.508    \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m-1.791   \u001b[39m | \u001b[39m338.6    \u001b[39m | \u001b[39m0.9      \u001b[39m | \u001b[39m893.4    \u001b[39m | \u001b[39m421.1    \u001b[39m | \u001b[39m1.12e+03 \u001b[39m | \u001b[39m1e-08    \u001b[39m | \u001b[39m2.0      \u001b[39m |\n",
      "=============================================================================================================\n",
      "Best Hyperparameters: {'batch_size': 201.7718989482918, 'dropout_rate': 0.8556428757689246, 'layer1_size': 1132.9188884843531, 'layer2_size': 932.382360232343, 'layer3_size': 266.6520352254245, 'learning_rate': 0.015599460473675063, 'optimizer_idx': 0.11616722433639892}\n",
      "Epoch [1/5000], Loss: 1.8056, Val Loss: 1.6380\n",
      "Epoch [2/5000], Loss: 1.4311, Val Loss: 1.2928\n",
      "Epoch [3/5000], Loss: 1.2175, Val Loss: 1.1876\n",
      "Epoch [4/5000], Loss: 1.1154, Val Loss: 1.0598\n",
      "Epoch [5/5000], Loss: 0.9979, Val Loss: 0.9554\n",
      "Epoch [6/5000], Loss: 0.9553, Val Loss: 0.8870\n",
      "Epoch [7/5000], Loss: 0.9592, Val Loss: 0.8359\n",
      "Epoch [8/5000], Loss: 0.8604, Val Loss: 0.7796\n",
      "Epoch [9/5000], Loss: 0.8167, Val Loss: 0.7184\n",
      "Epoch [10/5000], Loss: 0.7848, Val Loss: 0.6549\n",
      "Epoch [11/5000], Loss: 0.7271, Val Loss: 0.6004\n",
      "Epoch [12/5000], Loss: 0.6549, Val Loss: 0.5520\n",
      "Epoch [13/5000], Loss: 0.6640, Val Loss: 0.5073\n",
      "Epoch [14/5000], Loss: 0.5989, Val Loss: 0.4683\n",
      "Epoch [15/5000], Loss: 0.5371, Val Loss: 0.4354\n",
      "Epoch [16/5000], Loss: 0.5155, Val Loss: 0.4075\n",
      "Epoch [17/5000], Loss: 0.4684, Val Loss: 0.3883\n",
      "Epoch [18/5000], Loss: 0.4563, Val Loss: 0.3730\n",
      "Epoch [19/5000], Loss: 0.4619, Val Loss: 0.3544\n",
      "Epoch [20/5000], Loss: 0.4259, Val Loss: 0.3489\n",
      "Epoch [21/5000], Loss: 0.3826, Val Loss: 0.3415\n",
      "Epoch [22/5000], Loss: 0.3886, Val Loss: 0.3342\n",
      "Epoch [23/5000], Loss: 0.3462, Val Loss: 0.3195\n",
      "Epoch [24/5000], Loss: 0.3447, Val Loss: 0.3067\n",
      "Epoch [25/5000], Loss: 0.3491, Val Loss: 0.3034\n",
      "Epoch [26/5000], Loss: 0.3125, Val Loss: 0.3074\n",
      "Epoch [27/5000], Loss: 0.2970, Val Loss: 0.3134\n",
      "Epoch [28/5000], Loss: 0.2848, Val Loss: 0.3167\n",
      "Epoch [29/5000], Loss: 0.3181, Val Loss: 0.3210\n",
      "Epoch [30/5000], Loss: 0.2485, Val Loss: 0.3193\n",
      "Epoch [31/5000], Loss: 0.2397, Val Loss: 0.3188\n",
      "Epoch [32/5000], Loss: 0.2700, Val Loss: 0.3264\n",
      "Epoch [33/5000], Loss: 0.2956, Val Loss: 0.3386\n",
      "Epoch [34/5000], Loss: 0.2172, Val Loss: 0.3461\n",
      "Epoch [35/5000], Loss: 0.2303, Val Loss: 0.3524\n",
      "Epoch [36/5000], Loss: 0.2086, Val Loss: 0.3459\n",
      "Epoch [37/5000], Loss: 0.2107, Val Loss: 0.3448\n",
      "Epoch [38/5000], Loss: 0.2077, Val Loss: 0.3494\n",
      "Epoch [39/5000], Loss: 0.1955, Val Loss: 0.3521\n",
      "Epoch [40/5000], Loss: 0.2150, Val Loss: 0.3568\n",
      "Epoch [41/5000], Loss: 0.1941, Val Loss: 0.3586\n",
      "Epoch [42/5000], Loss: 0.1885, Val Loss: 0.3662\n",
      "Epoch [43/5000], Loss: 0.1946, Val Loss: 0.3657\n",
      "Epoch [44/5000], Loss: 0.1963, Val Loss: 0.3685\n",
      "Epoch [45/5000], Loss: 0.1465, Val Loss: 0.3794\n",
      "Epoch [46/5000], Loss: 0.1878, Val Loss: 0.3906\n",
      "Epoch [47/5000], Loss: 0.1523, Val Loss: 0.4033\n",
      "Epoch [48/5000], Loss: 0.1644, Val Loss: 0.4138\n",
      "Epoch [49/5000], Loss: 0.1717, Val Loss: 0.4258\n",
      "Epoch [50/5000], Loss: 0.1301, Val Loss: 0.4388\n",
      "Epoch [51/5000], Loss: 0.1412, Val Loss: 0.4331\n",
      "Epoch [52/5000], Loss: 0.1490, Val Loss: 0.4294\n",
      "Epoch [53/5000], Loss: 0.1486, Val Loss: 0.4351\n",
      "Epoch [54/5000], Loss: 0.1183, Val Loss: 0.4491\n",
      "Epoch [55/5000], Loss: 0.1563, Val Loss: 0.4460\n",
      "Epoch [56/5000], Loss: 0.1304, Val Loss: 0.4526\n",
      "Epoch [57/5000], Loss: 0.1135, Val Loss: 0.4571\n",
      "Epoch [58/5000], Loss: 0.1297, Val Loss: 0.4689\n",
      "Epoch [59/5000], Loss: 0.1151, Val Loss: 0.4816\n",
      "Epoch [60/5000], Loss: 0.1031, Val Loss: 0.4819\n",
      "Epoch [61/5000], Loss: 0.1058, Val Loss: 0.4755\n",
      "Epoch [62/5000], Loss: 0.1177, Val Loss: 0.4757\n",
      "Epoch [63/5000], Loss: 0.1178, Val Loss: 0.4790\n",
      "Epoch [64/5000], Loss: 0.1287, Val Loss: 0.4643\n",
      "Epoch [65/5000], Loss: 0.0956, Val Loss: 0.4372\n",
      "Epoch [66/5000], Loss: 0.1093, Val Loss: 0.4316\n",
      "Epoch [67/5000], Loss: 0.1304, Val Loss: 0.4507\n",
      "Epoch [68/5000], Loss: 0.1007, Val Loss: 0.4668\n",
      "Epoch [69/5000], Loss: 0.1001, Val Loss: 0.4628\n",
      "Epoch [70/5000], Loss: 0.1117, Val Loss: 0.4686\n",
      "Epoch [71/5000], Loss: 0.1131, Val Loss: 0.4676\n",
      "Epoch [72/5000], Loss: 0.0998, Val Loss: 0.4669\n",
      "Epoch [73/5000], Loss: 0.1006, Val Loss: 0.4773\n",
      "Epoch [74/5000], Loss: 0.0810, Val Loss: 0.4733\n",
      "Epoch [75/5000], Loss: 0.0991, Val Loss: 0.4812\n",
      "Early stopping triggered\n",
      "Test Loss: 0.3117, Test Accuracy: 0.8832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\AppData\\Local\\Temp\\ipykernel_3512\\3000357372.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  final_model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final optimized model saved to final_model_optimized.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Assuming X_train_pca, y_train, X_val_pca, y_val, X_test_pca, y_test are already defined as pandas DataFrames or numpy arrays\n",
    "\n",
    "# Encode labels to start from 0 using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(np.unique(np.concatenate([y_train, y_val, y_test])))\n",
    "y_train_tensor = torch.tensor(le.transform(y_train), dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(le.transform(y_val), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(le.transform(y_test), dtype=torch.long)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Convert PCA features to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_pca.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_pca.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_pca.values, dtype=torch.float32)\n",
    "\n",
    "# Check for NaNs/Infs in input tensors\n",
    "assert not torch.isnan(X_train_tensor).any(), \"X_train_tensor has NaNs\"\n",
    "assert not torch.isinf(X_train_tensor).any(), \"X_train_tensor has Infs\"\n",
    "assert not torch.isnan(X_val_tensor).any(), \"X_val_tensor has NaNs\"\n",
    "assert not torch.isinf(X_val_tensor).any(), \"X_val_tensor has Infs\"\n",
    "assert not torch.isnan(X_test_tensor).any(), \"X_test_tensor has NaNs\"\n",
    "assert not torch.isinf(X_test_tensor).any(), \"X_test_tensor has Infs\"\n",
    "\n",
    "# Create DataLoader for training and validation sets (batch size will be set dynamically)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the neural network architecture\n",
    "class ImprovedNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, layer1_size, layer2_size, layer3_size, dropout_rate):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, layer1_size)\n",
    "        self.bn1 = nn.BatchNorm1d(layer1_size)\n",
    "        self.fc2 = nn.Linear(layer1_size, layer2_size)\n",
    "        self.bn2 = nn.BatchNorm1d(layer2_size)\n",
    "        self.fc3 = nn.Linear(layer2_size, layer3_size)\n",
    "        self.bn3 = nn.BatchNorm1d(layer3_size)\n",
    "        self.fc4 = nn.Linear(layer3_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(layer1_size, layer2_size, layer3_size, dropout_rate, learning_rate, batch_size, optimizer_idx):\n",
    "    # Map optimizer index to actual optimizer\n",
    "    optimizers = [optim.Adam, optim.SGD, optim.RMSprop]\n",
    "    optimizer_class = optimizers[int(optimizer_idx)]\n",
    "\n",
    "    # Create DataLoader with current batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=int(batch_size), shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=int(batch_size), shuffle=False)  # drop_last not needed here\n",
    "\n",
    "    # Initialize model\n",
    "    model = ImprovedNN(input_size=X_train_pca.shape[1], num_classes=num_classes,\n",
    "                       layer1_size=int(layer1_size), layer2_size=int(layer2_size), layer3_size=int(layer3_size),\n",
    "                       dropout_rate=dropout_rate)\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    num_epochs = 5000\n",
    "    patience = 50\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            break\n",
    "\n",
    "    return -best_val_loss  # Return negative validation loss for maximization\n",
    "\n",
    "# Define the search space for Bayesian optimization\n",
    "pbounds = {\n",
    "    'layer1_size': (32, 1536),      # Number of neurons in the first layer\n",
    "    'layer2_size': (32, 1536),      # Number of neurons in the second layer\n",
    "    'layer3_size': (32, 1536),      # Number of neurons in the third layer\n",
    "    'dropout_rate': (0.0, 0.9),    # Dropout probability\n",
    "    'learning_rate': (1e-8, 1e-1), # Learning rate for the optimizer\n",
    "    'batch_size': (16, 512),       # Batch size for training\n",
    "    'optimizer_idx': (0, 2)        # 0: Adam, 1: SGD, 2: RMSprop\n",
    "}\n",
    "\n",
    "# Initialize Bayesian Optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_and_evaluate,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Run optimization with 10 trials (5 initial points + 5 optimization steps)\n",
    "optimizer.maximize(\n",
    "    init_points=10,  # Number of random initial points\n",
    "    n_iter=20,       # Number of optimization steps\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = optimizer.max['params']\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "train_loader = DataLoader(train_dataset, batch_size=int(best_params['batch_size']), shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=int(best_params['batch_size']), shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=int(best_params['batch_size']))\n",
    "\n",
    "final_model = ImprovedNN(\n",
    "    input_size=X_train_pca.shape[1],\n",
    "    num_classes=num_classes,\n",
    "    layer1_size=int(best_params['layer1_size']),\n",
    "    layer2_size=int(best_params['layer2_size']),\n",
    "    layer3_size=int(best_params['layer3_size']),\n",
    "    dropout_rate=best_params['dropout_rate']\n",
    ")\n",
    "final_model.to(device)\n",
    "\n",
    "optimizers = [optim.Adam, optim.SGD, optim.RMSprop]\n",
    "final_optimizer = optimizers[int(best_params['optimizer_idx'])](final_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "# Training loop for the final model\n",
    "num_epochs = 5000\n",
    "patience = 50\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "best_model_path = 'best_model_optimized.pth'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    final_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        final_optimizer.zero_grad()\n",
    "        outputs = final_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        final_optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation step\n",
    "    final_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = final_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(final_model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "final_model.load_state_dict(torch.load(best_model_path))\n",
    "final_model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = final_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(X_test_tensor)\n",
    "test_accuracy = test_correct / len(X_test_tensor)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Save the final optimized model\n",
    "final_model_path = 'final_model_optimized.pth'\n",
    "torch.save(final_model.state_dict(), final_model_path)\n",
    "print(f\"Final optimized model saved to {final_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
