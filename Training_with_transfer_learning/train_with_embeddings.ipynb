{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f36439",
   "metadata": {},
   "source": [
    "# Training the model using image embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637789b1",
   "metadata": {},
   "source": [
    "### Reading the spectrogram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72cadd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_data = pd.read_csv(r'spectrogram_features_nomic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04ca23d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display max column width\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b1ca411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_758</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/3P66A03R2_spectrogram.png</td>\n",
       "      <td>-0.017944</td>\n",
       "      <td>-0.051605</td>\n",
       "      <td>0.012337</td>\n",
       "      <td>-0.053680</td>\n",
       "      <td>-0.025848</td>\n",
       "      <td>-0.051849</td>\n",
       "      <td>-0.004677</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>-0.045959</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038086</td>\n",
       "      <td>-0.037781</td>\n",
       "      <td>-0.047546</td>\n",
       "      <td>-0.035370</td>\n",
       "      <td>-0.020569</td>\n",
       "      <td>-0.023621</td>\n",
       "      <td>-0.021637</td>\n",
       "      <td>-0.053284</td>\n",
       "      <td>-0.032501</td>\n",
       "      <td>-0.045715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/1P68A01R1_spectrogram.png</td>\n",
       "      <td>-0.024246</td>\n",
       "      <td>-0.045624</td>\n",
       "      <td>-0.002094</td>\n",
       "      <td>-0.055298</td>\n",
       "      <td>-0.020370</td>\n",
       "      <td>-0.051697</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>-0.061829</td>\n",
       "      <td>-0.045349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033142</td>\n",
       "      <td>-0.036469</td>\n",
       "      <td>-0.046112</td>\n",
       "      <td>-0.032684</td>\n",
       "      <td>-0.028214</td>\n",
       "      <td>-0.022568</td>\n",
       "      <td>-0.017578</td>\n",
       "      <td>-0.051758</td>\n",
       "      <td>-0.024597</td>\n",
       "      <td>-0.050476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/5P60A05R1_spectrogram.png</td>\n",
       "      <td>-0.024475</td>\n",
       "      <td>-0.047882</td>\n",
       "      <td>0.010353</td>\n",
       "      <td>-0.051758</td>\n",
       "      <td>-0.026199</td>\n",
       "      <td>-0.055176</td>\n",
       "      <td>-0.004349</td>\n",
       "      <td>-0.059357</td>\n",
       "      <td>-0.045990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031830</td>\n",
       "      <td>-0.046814</td>\n",
       "      <td>-0.046326</td>\n",
       "      <td>-0.035370</td>\n",
       "      <td>-0.022995</td>\n",
       "      <td>-0.028809</td>\n",
       "      <td>-0.013321</td>\n",
       "      <td>-0.050354</td>\n",
       "      <td>-0.039368</td>\n",
       "      <td>-0.048035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/3P65A03R3_spectrogram.png</td>\n",
       "      <td>-0.016022</td>\n",
       "      <td>-0.052368</td>\n",
       "      <td>0.010658</td>\n",
       "      <td>-0.056366</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>-0.049530</td>\n",
       "      <td>-0.000743</td>\n",
       "      <td>-0.065552</td>\n",
       "      <td>-0.049408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039215</td>\n",
       "      <td>-0.041046</td>\n",
       "      <td>-0.048279</td>\n",
       "      <td>-0.034607</td>\n",
       "      <td>-0.020554</td>\n",
       "      <td>-0.026443</td>\n",
       "      <td>-0.021194</td>\n",
       "      <td>-0.051147</td>\n",
       "      <td>-0.030838</td>\n",
       "      <td>-0.047821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/1P62A01R2_spectrogram.png</td>\n",
       "      <td>-0.025696</td>\n",
       "      <td>-0.047058</td>\n",
       "      <td>-0.001618</td>\n",
       "      <td>-0.056396</td>\n",
       "      <td>-0.022232</td>\n",
       "      <td>-0.048645</td>\n",
       "      <td>0.008240</td>\n",
       "      <td>-0.060364</td>\n",
       "      <td>-0.049591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036163</td>\n",
       "      <td>-0.039551</td>\n",
       "      <td>-0.045380</td>\n",
       "      <td>-0.033569</td>\n",
       "      <td>-0.028107</td>\n",
       "      <td>-0.026535</td>\n",
       "      <td>-0.018280</td>\n",
       "      <td>-0.049042</td>\n",
       "      <td>-0.026566</td>\n",
       "      <td>-0.052155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/6P56A06R02_spectrogram.png</td>\n",
       "      <td>-0.020493</td>\n",
       "      <td>-0.055481</td>\n",
       "      <td>0.010796</td>\n",
       "      <td>-0.059204</td>\n",
       "      <td>-0.028580</td>\n",
       "      <td>-0.047638</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>-0.049713</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042786</td>\n",
       "      <td>-0.041351</td>\n",
       "      <td>-0.054199</td>\n",
       "      <td>-0.034180</td>\n",
       "      <td>-0.025711</td>\n",
       "      <td>-0.029419</td>\n",
       "      <td>-0.023453</td>\n",
       "      <td>-0.051300</td>\n",
       "      <td>-0.034241</td>\n",
       "      <td>-0.043091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/1P42A01R03_spectrogram.png</td>\n",
       "      <td>-0.018906</td>\n",
       "      <td>-0.048553</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>-0.051208</td>\n",
       "      <td>-0.019638</td>\n",
       "      <td>-0.043152</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>-0.064453</td>\n",
       "      <td>-0.051941</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040375</td>\n",
       "      <td>-0.037933</td>\n",
       "      <td>-0.044678</td>\n",
       "      <td>-0.033142</td>\n",
       "      <td>-0.024460</td>\n",
       "      <td>-0.027588</td>\n",
       "      <td>-0.016556</td>\n",
       "      <td>-0.050079</td>\n",
       "      <td>-0.030457</td>\n",
       "      <td>-0.048584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/6P40A06R03_spectrogram.png</td>\n",
       "      <td>-0.016830</td>\n",
       "      <td>-0.056915</td>\n",
       "      <td>0.007698</td>\n",
       "      <td>-0.054169</td>\n",
       "      <td>-0.028030</td>\n",
       "      <td>-0.051147</td>\n",
       "      <td>-0.003632</td>\n",
       "      <td>-0.073975</td>\n",
       "      <td>-0.046173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042023</td>\n",
       "      <td>-0.038544</td>\n",
       "      <td>-0.047607</td>\n",
       "      <td>-0.042938</td>\n",
       "      <td>-0.019638</td>\n",
       "      <td>-0.031830</td>\n",
       "      <td>-0.025299</td>\n",
       "      <td>-0.046600</td>\n",
       "      <td>-0.025421</td>\n",
       "      <td>-0.043243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/2P56A02R01_spectrogram.png</td>\n",
       "      <td>-0.013687</td>\n",
       "      <td>-0.048248</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>-0.053192</td>\n",
       "      <td>-0.025574</td>\n",
       "      <td>-0.045898</td>\n",
       "      <td>-0.008392</td>\n",
       "      <td>-0.071533</td>\n",
       "      <td>-0.052246</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036255</td>\n",
       "      <td>-0.046204</td>\n",
       "      <td>-0.049713</td>\n",
       "      <td>-0.041077</td>\n",
       "      <td>-0.023148</td>\n",
       "      <td>-0.027405</td>\n",
       "      <td>-0.025772</td>\n",
       "      <td>-0.052673</td>\n",
       "      <td>-0.029388</td>\n",
       "      <td>-0.052155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>spectrograms/Sample_data_preprocessing+label_extraction/5P56A05R3_spectrogram.png</td>\n",
       "      <td>-0.021790</td>\n",
       "      <td>-0.049316</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>-0.052979</td>\n",
       "      <td>-0.025909</td>\n",
       "      <td>-0.047943</td>\n",
       "      <td>-0.004124</td>\n",
       "      <td>-0.072876</td>\n",
       "      <td>-0.054413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042267</td>\n",
       "      <td>-0.040314</td>\n",
       "      <td>-0.045197</td>\n",
       "      <td>-0.046936</td>\n",
       "      <td>-0.017044</td>\n",
       "      <td>-0.031281</td>\n",
       "      <td>-0.025589</td>\n",
       "      <td>-0.050629</td>\n",
       "      <td>-0.032501</td>\n",
       "      <td>-0.043182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1754 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             image_path  \\\n",
       "0                            spectrograms/4 July 2018 Dataset/3P66A03R2_spectrogram.png   \n",
       "1                            spectrograms/4 July 2018 Dataset/1P68A01R1_spectrogram.png   \n",
       "2                            spectrograms/4 July 2018 Dataset/5P60A05R1_spectrogram.png   \n",
       "3                            spectrograms/4 July 2018 Dataset/3P65A03R3_spectrogram.png   \n",
       "4                            spectrograms/4 July 2018 Dataset/1P62A01R2_spectrogram.png   \n",
       "...                                                                                 ...   \n",
       "1749                    spectrograms/1 December 2017 Dataset/6P56A06R02_spectrogram.png   \n",
       "1750                    spectrograms/1 December 2017 Dataset/1P42A01R03_spectrogram.png   \n",
       "1751                    spectrograms/1 December 2017 Dataset/6P40A06R03_spectrogram.png   \n",
       "1752                    spectrograms/1 December 2017 Dataset/2P56A02R01_spectrogram.png   \n",
       "1753  spectrograms/Sample_data_preprocessing+label_extraction/5P56A05R3_spectrogram.png   \n",
       "\n",
       "      feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0     -0.017944  -0.051605   0.012337  -0.053680  -0.025848  -0.051849   \n",
       "1     -0.024246  -0.045624  -0.002094  -0.055298  -0.020370  -0.051697   \n",
       "2     -0.024475  -0.047882   0.010353  -0.051758  -0.026199  -0.055176   \n",
       "3     -0.016022  -0.052368   0.010658  -0.056366  -0.026611  -0.049530   \n",
       "4     -0.025696  -0.047058  -0.001618  -0.056396  -0.022232  -0.048645   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1749  -0.020493  -0.055481   0.010796  -0.059204  -0.028580  -0.047638   \n",
       "1750  -0.018906  -0.048553   0.002321  -0.051208  -0.019638  -0.043152   \n",
       "1751  -0.016830  -0.056915   0.007698  -0.054169  -0.028030  -0.051147   \n",
       "1752  -0.013687  -0.048248   0.008827  -0.053192  -0.025574  -0.045898   \n",
       "1753  -0.021790  -0.049316   0.010201  -0.052979  -0.025909  -0.047943   \n",
       "\n",
       "      feature_6  feature_7  feature_8  ...  feature_758  feature_759  \\\n",
       "0     -0.004677  -0.064941  -0.045959  ...    -0.038086    -0.037781   \n",
       "1      0.012650  -0.061829  -0.045349  ...    -0.033142    -0.036469   \n",
       "2     -0.004349  -0.059357  -0.045990  ...    -0.031830    -0.046814   \n",
       "3     -0.000743  -0.065552  -0.049408  ...    -0.039215    -0.041046   \n",
       "4      0.008240  -0.060364  -0.049591  ...    -0.036163    -0.039551   \n",
       "...         ...        ...        ...  ...          ...          ...   \n",
       "1749  -0.000404  -0.080078  -0.049713  ...    -0.042786    -0.041351   \n",
       "1750   0.011803  -0.064453  -0.051941  ...    -0.040375    -0.037933   \n",
       "1751  -0.003632  -0.073975  -0.046173  ...    -0.042023    -0.038544   \n",
       "1752  -0.008392  -0.071533  -0.052246  ...    -0.036255    -0.046204   \n",
       "1753  -0.004124  -0.072876  -0.054413  ...    -0.042267    -0.040314   \n",
       "\n",
       "      feature_760  feature_761  feature_762  feature_763  feature_764  \\\n",
       "0       -0.047546    -0.035370    -0.020569    -0.023621    -0.021637   \n",
       "1       -0.046112    -0.032684    -0.028214    -0.022568    -0.017578   \n",
       "2       -0.046326    -0.035370    -0.022995    -0.028809    -0.013321   \n",
       "3       -0.048279    -0.034607    -0.020554    -0.026443    -0.021194   \n",
       "4       -0.045380    -0.033569    -0.028107    -0.026535    -0.018280   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1749    -0.054199    -0.034180    -0.025711    -0.029419    -0.023453   \n",
       "1750    -0.044678    -0.033142    -0.024460    -0.027588    -0.016556   \n",
       "1751    -0.047607    -0.042938    -0.019638    -0.031830    -0.025299   \n",
       "1752    -0.049713    -0.041077    -0.023148    -0.027405    -0.025772   \n",
       "1753    -0.045197    -0.046936    -0.017044    -0.031281    -0.025589   \n",
       "\n",
       "      feature_765  feature_766  feature_767  \n",
       "0       -0.053284    -0.032501    -0.045715  \n",
       "1       -0.051758    -0.024597    -0.050476  \n",
       "2       -0.050354    -0.039368    -0.048035  \n",
       "3       -0.051147    -0.030838    -0.047821  \n",
       "4       -0.049042    -0.026566    -0.052155  \n",
       "...           ...          ...          ...  \n",
       "1749    -0.051300    -0.034241    -0.043091  \n",
       "1750    -0.050079    -0.030457    -0.048584  \n",
       "1751    -0.046600    -0.025421    -0.043243  \n",
       "1752    -0.052673    -0.029388    -0.052155  \n",
       "1753    -0.050629    -0.032501    -0.043182  \n",
       "\n",
       "[1754 rows x 769 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c013e",
   "metadata": {},
   "source": [
    "# Extracting the labels of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8a9ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   image_path  label\n",
      "0  spectrograms/4 July 2018 Dataset/3P66A03R2_spectrogram.png      3\n",
      "1  spectrograms/4 July 2018 Dataset/1P68A01R1_spectrogram.png      1\n",
      "2  spectrograms/4 July 2018 Dataset/5P60A05R1_spectrogram.png      5\n",
      "3  spectrograms/4 July 2018 Dataset/3P65A03R3_spectrogram.png      3\n",
      "4  spectrograms/4 July 2018 Dataset/1P62A01R2_spectrogram.png      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Assuming 'df' is your pandas DataFrame\n",
    "# For example:\n",
    "# df = pd.DataFrame({'image_path': ['/path/to/1P24A01R2_spectrogram.png']})\n",
    "\n",
    "# If already loaded:\n",
    "df = all_data.copy()\n",
    "\n",
    "# Function to extract the first number from the filename only\n",
    "def extract_first_number_from_filename(path):\n",
    "    filename = os.path.basename(path)  # Extracts just the filename\n",
    "    match = re.search(r'(\\d+)', filename)  # Looks for the first digit sequence\n",
    "    if match:\n",
    "        return int(match.group(1))  # Returns the number as an integer\n",
    "    return None  # If no number is found\n",
    "\n",
    "# Apply the function to the 'image_path' column and create the 'label' column\n",
    "df['label'] = df['image_path'].apply(extract_first_number_from_filename)\n",
    "\n",
    "# Display the DataFrame with the new 'label' column\n",
    "print(df[['image_path', 'label']].head())\n",
    "\n",
    "# To see the full DataFrame with the new column:\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cc1c855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/3P66A03R2_spectrogram.png</td>\n",
       "      <td>-0.017944</td>\n",
       "      <td>-0.051605</td>\n",
       "      <td>0.012337</td>\n",
       "      <td>-0.053680</td>\n",
       "      <td>-0.025848</td>\n",
       "      <td>-0.051849</td>\n",
       "      <td>-0.004677</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>-0.045959</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037781</td>\n",
       "      <td>-0.047546</td>\n",
       "      <td>-0.035370</td>\n",
       "      <td>-0.020569</td>\n",
       "      <td>-0.023621</td>\n",
       "      <td>-0.021637</td>\n",
       "      <td>-0.053284</td>\n",
       "      <td>-0.032501</td>\n",
       "      <td>-0.045715</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/1P68A01R1_spectrogram.png</td>\n",
       "      <td>-0.024246</td>\n",
       "      <td>-0.045624</td>\n",
       "      <td>-0.002094</td>\n",
       "      <td>-0.055298</td>\n",
       "      <td>-0.020370</td>\n",
       "      <td>-0.051697</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>-0.061829</td>\n",
       "      <td>-0.045349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036469</td>\n",
       "      <td>-0.046112</td>\n",
       "      <td>-0.032684</td>\n",
       "      <td>-0.028214</td>\n",
       "      <td>-0.022568</td>\n",
       "      <td>-0.017578</td>\n",
       "      <td>-0.051758</td>\n",
       "      <td>-0.024597</td>\n",
       "      <td>-0.050476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/5P60A05R1_spectrogram.png</td>\n",
       "      <td>-0.024475</td>\n",
       "      <td>-0.047882</td>\n",
       "      <td>0.010353</td>\n",
       "      <td>-0.051758</td>\n",
       "      <td>-0.026199</td>\n",
       "      <td>-0.055176</td>\n",
       "      <td>-0.004349</td>\n",
       "      <td>-0.059357</td>\n",
       "      <td>-0.045990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046814</td>\n",
       "      <td>-0.046326</td>\n",
       "      <td>-0.035370</td>\n",
       "      <td>-0.022995</td>\n",
       "      <td>-0.028809</td>\n",
       "      <td>-0.013321</td>\n",
       "      <td>-0.050354</td>\n",
       "      <td>-0.039368</td>\n",
       "      <td>-0.048035</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/3P65A03R3_spectrogram.png</td>\n",
       "      <td>-0.016022</td>\n",
       "      <td>-0.052368</td>\n",
       "      <td>0.010658</td>\n",
       "      <td>-0.056366</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>-0.049530</td>\n",
       "      <td>-0.000743</td>\n",
       "      <td>-0.065552</td>\n",
       "      <td>-0.049408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041046</td>\n",
       "      <td>-0.048279</td>\n",
       "      <td>-0.034607</td>\n",
       "      <td>-0.020554</td>\n",
       "      <td>-0.026443</td>\n",
       "      <td>-0.021194</td>\n",
       "      <td>-0.051147</td>\n",
       "      <td>-0.030838</td>\n",
       "      <td>-0.047821</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/1P62A01R2_spectrogram.png</td>\n",
       "      <td>-0.025696</td>\n",
       "      <td>-0.047058</td>\n",
       "      <td>-0.001618</td>\n",
       "      <td>-0.056396</td>\n",
       "      <td>-0.022232</td>\n",
       "      <td>-0.048645</td>\n",
       "      <td>0.008240</td>\n",
       "      <td>-0.060364</td>\n",
       "      <td>-0.049591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039551</td>\n",
       "      <td>-0.045380</td>\n",
       "      <td>-0.033569</td>\n",
       "      <td>-0.028107</td>\n",
       "      <td>-0.026535</td>\n",
       "      <td>-0.018280</td>\n",
       "      <td>-0.049042</td>\n",
       "      <td>-0.026566</td>\n",
       "      <td>-0.052155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/6P56A06R02_spectrogram.png</td>\n",
       "      <td>-0.020493</td>\n",
       "      <td>-0.055481</td>\n",
       "      <td>0.010796</td>\n",
       "      <td>-0.059204</td>\n",
       "      <td>-0.028580</td>\n",
       "      <td>-0.047638</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>-0.049713</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041351</td>\n",
       "      <td>-0.054199</td>\n",
       "      <td>-0.034180</td>\n",
       "      <td>-0.025711</td>\n",
       "      <td>-0.029419</td>\n",
       "      <td>-0.023453</td>\n",
       "      <td>-0.051300</td>\n",
       "      <td>-0.034241</td>\n",
       "      <td>-0.043091</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/1P42A01R03_spectrogram.png</td>\n",
       "      <td>-0.018906</td>\n",
       "      <td>-0.048553</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>-0.051208</td>\n",
       "      <td>-0.019638</td>\n",
       "      <td>-0.043152</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>-0.064453</td>\n",
       "      <td>-0.051941</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037933</td>\n",
       "      <td>-0.044678</td>\n",
       "      <td>-0.033142</td>\n",
       "      <td>-0.024460</td>\n",
       "      <td>-0.027588</td>\n",
       "      <td>-0.016556</td>\n",
       "      <td>-0.050079</td>\n",
       "      <td>-0.030457</td>\n",
       "      <td>-0.048584</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/6P40A06R03_spectrogram.png</td>\n",
       "      <td>-0.016830</td>\n",
       "      <td>-0.056915</td>\n",
       "      <td>0.007698</td>\n",
       "      <td>-0.054169</td>\n",
       "      <td>-0.028030</td>\n",
       "      <td>-0.051147</td>\n",
       "      <td>-0.003632</td>\n",
       "      <td>-0.073975</td>\n",
       "      <td>-0.046173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038544</td>\n",
       "      <td>-0.047607</td>\n",
       "      <td>-0.042938</td>\n",
       "      <td>-0.019638</td>\n",
       "      <td>-0.031830</td>\n",
       "      <td>-0.025299</td>\n",
       "      <td>-0.046600</td>\n",
       "      <td>-0.025421</td>\n",
       "      <td>-0.043243</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/2P56A02R01_spectrogram.png</td>\n",
       "      <td>-0.013687</td>\n",
       "      <td>-0.048248</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>-0.053192</td>\n",
       "      <td>-0.025574</td>\n",
       "      <td>-0.045898</td>\n",
       "      <td>-0.008392</td>\n",
       "      <td>-0.071533</td>\n",
       "      <td>-0.052246</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046204</td>\n",
       "      <td>-0.049713</td>\n",
       "      <td>-0.041077</td>\n",
       "      <td>-0.023148</td>\n",
       "      <td>-0.027405</td>\n",
       "      <td>-0.025772</td>\n",
       "      <td>-0.052673</td>\n",
       "      <td>-0.029388</td>\n",
       "      <td>-0.052155</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>spectrograms/Sample_data_preprocessing+label_extraction/5P56A05R3_spectrogram.png</td>\n",
       "      <td>-0.021790</td>\n",
       "      <td>-0.049316</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>-0.052979</td>\n",
       "      <td>-0.025909</td>\n",
       "      <td>-0.047943</td>\n",
       "      <td>-0.004124</td>\n",
       "      <td>-0.072876</td>\n",
       "      <td>-0.054413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040314</td>\n",
       "      <td>-0.045197</td>\n",
       "      <td>-0.046936</td>\n",
       "      <td>-0.017044</td>\n",
       "      <td>-0.031281</td>\n",
       "      <td>-0.025589</td>\n",
       "      <td>-0.050629</td>\n",
       "      <td>-0.032501</td>\n",
       "      <td>-0.043182</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1754 rows × 770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             image_path  \\\n",
       "0                            spectrograms/4 July 2018 Dataset/3P66A03R2_spectrogram.png   \n",
       "1                            spectrograms/4 July 2018 Dataset/1P68A01R1_spectrogram.png   \n",
       "2                            spectrograms/4 July 2018 Dataset/5P60A05R1_spectrogram.png   \n",
       "3                            spectrograms/4 July 2018 Dataset/3P65A03R3_spectrogram.png   \n",
       "4                            spectrograms/4 July 2018 Dataset/1P62A01R2_spectrogram.png   \n",
       "...                                                                                 ...   \n",
       "1749                    spectrograms/1 December 2017 Dataset/6P56A06R02_spectrogram.png   \n",
       "1750                    spectrograms/1 December 2017 Dataset/1P42A01R03_spectrogram.png   \n",
       "1751                    spectrograms/1 December 2017 Dataset/6P40A06R03_spectrogram.png   \n",
       "1752                    spectrograms/1 December 2017 Dataset/2P56A02R01_spectrogram.png   \n",
       "1753  spectrograms/Sample_data_preprocessing+label_extraction/5P56A05R3_spectrogram.png   \n",
       "\n",
       "      feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0     -0.017944  -0.051605   0.012337  -0.053680  -0.025848  -0.051849   \n",
       "1     -0.024246  -0.045624  -0.002094  -0.055298  -0.020370  -0.051697   \n",
       "2     -0.024475  -0.047882   0.010353  -0.051758  -0.026199  -0.055176   \n",
       "3     -0.016022  -0.052368   0.010658  -0.056366  -0.026611  -0.049530   \n",
       "4     -0.025696  -0.047058  -0.001618  -0.056396  -0.022232  -0.048645   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1749  -0.020493  -0.055481   0.010796  -0.059204  -0.028580  -0.047638   \n",
       "1750  -0.018906  -0.048553   0.002321  -0.051208  -0.019638  -0.043152   \n",
       "1751  -0.016830  -0.056915   0.007698  -0.054169  -0.028030  -0.051147   \n",
       "1752  -0.013687  -0.048248   0.008827  -0.053192  -0.025574  -0.045898   \n",
       "1753  -0.021790  -0.049316   0.010201  -0.052979  -0.025909  -0.047943   \n",
       "\n",
       "      feature_6  feature_7  feature_8  ...  feature_759  feature_760  \\\n",
       "0     -0.004677  -0.064941  -0.045959  ...    -0.037781    -0.047546   \n",
       "1      0.012650  -0.061829  -0.045349  ...    -0.036469    -0.046112   \n",
       "2     -0.004349  -0.059357  -0.045990  ...    -0.046814    -0.046326   \n",
       "3     -0.000743  -0.065552  -0.049408  ...    -0.041046    -0.048279   \n",
       "4      0.008240  -0.060364  -0.049591  ...    -0.039551    -0.045380   \n",
       "...         ...        ...        ...  ...          ...          ...   \n",
       "1749  -0.000404  -0.080078  -0.049713  ...    -0.041351    -0.054199   \n",
       "1750   0.011803  -0.064453  -0.051941  ...    -0.037933    -0.044678   \n",
       "1751  -0.003632  -0.073975  -0.046173  ...    -0.038544    -0.047607   \n",
       "1752  -0.008392  -0.071533  -0.052246  ...    -0.046204    -0.049713   \n",
       "1753  -0.004124  -0.072876  -0.054413  ...    -0.040314    -0.045197   \n",
       "\n",
       "      feature_761  feature_762  feature_763  feature_764  feature_765  \\\n",
       "0       -0.035370    -0.020569    -0.023621    -0.021637    -0.053284   \n",
       "1       -0.032684    -0.028214    -0.022568    -0.017578    -0.051758   \n",
       "2       -0.035370    -0.022995    -0.028809    -0.013321    -0.050354   \n",
       "3       -0.034607    -0.020554    -0.026443    -0.021194    -0.051147   \n",
       "4       -0.033569    -0.028107    -0.026535    -0.018280    -0.049042   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1749    -0.034180    -0.025711    -0.029419    -0.023453    -0.051300   \n",
       "1750    -0.033142    -0.024460    -0.027588    -0.016556    -0.050079   \n",
       "1751    -0.042938    -0.019638    -0.031830    -0.025299    -0.046600   \n",
       "1752    -0.041077    -0.023148    -0.027405    -0.025772    -0.052673   \n",
       "1753    -0.046936    -0.017044    -0.031281    -0.025589    -0.050629   \n",
       "\n",
       "      feature_766  feature_767  label  \n",
       "0       -0.032501    -0.045715      3  \n",
       "1       -0.024597    -0.050476      1  \n",
       "2       -0.039368    -0.048035      5  \n",
       "3       -0.030838    -0.047821      3  \n",
       "4       -0.026566    -0.052155      1  \n",
       "...           ...          ...    ...  \n",
       "1749    -0.034241    -0.043091      6  \n",
       "1750    -0.030457    -0.048584      1  \n",
       "1751    -0.025421    -0.043243      6  \n",
       "1752    -0.029388    -0.052155      2  \n",
       "1753    -0.032501    -0.043182      5  \n",
       "\n",
       "[1754 rows x 770 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f7a22",
   "metadata": {},
   "source": [
    "# Splitting the data in train,test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad377719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns = ['image_path', 'label']), df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "\n",
    "#split in validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321b843",
   "metadata": {},
   "source": [
    "## Performing PCA to reduce dimensionality from 1151 down to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88d94115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform PCA on the training data\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=768)  # Reduce to 2 dimensions for visualization\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_val_pca = pca.transform(X_val)\n",
    "# Convert the PCA results back to DataFrame for easier handling\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(X_train_pca.shape[1])])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(X_test_pca.shape[1])])\n",
    "X_val_pca = pd.DataFrame(X_val_pca, columns=[f'PC{i+1}' for i in range(X_val_pca.shape[1])])\n",
    "# Display the PCA results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd5135",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd90d4fd",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5947c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_jobs=8, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_jobs=8, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_jobs=8, random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs = 8)\n",
    "# Fit the model on the training data\n",
    "rf_classifier.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976a573",
   "metadata": {},
   "source": [
    "### Calculating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d640c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    63\n",
       "3    62\n",
       "4    62\n",
       "1    62\n",
       "5    62\n",
       "6    40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "450cbae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    200\n",
       "4    199\n",
       "2    199\n",
       "3    199\n",
       "5    198\n",
       "6    127\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94c86a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    50\n",
       "3    50\n",
       "4    50\n",
       "1    50\n",
       "5    50\n",
       "6    31\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71a5757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.95      0.97        62\n",
      "           2       0.52      0.62      0.57        63\n",
      "           3       0.60      0.58      0.59        62\n",
      "           4       0.46      0.50      0.48        62\n",
      "           5       0.45      0.50      0.47        62\n",
      "           6       0.95      0.45      0.61        40\n",
      "\n",
      "    accuracy                           0.61       351\n",
      "   macro avg       0.66      0.60      0.61       351\n",
      "weighted avg       0.64      0.61      0.61       351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_classifier.predict(X_test_pca)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Generate the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5bc7ec",
   "metadata": {},
   "source": [
    "### Plotting the feature importance of the top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d27730c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Get feature importances from the trained model\n",
    "importances = rf_classifier.feature_importances_\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train_pca.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the top 10 feature importances\n",
    "top_n = 10\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_importances['Feature'][:top_n], feature_importances['Importance'][:top_n], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importances from Random Forest Classifier')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9cec9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.79743203\n",
      "Iteration 2, loss = 1.78906840\n",
      "Iteration 3, loss = 1.78394265\n",
      "Iteration 4, loss = 1.77934717\n",
      "Iteration 5, loss = 1.77507918\n",
      "Iteration 6, loss = 1.77104259\n",
      "Iteration 7, loss = 1.76711800\n",
      "Iteration 8, loss = 1.76302719\n",
      "Iteration 9, loss = 1.75912143\n",
      "Iteration 10, loss = 1.75394678\n",
      "Iteration 11, loss = 1.74845489\n",
      "Iteration 12, loss = 1.74207904\n",
      "Iteration 13, loss = 1.73468217\n",
      "Iteration 14, loss = 1.72579578\n",
      "Iteration 15, loss = 1.71574940\n",
      "Iteration 16, loss = 1.70427272\n",
      "Iteration 17, loss = 1.69111666\n",
      "Iteration 18, loss = 1.67561519\n",
      "Iteration 19, loss = 1.65780876\n",
      "Iteration 20, loss = 1.63796704\n",
      "Iteration 21, loss = 1.61573325\n",
      "Iteration 22, loss = 1.59079131\n",
      "Iteration 23, loss = 1.56322587\n",
      "Iteration 24, loss = 1.53357830\n",
      "Iteration 25, loss = 1.50145060\n",
      "Iteration 26, loss = 1.46750897\n",
      "Iteration 27, loss = 1.43285763\n",
      "Iteration 28, loss = 1.39739835\n",
      "Iteration 29, loss = 1.36134290\n",
      "Iteration 30, loss = 1.32628075\n",
      "Iteration 31, loss = 1.29187235\n",
      "Iteration 32, loss = 1.25707173\n",
      "Iteration 33, loss = 1.22448756\n",
      "Iteration 34, loss = 1.19132521\n",
      "Iteration 35, loss = 1.15933332\n",
      "Iteration 36, loss = 1.12838557\n",
      "Iteration 37, loss = 1.09752728\n",
      "Iteration 38, loss = 1.06721981\n",
      "Iteration 39, loss = 1.03759521\n",
      "Iteration 40, loss = 1.00831690\n",
      "Iteration 41, loss = 0.97955173\n",
      "Iteration 42, loss = 0.95094203\n",
      "Iteration 43, loss = 0.92399405\n",
      "Iteration 44, loss = 0.89727897\n",
      "Iteration 45, loss = 0.87333821\n",
      "Iteration 46, loss = 0.84714641\n",
      "Iteration 47, loss = 0.82536853\n",
      "Iteration 48, loss = 0.80208286\n",
      "Iteration 49, loss = 0.78127010\n",
      "Iteration 50, loss = 0.75973966\n",
      "Iteration 51, loss = 0.74176160\n",
      "Iteration 52, loss = 0.72367207\n",
      "Iteration 53, loss = 0.70409828\n",
      "Iteration 54, loss = 0.68869516\n",
      "Iteration 55, loss = 0.67188923\n",
      "Iteration 56, loss = 0.65569653\n",
      "Iteration 57, loss = 0.64204244\n",
      "Iteration 58, loss = 0.62689105\n",
      "Iteration 59, loss = 0.61344247\n",
      "Iteration 60, loss = 0.60083938\n",
      "Iteration 61, loss = 0.58767664\n",
      "Iteration 62, loss = 0.57643292\n",
      "Iteration 63, loss = 0.56389264\n",
      "Iteration 64, loss = 0.55255082\n",
      "Iteration 65, loss = 0.54213426\n",
      "Iteration 66, loss = 0.53079557\n",
      "Iteration 67, loss = 0.51942792\n",
      "Iteration 68, loss = 0.50940720\n",
      "Iteration 69, loss = 0.49978029\n",
      "Iteration 70, loss = 0.49041361\n",
      "Iteration 71, loss = 0.48110091\n",
      "Iteration 72, loss = 0.47213039\n",
      "Iteration 73, loss = 0.46392412\n",
      "Iteration 74, loss = 0.45458923\n",
      "Iteration 75, loss = 0.44697999\n",
      "Iteration 76, loss = 0.43902083\n",
      "Iteration 77, loss = 0.43034086\n",
      "Iteration 78, loss = 0.42301705\n",
      "Iteration 79, loss = 0.41545216\n",
      "Iteration 80, loss = 0.40758993\n",
      "Iteration 81, loss = 0.40085989\n",
      "Iteration 82, loss = 0.39364775\n",
      "Iteration 83, loss = 0.38920731\n",
      "Iteration 84, loss = 0.38146170\n",
      "Iteration 85, loss = 0.37479571\n",
      "Iteration 86, loss = 0.36781222\n",
      "Iteration 87, loss = 0.36169524\n",
      "Iteration 88, loss = 0.35564466\n",
      "Iteration 89, loss = 0.34887574\n",
      "Iteration 90, loss = 0.34346948\n",
      "Iteration 91, loss = 0.33821892\n",
      "Iteration 92, loss = 0.33225432\n",
      "Iteration 93, loss = 0.32651617\n",
      "Iteration 94, loss = 0.32160039\n",
      "Iteration 95, loss = 0.31776331\n",
      "Iteration 96, loss = 0.31090705\n",
      "Iteration 97, loss = 0.30630940\n",
      "Iteration 98, loss = 0.30128414\n",
      "Iteration 99, loss = 0.29665321\n",
      "Iteration 100, loss = 0.29210080\n",
      "Iteration 101, loss = 0.28754329\n",
      "Iteration 102, loss = 0.28431508\n",
      "Iteration 103, loss = 0.27888318\n",
      "Iteration 104, loss = 0.27456854\n",
      "Iteration 105, loss = 0.27078788\n",
      "Iteration 106, loss = 0.26627569\n",
      "Iteration 107, loss = 0.26192490\n",
      "Iteration 108, loss = 0.25879643\n",
      "Iteration 109, loss = 0.25444839\n",
      "Iteration 110, loss = 0.25100789\n",
      "Iteration 111, loss = 0.24642991\n",
      "Iteration 112, loss = 0.24489445\n",
      "Iteration 113, loss = 0.23933796\n",
      "Iteration 114, loss = 0.23627790\n",
      "Iteration 115, loss = 0.23487958\n",
      "Iteration 116, loss = 0.23002603\n",
      "Iteration 117, loss = 0.22688039\n",
      "Iteration 118, loss = 0.22458090\n",
      "Iteration 119, loss = 0.21972211\n",
      "Iteration 120, loss = 0.21693724\n",
      "Iteration 121, loss = 0.21365644\n",
      "Iteration 122, loss = 0.21049416\n",
      "Iteration 123, loss = 0.20753806\n",
      "Iteration 124, loss = 0.20513433\n",
      "Iteration 125, loss = 0.20199178\n",
      "Iteration 126, loss = 0.19892660\n",
      "Iteration 127, loss = 0.19650415\n",
      "Iteration 128, loss = 0.19372422\n",
      "Iteration 129, loss = 0.19131843\n",
      "Iteration 130, loss = 0.18949717\n",
      "Iteration 131, loss = 0.18720730\n",
      "Iteration 132, loss = 0.18409699\n",
      "Iteration 133, loss = 0.18167451\n",
      "Iteration 134, loss = 0.18026894\n",
      "Iteration 135, loss = 0.17639245\n",
      "Iteration 136, loss = 0.17492215\n",
      "Iteration 137, loss = 0.17105895\n",
      "Iteration 138, loss = 0.16922206\n",
      "Iteration 139, loss = 0.16666146\n",
      "Iteration 140, loss = 0.16445066\n",
      "Iteration 141, loss = 0.16310201\n",
      "Iteration 142, loss = 0.16029556\n",
      "Iteration 143, loss = 0.15840185\n",
      "Iteration 144, loss = 0.15640876\n",
      "Iteration 145, loss = 0.15438153\n",
      "Iteration 146, loss = 0.15281046\n",
      "Iteration 147, loss = 0.15000373\n",
      "Iteration 148, loss = 0.14863943\n",
      "Iteration 149, loss = 0.14620027\n",
      "Iteration 150, loss = 0.14412438\n",
      "Iteration 151, loss = 0.14237921\n",
      "Iteration 152, loss = 0.14076082\n",
      "Iteration 153, loss = 0.13855287\n",
      "Iteration 154, loss = 0.13707905\n",
      "Iteration 155, loss = 0.13498066\n",
      "Iteration 156, loss = 0.13386182\n",
      "Iteration 157, loss = 0.13209389\n",
      "Iteration 158, loss = 0.13005991\n",
      "Iteration 159, loss = 0.12814364\n",
      "Iteration 160, loss = 0.12829851\n",
      "Iteration 161, loss = 0.12522450\n",
      "Iteration 162, loss = 0.12395589\n",
      "Iteration 163, loss = 0.12207352\n",
      "Iteration 164, loss = 0.12032238\n",
      "Iteration 165, loss = 0.11877019\n",
      "Iteration 166, loss = 0.11724411\n",
      "Iteration 167, loss = 0.11663756\n",
      "Iteration 168, loss = 0.11456540\n",
      "Iteration 169, loss = 0.11307258\n",
      "Iteration 170, loss = 0.11155319\n",
      "Iteration 171, loss = 0.11020636\n",
      "Iteration 172, loss = 0.10897009\n",
      "Iteration 173, loss = 0.10744751\n",
      "Iteration 174, loss = 0.10600068\n",
      "Iteration 175, loss = 0.10444203\n",
      "Iteration 176, loss = 0.10337738\n",
      "Iteration 177, loss = 0.10386208\n",
      "Iteration 178, loss = 0.10197048\n",
      "Iteration 179, loss = 0.09993853\n",
      "Iteration 180, loss = 0.09830098\n",
      "Iteration 181, loss = 0.09750494\n",
      "Iteration 182, loss = 0.09593062\n",
      "Iteration 183, loss = 0.09509048\n",
      "Iteration 184, loss = 0.09490623\n",
      "Iteration 185, loss = 0.09263267\n",
      "Iteration 186, loss = 0.09113449\n",
      "Iteration 187, loss = 0.09053492\n",
      "Iteration 188, loss = 0.08930729\n",
      "Iteration 189, loss = 0.08753033\n",
      "Iteration 190, loss = 0.08660288\n",
      "Iteration 191, loss = 0.08598611\n",
      "Iteration 192, loss = 0.08410032\n",
      "Iteration 193, loss = 0.08350426\n",
      "Iteration 194, loss = 0.08239520\n",
      "Iteration 195, loss = 0.08179300\n",
      "Iteration 196, loss = 0.08070251\n",
      "Iteration 197, loss = 0.07964729\n",
      "Iteration 198, loss = 0.07862482\n",
      "Iteration 199, loss = 0.07745351\n",
      "Iteration 200, loss = 0.07648163\n",
      "Iteration 201, loss = 0.07568690\n",
      "Iteration 202, loss = 0.07497462\n",
      "Iteration 203, loss = 0.07358029\n",
      "Iteration 204, loss = 0.07300648\n",
      "Iteration 205, loss = 0.07167184\n",
      "Iteration 206, loss = 0.07099915\n",
      "Iteration 207, loss = 0.07043803\n",
      "Iteration 208, loss = 0.06912498\n",
      "Iteration 209, loss = 0.06887038\n",
      "Iteration 210, loss = 0.06716260\n",
      "Iteration 211, loss = 0.06739903\n",
      "Iteration 212, loss = 0.06594105\n",
      "Iteration 213, loss = 0.06523261\n",
      "Iteration 214, loss = 0.06424564\n",
      "Iteration 215, loss = 0.06350670\n",
      "Iteration 216, loss = 0.06266669\n",
      "Iteration 217, loss = 0.06199795\n",
      "Iteration 218, loss = 0.06129824\n",
      "Iteration 219, loss = 0.06066986\n",
      "Iteration 220, loss = 0.05970500\n",
      "Iteration 221, loss = 0.05921780\n",
      "Iteration 222, loss = 0.05825767\n",
      "Iteration 223, loss = 0.05794582\n",
      "Iteration 224, loss = 0.05703300\n",
      "Iteration 225, loss = 0.05630783\n",
      "Iteration 226, loss = 0.05598378\n",
      "Iteration 227, loss = 0.05553449\n",
      "Iteration 228, loss = 0.05536203\n",
      "Iteration 229, loss = 0.05347960\n",
      "Iteration 230, loss = 0.05371939\n",
      "Iteration 231, loss = 0.05239362\n",
      "Iteration 232, loss = 0.05182050\n",
      "Iteration 233, loss = 0.05136816\n",
      "Iteration 234, loss = 0.05059739\n",
      "Iteration 235, loss = 0.05012664\n",
      "Iteration 236, loss = 0.04954790\n",
      "Iteration 237, loss = 0.04882493\n",
      "Iteration 238, loss = 0.04842548\n",
      "Iteration 239, loss = 0.04765070\n",
      "Iteration 240, loss = 0.04728283\n",
      "Iteration 241, loss = 0.04667860\n",
      "Iteration 242, loss = 0.04627832\n",
      "Iteration 243, loss = 0.04544429\n",
      "Iteration 244, loss = 0.04512448\n",
      "Iteration 245, loss = 0.04468491\n",
      "Iteration 246, loss = 0.04404109\n",
      "Iteration 247, loss = 0.04358298\n",
      "Iteration 248, loss = 0.04290218\n",
      "Iteration 249, loss = 0.04277123\n",
      "Iteration 250, loss = 0.04202474\n",
      "Iteration 251, loss = 0.04161890\n",
      "Iteration 252, loss = 0.04131328\n",
      "Iteration 253, loss = 0.04078540\n",
      "Iteration 254, loss = 0.04018891\n",
      "Iteration 255, loss = 0.03982893\n",
      "Iteration 256, loss = 0.03949924\n",
      "Iteration 257, loss = 0.03899354\n",
      "Iteration 258, loss = 0.03842550\n",
      "Iteration 259, loss = 0.03812655\n",
      "Iteration 260, loss = 0.03767514\n",
      "Iteration 261, loss = 0.03728163\n",
      "Iteration 262, loss = 0.03689990\n",
      "Iteration 263, loss = 0.03642141\n",
      "Iteration 264, loss = 0.03612708\n",
      "Iteration 265, loss = 0.03562190\n",
      "Iteration 266, loss = 0.03550232\n",
      "Iteration 267, loss = 0.03532262\n",
      "Iteration 268, loss = 0.03480655\n",
      "Iteration 269, loss = 0.03411397\n",
      "Iteration 270, loss = 0.03397666\n",
      "Iteration 271, loss = 0.03363758\n",
      "Iteration 272, loss = 0.03310530\n",
      "Iteration 273, loss = 0.03308439\n",
      "Iteration 274, loss = 0.03265662\n",
      "Iteration 275, loss = 0.03222600\n",
      "Iteration 276, loss = 0.03175954\n",
      "Iteration 277, loss = 0.03151655\n",
      "Iteration 278, loss = 0.03109565\n",
      "Iteration 279, loss = 0.03078419\n",
      "Iteration 280, loss = 0.03048551\n",
      "Iteration 281, loss = 0.03013900\n",
      "Iteration 282, loss = 0.02996926\n",
      "Iteration 283, loss = 0.02962031\n",
      "Iteration 284, loss = 0.02934170\n",
      "Iteration 285, loss = 0.02900245\n",
      "Iteration 286, loss = 0.02887533\n",
      "Iteration 287, loss = 0.02840434\n",
      "Iteration 288, loss = 0.02828761\n",
      "Iteration 289, loss = 0.02785271\n",
      "Iteration 290, loss = 0.02768420\n",
      "Iteration 291, loss = 0.02740971\n",
      "Iteration 292, loss = 0.02711522\n",
      "Iteration 293, loss = 0.02691430\n",
      "Iteration 294, loss = 0.02655606\n",
      "Iteration 295, loss = 0.02638113\n",
      "Iteration 296, loss = 0.02618774\n",
      "Iteration 297, loss = 0.02596201\n",
      "Iteration 298, loss = 0.02591616\n",
      "Iteration 299, loss = 0.02566195\n",
      "Iteration 300, loss = 0.02513147\n",
      "Iteration 301, loss = 0.02512941\n",
      "Iteration 302, loss = 0.02478862\n",
      "Iteration 303, loss = 0.02459874\n",
      "Iteration 304, loss = 0.02424390\n",
      "Iteration 305, loss = 0.02409169\n",
      "Iteration 306, loss = 0.02381514\n",
      "Iteration 307, loss = 0.02358000\n",
      "Iteration 308, loss = 0.02342007\n",
      "Iteration 309, loss = 0.02317166\n",
      "Iteration 310, loss = 0.02301390\n",
      "Iteration 311, loss = 0.02283480\n",
      "Iteration 312, loss = 0.02259054\n",
      "Iteration 313, loss = 0.02237808\n",
      "Iteration 314, loss = 0.02220137\n",
      "Iteration 315, loss = 0.02202199\n",
      "Iteration 316, loss = 0.02181655\n",
      "Iteration 317, loss = 0.02166555\n",
      "Iteration 318, loss = 0.02154018\n",
      "Iteration 319, loss = 0.02130212\n",
      "Iteration 320, loss = 0.02112883\n",
      "Iteration 321, loss = 0.02089943\n",
      "Iteration 322, loss = 0.02081574\n",
      "Iteration 323, loss = 0.02061295\n",
      "Iteration 324, loss = 0.02048849\n",
      "Iteration 325, loss = 0.02033895\n",
      "Iteration 326, loss = 0.02007706\n",
      "Iteration 327, loss = 0.01995390\n",
      "Iteration 328, loss = 0.01974167\n",
      "Iteration 329, loss = 0.01963051\n",
      "Iteration 330, loss = 0.01956315\n",
      "Iteration 331, loss = 0.01927257\n",
      "Iteration 332, loss = 0.01919200\n",
      "Iteration 333, loss = 0.01901109\n",
      "Iteration 334, loss = 0.01883549\n",
      "Iteration 335, loss = 0.01869177\n",
      "Iteration 336, loss = 0.01856136\n",
      "Iteration 337, loss = 0.01843907\n",
      "Iteration 338, loss = 0.01829622\n",
      "Iteration 339, loss = 0.01814524\n",
      "Iteration 340, loss = 0.01801086\n",
      "Iteration 341, loss = 0.01788884\n",
      "Iteration 342, loss = 0.01769654\n",
      "Iteration 343, loss = 0.01769383\n",
      "Iteration 344, loss = 0.01750899\n",
      "Iteration 345, loss = 0.01736880\n",
      "Iteration 346, loss = 0.01722189\n",
      "Iteration 347, loss = 0.01711556\n",
      "Iteration 348, loss = 0.01696594\n",
      "Iteration 349, loss = 0.01687054\n",
      "Iteration 350, loss = 0.01672203\n",
      "Iteration 351, loss = 0.01666451\n",
      "Iteration 352, loss = 0.01652159\n",
      "Iteration 353, loss = 0.01641684\n",
      "Iteration 354, loss = 0.01632220\n",
      "Iteration 355, loss = 0.01617786\n",
      "Iteration 356, loss = 0.01604935\n",
      "Iteration 357, loss = 0.01599791\n",
      "Iteration 358, loss = 0.01585935\n",
      "Iteration 359, loss = 0.01577008\n",
      "Iteration 360, loss = 0.01568123\n",
      "Iteration 361, loss = 0.01566831\n",
      "Iteration 362, loss = 0.01555847\n",
      "Iteration 363, loss = 0.01535502\n",
      "Iteration 364, loss = 0.01530176\n",
      "Iteration 365, loss = 0.01519061\n",
      "Iteration 366, loss = 0.01507484\n",
      "Iteration 367, loss = 0.01494157\n",
      "Iteration 368, loss = 0.01483614\n",
      "Iteration 369, loss = 0.01474074\n",
      "Iteration 370, loss = 0.01466027\n",
      "Iteration 371, loss = 0.01457245\n",
      "Iteration 372, loss = 0.01447466\n",
      "Iteration 373, loss = 0.01438691\n",
      "Iteration 374, loss = 0.01430212\n",
      "Iteration 375, loss = 0.01422506\n",
      "Iteration 376, loss = 0.01410959\n",
      "Iteration 377, loss = 0.01403868\n",
      "Iteration 378, loss = 0.01396590\n",
      "Iteration 379, loss = 0.01390103\n",
      "Iteration 380, loss = 0.01377262\n",
      "Iteration 381, loss = 0.01376315\n",
      "Iteration 382, loss = 0.01359647\n",
      "Iteration 383, loss = 0.01354660\n",
      "Iteration 384, loss = 0.01346149\n",
      "Iteration 385, loss = 0.01338408\n",
      "Iteration 386, loss = 0.01331302\n",
      "Iteration 387, loss = 0.01326258\n",
      "Iteration 388, loss = 0.01317537\n",
      "Iteration 389, loss = 0.01305759\n",
      "Iteration 390, loss = 0.01303130\n",
      "Iteration 391, loss = 0.01293459\n",
      "Iteration 392, loss = 0.01289372\n",
      "Iteration 393, loss = 0.01286134\n",
      "Iteration 394, loss = 0.01273785\n",
      "Iteration 395, loss = 0.01268218\n",
      "Iteration 396, loss = 0.01263290\n",
      "Iteration 397, loss = 0.01252506\n",
      "Iteration 398, loss = 0.01246658\n",
      "Iteration 399, loss = 0.01237731\n",
      "Iteration 400, loss = 0.01233606\n",
      "Iteration 401, loss = 0.01226726\n",
      "Iteration 402, loss = 0.01219378\n",
      "Iteration 403, loss = 0.01215639\n",
      "Iteration 404, loss = 0.01209543\n",
      "Iteration 405, loss = 0.01200804\n",
      "Iteration 406, loss = 0.01196147\n",
      "Iteration 407, loss = 0.01189105\n",
      "Iteration 408, loss = 0.01182613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP Validation Score: 0.8114\n",
      "MLP Test Score: 0.8348\n"
     ]
    }
   ],
   "source": [
    "#multi-layer perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Create a Multi-layer Perceptron Classifier and evaluate on validation set\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42, verbose = True)\n",
    "mlp_classifier.fit(X_train_pca, y_train)\n",
    "# Evaluate the model on the validation set\n",
    "mlp_val_score = mlp_classifier.score(X_val_pca, y_val)\n",
    "print(f'MLP Validation Score: {mlp_val_score:.4f}')\n",
    "\n",
    "#test score\n",
    "mlp_test_score = mlp_classifier.score(X_test_pca, y_test)\n",
    "print(f'MLP Test Score: {mlp_test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c83a9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_train_tensor: tensor([0, 1, 2, 3, 4, 5])\n",
      "Unique values in y_val_tensor: tensor([0, 1, 2, 3, 4, 5])\n",
      "Unique values in y_test_tensor: tensor([0, 1, 2, 3, 4, 5])\n",
      "num_classes: 6\n",
      "X_train_tensor has NaNs: tensor(False)\n",
      "X_train_tensor has Infs: tensor(False)\n",
      "X_val_tensor has NaNs: tensor(False)\n",
      "X_val_tensor has Infs: tensor(False)\n",
      "X_test_tensor has NaNs: tensor(False)\n",
      "X_test_tensor has Infs: tensor(False)\n",
      "Using device: cuda\n",
      "Epoch [1/5000], Loss: 1.7565, Val Loss: 1.6173\n",
      "Epoch [2/5000], Loss: 1.3340, Val Loss: 1.0426\n",
      "Epoch [3/5000], Loss: 0.9889, Val Loss: 0.8641\n",
      "Epoch [4/5000], Loss: 0.8076, Val Loss: 0.7189\n",
      "Epoch [5/5000], Loss: 0.6562, Val Loss: 0.6075\n",
      "Epoch [6/5000], Loss: 0.5656, Val Loss: 0.5652\n",
      "Epoch [7/5000], Loss: 0.4386, Val Loss: 0.6079\n",
      "Epoch [8/5000], Loss: 0.3782, Val Loss: 0.5783\n",
      "Epoch [9/5000], Loss: 0.3777, Val Loss: 0.5095\n",
      "Epoch [10/5000], Loss: 0.2791, Val Loss: 0.5081\n",
      "Epoch [11/5000], Loss: 0.2367, Val Loss: 0.5219\n",
      "Epoch [12/5000], Loss: 0.2060, Val Loss: 0.5356\n",
      "Epoch [13/5000], Loss: 0.1855, Val Loss: 0.5102\n",
      "Epoch [14/5000], Loss: 0.1659, Val Loss: 0.5265\n",
      "Epoch [15/5000], Loss: 0.1515, Val Loss: 0.5680\n",
      "Epoch [16/5000], Loss: 0.1352, Val Loss: 0.5709\n",
      "Epoch [17/5000], Loss: 0.1406, Val Loss: 0.6994\n",
      "Epoch [18/5000], Loss: 0.1086, Val Loss: 0.6972\n",
      "Epoch [19/5000], Loss: 0.1466, Val Loss: 0.6276\n",
      "Epoch [20/5000], Loss: 0.0932, Val Loss: 0.7531\n",
      "Epoch [21/5000], Loss: 0.0821, Val Loss: 0.6461\n",
      "Epoch [22/5000], Loss: 0.0658, Val Loss: 0.7166\n",
      "Epoch [23/5000], Loss: 0.0673, Val Loss: 0.7105\n",
      "Epoch [24/5000], Loss: 0.0640, Val Loss: 0.7236\n",
      "Epoch [25/5000], Loss: 0.0426, Val Loss: 0.8006\n",
      "Epoch [26/5000], Loss: 0.0347, Val Loss: 0.8514\n",
      "Epoch [27/5000], Loss: 0.0296, Val Loss: 0.8666\n",
      "Epoch [28/5000], Loss: 0.0705, Val Loss: 0.7738\n",
      "Epoch [29/5000], Loss: 0.0372, Val Loss: 0.8095\n",
      "Epoch [30/5000], Loss: 0.0328, Val Loss: 0.8566\n",
      "Epoch [31/5000], Loss: 0.0365, Val Loss: 0.7631\n",
      "Epoch [32/5000], Loss: 0.0281, Val Loss: 0.8158\n",
      "Epoch [33/5000], Loss: 0.0231, Val Loss: 0.9013\n",
      "Epoch [34/5000], Loss: 0.0174, Val Loss: 0.9086\n",
      "Epoch [35/5000], Loss: 0.0619, Val Loss: 0.8779\n",
      "Epoch [36/5000], Loss: 0.0429, Val Loss: 0.8177\n",
      "Epoch [37/5000], Loss: 0.0204, Val Loss: 0.8113\n",
      "Epoch [38/5000], Loss: 0.0153, Val Loss: 0.9119\n",
      "Epoch [39/5000], Loss: 0.0165, Val Loss: 0.9211\n",
      "Epoch [40/5000], Loss: 0.0142, Val Loss: 0.9564\n",
      "Epoch [41/5000], Loss: 0.0088, Val Loss: 0.9682\n",
      "Epoch [42/5000], Loss: 0.0114, Val Loss: 1.0085\n",
      "Epoch [43/5000], Loss: 0.0089, Val Loss: 0.9744\n",
      "Epoch [44/5000], Loss: 0.0065, Val Loss: 0.9884\n",
      "Epoch [45/5000], Loss: 0.0174, Val Loss: 0.9635\n",
      "Epoch [46/5000], Loss: 0.0085, Val Loss: 1.0213\n",
      "Epoch [47/5000], Loss: 0.0088, Val Loss: 1.0850\n",
      "Epoch [48/5000], Loss: 0.1156, Val Loss: 1.0626\n",
      "Epoch [49/5000], Loss: 0.0308, Val Loss: 0.8580\n",
      "Epoch [50/5000], Loss: 0.0279, Val Loss: 0.9277\n",
      "Epoch [51/5000], Loss: 0.0206, Val Loss: 0.8093\n",
      "Epoch [52/5000], Loss: 0.0144, Val Loss: 0.9375\n",
      "Epoch [53/5000], Loss: 0.0215, Val Loss: 0.9041\n",
      "Epoch [54/5000], Loss: 0.0183, Val Loss: 0.9674\n",
      "Epoch [55/5000], Loss: 0.0100, Val Loss: 1.0001\n",
      "Epoch [56/5000], Loss: 0.0070, Val Loss: 1.0444\n",
      "Epoch [57/5000], Loss: 0.0063, Val Loss: 0.9832\n",
      "Epoch [58/5000], Loss: 0.0060, Val Loss: 1.0051\n",
      "Epoch [59/5000], Loss: 0.0045, Val Loss: 1.0932\n",
      "Epoch [60/5000], Loss: 0.0461, Val Loss: 0.9290\n",
      "Epoch [61/5000], Loss: 0.0242, Val Loss: 0.9542\n",
      "Epoch [62/5000], Loss: 0.0081, Val Loss: 0.9468\n",
      "Epoch [63/5000], Loss: 0.0081, Val Loss: 1.0858\n",
      "Epoch [64/5000], Loss: 0.0058, Val Loss: 1.0723\n",
      "Epoch [65/5000], Loss: 0.0096, Val Loss: 1.0191\n",
      "Epoch [66/5000], Loss: 0.0101, Val Loss: 1.0580\n",
      "Epoch [67/5000], Loss: 0.0065, Val Loss: 1.0353\n",
      "Epoch [68/5000], Loss: 0.0051, Val Loss: 1.1753\n",
      "Epoch [69/5000], Loss: 0.0033, Val Loss: 1.0775\n",
      "Epoch [70/5000], Loss: 0.0022, Val Loss: 1.0723\n",
      "Epoch [71/5000], Loss: 0.0023, Val Loss: 1.1701\n",
      "Epoch [72/5000], Loss: 0.0030, Val Loss: 1.1243\n",
      "Epoch [73/5000], Loss: 0.0033, Val Loss: 1.1846\n",
      "Epoch [74/5000], Loss: 0.0033, Val Loss: 1.1871\n",
      "Epoch [75/5000], Loss: 0.0014, Val Loss: 1.1801\n",
      "Epoch [76/5000], Loss: 0.0044, Val Loss: 1.1780\n",
      "Epoch [77/5000], Loss: 0.0022, Val Loss: 1.1823\n",
      "Epoch [78/5000], Loss: 0.0031, Val Loss: 1.1793\n",
      "Epoch [79/5000], Loss: 0.0022, Val Loss: 1.2095\n",
      "Epoch [80/5000], Loss: 0.0017, Val Loss: 1.1605\n",
      "Epoch [81/5000], Loss: 0.0023, Val Loss: 1.2427\n",
      "Epoch [82/5000], Loss: 0.0009, Val Loss: 1.2132\n",
      "Epoch [83/5000], Loss: 0.0013, Val Loss: 1.2739\n",
      "Epoch [84/5000], Loss: 0.0009, Val Loss: 1.2486\n",
      "Epoch [85/5000], Loss: 0.0010, Val Loss: 1.2524\n",
      "Epoch [86/5000], Loss: 0.0021, Val Loss: 1.2868\n",
      "Epoch [87/5000], Loss: 0.0030, Val Loss: 1.2575\n",
      "Epoch [88/5000], Loss: 0.0047, Val Loss: 1.2447\n",
      "Epoch [89/5000], Loss: 0.0021, Val Loss: 1.2534\n",
      "Epoch [90/5000], Loss: 0.0018, Val Loss: 1.2344\n",
      "Epoch [91/5000], Loss: 0.0043, Val Loss: 1.1773\n",
      "Epoch [92/5000], Loss: 0.0071, Val Loss: 1.2011\n",
      "Epoch [93/5000], Loss: 0.0095, Val Loss: 1.1563\n",
      "Epoch [94/5000], Loss: 0.0029, Val Loss: 1.1964\n",
      "Epoch [95/5000], Loss: 0.0117, Val Loss: 1.2197\n",
      "Epoch [96/5000], Loss: 0.0146, Val Loss: 1.1428\n",
      "Epoch [97/5000], Loss: 0.0130, Val Loss: 1.1962\n",
      "Epoch [98/5000], Loss: 0.0123, Val Loss: 1.2993\n",
      "Epoch [99/5000], Loss: 0.0227, Val Loss: 1.1399\n",
      "Epoch [100/5000], Loss: 0.0200, Val Loss: 1.5125\n",
      "Epoch [101/5000], Loss: 0.0407, Val Loss: 1.1836\n",
      "Epoch [102/5000], Loss: 0.0115, Val Loss: 1.1027\n",
      "Epoch [103/5000], Loss: 0.0127, Val Loss: 1.1209\n",
      "Epoch [104/5000], Loss: 0.0050, Val Loss: 1.2219\n",
      "Epoch [105/5000], Loss: 0.0086, Val Loss: 1.1790\n",
      "Epoch [106/5000], Loss: 0.0103, Val Loss: 1.2713\n",
      "Epoch [107/5000], Loss: 0.0112, Val Loss: 1.3076\n",
      "Epoch [108/5000], Loss: 0.0054, Val Loss: 1.1582\n",
      "Epoch [109/5000], Loss: 0.0061, Val Loss: 1.4205\n",
      "Epoch [110/5000], Loss: 0.0042, Val Loss: 1.2107\n",
      "Epoch [111/5000], Loss: 0.0013, Val Loss: 1.2369\n",
      "Epoch [112/5000], Loss: 0.0039, Val Loss: 1.3110\n",
      "Epoch [113/5000], Loss: 0.0025, Val Loss: 1.3165\n",
      "Epoch [114/5000], Loss: 0.0014, Val Loss: 1.3059\n",
      "Epoch [115/5000], Loss: 0.0020, Val Loss: 1.4352\n",
      "Epoch [116/5000], Loss: 0.0109, Val Loss: 1.3151\n",
      "Epoch [117/5000], Loss: 0.0027, Val Loss: 1.2195\n",
      "Epoch [118/5000], Loss: 0.0038, Val Loss: 1.3552\n",
      "Epoch [119/5000], Loss: 0.0023, Val Loss: 1.4137\n",
      "Epoch [120/5000], Loss: 0.0012, Val Loss: 1.2504\n",
      "Epoch [121/5000], Loss: 0.0017, Val Loss: 1.3290\n",
      "Epoch [122/5000], Loss: 0.0011, Val Loss: 1.3130\n",
      "Epoch [123/5000], Loss: 0.0007, Val Loss: 1.4257\n",
      "Epoch [124/5000], Loss: 0.0013, Val Loss: 1.3322\n",
      "Epoch [125/5000], Loss: 0.0012, Val Loss: 1.4533\n",
      "Epoch [126/5000], Loss: 0.0006, Val Loss: 1.2558\n",
      "Epoch [127/5000], Loss: 0.0012, Val Loss: 1.4236\n",
      "Epoch [128/5000], Loss: 0.0014, Val Loss: 1.3577\n",
      "Epoch [129/5000], Loss: 0.0068, Val Loss: 1.2344\n",
      "Epoch [130/5000], Loss: 0.0022, Val Loss: 1.2505\n",
      "Epoch [131/5000], Loss: 0.0032, Val Loss: 1.4696\n",
      "Epoch [132/5000], Loss: 0.0019, Val Loss: 1.3634\n",
      "Epoch [133/5000], Loss: 0.0035, Val Loss: 1.5285\n",
      "Epoch [134/5000], Loss: 0.0106, Val Loss: 1.3516\n",
      "Epoch [135/5000], Loss: 0.1244, Val Loss: 1.3628\n",
      "Epoch [136/5000], Loss: 0.0898, Val Loss: 1.0352\n",
      "Epoch [137/5000], Loss: 0.0302, Val Loss: 1.0486\n",
      "Epoch [138/5000], Loss: 0.0196, Val Loss: 1.0590\n",
      "Epoch [139/5000], Loss: 0.0068, Val Loss: 1.2367\n",
      "Epoch [140/5000], Loss: 0.0122, Val Loss: 1.1490\n",
      "Epoch [141/5000], Loss: 0.0036, Val Loss: 1.1186\n",
      "Epoch [142/5000], Loss: 0.0047, Val Loss: 1.1552\n",
      "Epoch [143/5000], Loss: 0.0085, Val Loss: 1.1667\n",
      "Epoch [144/5000], Loss: 0.0029, Val Loss: 1.2235\n",
      "Epoch [145/5000], Loss: 0.0053, Val Loss: 1.2209\n",
      "Epoch [146/5000], Loss: 0.0052, Val Loss: 1.3495\n",
      "Epoch [147/5000], Loss: 0.0030, Val Loss: 1.2489\n",
      "Epoch [148/5000], Loss: 0.0070, Val Loss: 1.3845\n",
      "Epoch [149/5000], Loss: 0.0376, Val Loss: 1.2813\n",
      "Epoch [150/5000], Loss: 0.0239, Val Loss: 1.3876\n",
      "Epoch [151/5000], Loss: 0.0206, Val Loss: 1.3036\n",
      "Epoch [152/5000], Loss: 0.0130, Val Loss: 1.3649\n",
      "Epoch [153/5000], Loss: 0.0034, Val Loss: 1.4610\n",
      "Epoch [154/5000], Loss: 0.0079, Val Loss: 1.3553\n",
      "Epoch [155/5000], Loss: 0.0070, Val Loss: 1.3545\n",
      "Epoch [156/5000], Loss: 0.0117, Val Loss: 1.3119\n",
      "Epoch [157/5000], Loss: 0.0081, Val Loss: 1.6042\n",
      "Epoch [158/5000], Loss: 0.0026, Val Loss: 1.3084\n",
      "Epoch [159/5000], Loss: 0.0018, Val Loss: 1.3442\n",
      "Epoch [160/5000], Loss: 0.0067, Val Loss: 1.5150\n",
      "Epoch [161/5000], Loss: 0.0026, Val Loss: 1.4656\n",
      "Epoch [162/5000], Loss: 0.0031, Val Loss: 1.4440\n",
      "Epoch [163/5000], Loss: 0.0053, Val Loss: 1.5706\n",
      "Epoch [164/5000], Loss: 0.0007, Val Loss: 1.5266\n",
      "Epoch [165/5000], Loss: 0.0014, Val Loss: 1.5642\n",
      "Epoch [166/5000], Loss: 0.0078, Val Loss: 1.5884\n",
      "Epoch [167/5000], Loss: 0.0022, Val Loss: 1.4567\n",
      "Epoch [168/5000], Loss: 0.0073, Val Loss: 1.4547\n",
      "Epoch [169/5000], Loss: 0.0075, Val Loss: 1.4088\n",
      "Epoch [170/5000], Loss: 0.0011, Val Loss: 1.3873\n",
      "Epoch [171/5000], Loss: 0.0065, Val Loss: 1.5462\n",
      "Epoch [172/5000], Loss: 0.0014, Val Loss: 1.3985\n",
      "Epoch [173/5000], Loss: 0.0008, Val Loss: 1.4553\n",
      "Epoch [174/5000], Loss: 0.0023, Val Loss: 1.4815\n",
      "Epoch [175/5000], Loss: 0.0053, Val Loss: 1.3688\n",
      "Epoch [176/5000], Loss: 0.0020, Val Loss: 1.4504\n",
      "Epoch [177/5000], Loss: 0.0016, Val Loss: 1.5782\n",
      "Epoch [178/5000], Loss: 0.0018, Val Loss: 1.6526\n",
      "Epoch [179/5000], Loss: 0.0076, Val Loss: 1.6203\n",
      "Epoch [180/5000], Loss: 0.0050, Val Loss: 1.4918\n",
      "Epoch [181/5000], Loss: 0.0039, Val Loss: 1.3772\n",
      "Epoch [182/5000], Loss: 0.0054, Val Loss: 1.6540\n",
      "Epoch [183/5000], Loss: 0.0076, Val Loss: 1.6552\n",
      "Epoch [184/5000], Loss: 0.0037, Val Loss: 1.7942\n",
      "Epoch [185/5000], Loss: 0.0065, Val Loss: 1.6163\n",
      "Epoch [186/5000], Loss: 0.0014, Val Loss: 1.6178\n",
      "Epoch [187/5000], Loss: 0.0013, Val Loss: 1.6248\n",
      "Epoch [188/5000], Loss: 0.0009, Val Loss: 1.6746\n",
      "Epoch [189/5000], Loss: 0.0010, Val Loss: 1.6154\n",
      "Epoch [190/5000], Loss: 0.0020, Val Loss: 1.9507\n",
      "Epoch [191/5000], Loss: 0.0075, Val Loss: 1.7027\n",
      "Epoch [192/5000], Loss: 0.0046, Val Loss: 1.3893\n",
      "Epoch [193/5000], Loss: 0.0035, Val Loss: 1.5520\n",
      "Epoch [194/5000], Loss: 0.0121, Val Loss: 1.3244\n",
      "Epoch [195/5000], Loss: 0.0090, Val Loss: 1.2814\n",
      "Epoch [196/5000], Loss: 0.0110, Val Loss: 1.4459\n",
      "Epoch [197/5000], Loss: 0.0235, Val Loss: 1.4072\n",
      "Epoch [198/5000], Loss: 0.0422, Val Loss: 1.6132\n",
      "Epoch [199/5000], Loss: 0.0330, Val Loss: 1.4386\n",
      "Epoch [200/5000], Loss: 0.0285, Val Loss: 1.4060\n",
      "Epoch [201/5000], Loss: 0.0293, Val Loss: 1.3474\n",
      "Epoch [202/5000], Loss: 0.0150, Val Loss: 1.2764\n",
      "Epoch [203/5000], Loss: 0.0064, Val Loss: 1.2865\n",
      "Epoch [204/5000], Loss: 0.0059, Val Loss: 1.3951\n",
      "Epoch [205/5000], Loss: 0.0020, Val Loss: 1.3522\n",
      "Epoch [206/5000], Loss: 0.0010, Val Loss: 1.3545\n",
      "Epoch [207/5000], Loss: 0.0006, Val Loss: 1.3580\n",
      "Epoch [208/5000], Loss: 0.0005, Val Loss: 1.3735\n",
      "Epoch [209/5000], Loss: 0.0010, Val Loss: 1.4096\n",
      "Epoch [210/5000], Loss: 0.0005, Val Loss: 1.4534\n",
      "Early stopping triggered\n",
      "Loading best model from best_model.pth\n",
      "Test Loss: 0.4033, Test Accuracy: 0.8405\n",
      "Final model state saved to final_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\AppData\\Local\\Temp\\ipykernel_11628\\3050270799.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd # Assuming X_train_pca is a pandas DataFrame\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Encode labels to start from 0 using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Fit on all unique labels across all sets to ensure consistency\n",
    "le.fit(np.unique(np.concatenate([y_train, y_val, y_test])))\n",
    "\n",
    "# Transform all label sets\n",
    "y_train_tensor = torch.tensor(le.transform(y_train), dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(le.transform(y_val), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(le.transform(y_test), dtype=torch.long)\n",
    "\n",
    "# Define number of classes\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Debug: Check label ranges\n",
    "print(\"Unique values in y_train_tensor:\", torch.unique(y_train_tensor))\n",
    "print(\"Unique values in y_val_tensor:\", torch.unique(y_val_tensor))\n",
    "print(\"Unique values in y_test_tensor:\", torch.unique(y_test_tensor))\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "# Assertions for label range (already present, good practice)\n",
    "assert torch.all(y_train_tensor >= 0) and torch.all(y_train_tensor < num_classes), \\\n",
    "    f\"y_train_tensor has out-of-range labels. Min: {y_train_tensor.min()}, Max: {y_train_tensor.max()}, Expected range [0, {num_classes-1}]\"\n",
    "assert torch.all(y_val_tensor >= 0) and torch.all(y_val_tensor < num_classes), \\\n",
    "    f\"y_val_tensor has out-of-range labels. Min: {y_val_tensor.min()}, Max: {y_val_tensor.max()}, Expected range [0, {num_classes-1}]\"\n",
    "assert torch.all(y_test_tensor >= 0) and torch.all(y_test_tensor < num_classes), \\\n",
    "    f\"y_test_tensor has out-of-range labels. Min: {y_test_tensor.min()}, Max: {y_test_tensor.max()}, Expected range [0, {num_classes-1}]\"\n",
    "\n",
    "\n",
    "# Convert PCA features to PyTorch tensors\n",
    "# Ensure .values is used if X_pca is a pandas DataFrame\n",
    "X_train_tensor = torch.tensor(X_train_pca.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_pca.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_pca.values, dtype=torch.float32)\n",
    "\n",
    "# Check for NaNs/Infs in input tensors\n",
    "print(\"X_train_tensor has NaNs:\", torch.isnan(X_train_tensor).any())\n",
    "print(\"X_train_tensor has Infs:\", torch.isinf(X_train_tensor).any())\n",
    "print(\"X_val_tensor has NaNs:\", torch.isnan(X_val_tensor).any())\n",
    "print(\"X_val_tensor has Infs:\", torch.isinf(X_val_tensor).any())\n",
    "print(\"X_test_tensor has NaNs:\", torch.isnan(X_test_tensor).any())\n",
    "print(\"X_test_tensor has Infs:\", torch.isinf(X_test_tensor).any())\n",
    "\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x) # No activation here, as CrossEntropyLoss expects logits\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = X_train_pca.shape[1]\n",
    "model = SimpleNN(input_size, num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 5000\n",
    "patience = 200\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "best_model_path = 'best_model.pth' # Define path for saving best model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)  # Save best model\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Load best model for evaluation\n",
    "print(f\"Loading best model from {best_model_path}\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "with torch.no_grad():\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(X_test_tensor)\n",
    "test_accuracy = test_correct / len(X_test_tensor)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Save final model (optional, often you'd just use the best_model.pth for deployment)\n",
    "final_model_path = 'final_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model state saved to {final_model_path}\")\n",
    "\n",
    "# Load model for inference (example)\n",
    "# loaded_model = SimpleNN(input_size, num_classes)\n",
    "# loaded_model.load_state_dict(torch.load('final_model.pth'))\n",
    "# loaded_model.to(device) # Don't forget to move it to device if you want to use it on GPU\n",
    "# loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b68b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35bec12",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['PC1', 'PC3', 'PC2', 'PC7', 'PC4', 'PC5', 'PC15', 'PC6', 'PC12',\\n       'PC44'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m top_features \u001b[38;5;241m=\u001b[39m feature_importances[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m][:top_n]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Filter the training and testing sets to keep only the top features\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_train_top \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtop_features\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m X_test_top \u001b[38;5;241m=\u001b[39m X_test[top_features]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train the Random Forest Classifier on the top features\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adnane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Adnane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adnane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['PC1', 'PC3', 'PC2', 'PC7', 'PC4', 'PC5', 'PC15', 'PC6', 'PC12',\\n       'PC44'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "#only keep the top 10 features\n",
    "top_features = feature_importances['Feature'][:top_n].tolist()\n",
    "# Filter the training and testing sets to keep only the top features\n",
    "X_train_top = X_train[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "# Train the Random Forest Classifier on the top features\n",
    "rf_classifier_top = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=8)\n",
    "rf_classifier_top.fit(X_train_top, y_train)\n",
    "# Evaluate the model on the test set\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier_top.predict(X_test_top)\n",
    "#print the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of Random Forest Classifier on test set: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef2820f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8005698005698005\n"
     ]
    }
   ],
   "source": [
    "#evaluate teh model on the test set\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test_pca)\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
