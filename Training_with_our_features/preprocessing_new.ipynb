{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ================================================================\n",
    "#   INSHEP bulk-feature extractor — **multicore version**\n",
    "#   ▪  Spawns one worker per logical CPU (max 12 on your machine)\n",
    "#   ▪  Streams results straight into CSV  (appends row-by-row)\n",
    "#   ▪  Totally self-contained: just place this script beside the\n",
    "#       datasets/  directory and  python fast_extract.py\n",
    "# ================================================================\n",
    "import os, math, warnings, csv\n",
    "from pathlib import Path\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal, stats\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  optional pseudo-Zernike moments  (needs  pip install mahotas)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "try:\n",
    "    import mahotas as mh\n",
    "    _HAS_MAHOTAS = True\n",
    "except ImportError:\n",
    "    warnings.warn(\"⚠️  mahotas not found – pseudo-Zernike moments will be 0.\")\n",
    "    _HAS_MAHOTAS = False\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  GLOBAL CONSTANTS\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "DATASETS_ROOT   = Path(\"datasets\")\n",
    "CSV_PATH        = \"INSHEP_features.csv\"\n",
    "\n",
    "ACTIVITY_MAP = {\n",
    "    \"1\": \"walking\",\n",
    "    \"2\": \"sitting_down\",\n",
    "    \"3\": \"standing_up\",\n",
    "    \"4\": \"pick_object\",\n",
    "    \"5\": \"drink_water\",\n",
    "    \"6\": \"fall\",\n",
    "}\n",
    "\n",
    "TIME_WINDOW     = 200\n",
    "OVERLAP_FRAC    = 0.95\n",
    "PAD_FACTOR      = 4\n",
    "BUTTER_N        = 4\n",
    "BUTTER_CUT      = 0.0075         # high-pass cut-off (fraction of Nyquist)\n",
    "TORSO_V_MAX     = 0.25           # ± m/s\n",
    "DENSITY_THR_DB  = -3             # dB down from peak for masks\n",
    "\n",
    "FIELDNAMES = [\n",
    "    \"file_id\", \"activity\", \"path\",\n",
    "    \"mean_entropy\", \"mean_power\", \"variance\", \"stddev\",\n",
    "    \"max_vel\", \"amp_density\", \"kurtosis\", \"zernike_moment\",\n",
    "    \"periodicity\", \"mean_torso_power\", \"pos_neg_ratio\",\n",
    "    \"doppler_offset\", \"main_lobe_width\",\"auto_correlation\"\n",
    "]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  LOW-LEVEL UTILITIES  (top-level → picklable)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def read_dat(path: Path):\n",
    "    \"\"\"Load one *.dat file and return fc [Hz], Tsweep [s], MTI-filtered range-time matrix.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = [ln.strip() for ln in f]\n",
    "\n",
    "    fc, Tsweep_ms, NTS, Bw = map(float, lines[:4])\n",
    "    Tsweep = Tsweep_ms * 1e-3\n",
    "    NTS    = int(NTS)\n",
    "    raw    = np.array([complex(s.replace(\"i\", \"j\")) for s in lines[4:]])\n",
    "\n",
    "    n_chirps = raw.size // NTS\n",
    "    time_mat = raw[: n_chirps * NTS].reshape((NTS, n_chirps), order=\"F\")\n",
    "\n",
    "    rng_fft  = np.fft.fftshift(np.fft.fft(time_mat, axis=0), axes=0)\n",
    "    rng_half = rng_fft[NTS // 2 :, :]\n",
    "\n",
    "    b, a     = signal.butter(BUTTER_N, BUTTER_CUT, \"high\")\n",
    "    mti      = signal.lfilter(b, a, rng_half, axis=1)\n",
    "\n",
    "    return fc, Tsweep, mti[1:, :]     # skip leakage bin\n",
    "\n",
    "\n",
    "def stft_mag(mti, prf):\n",
    "    \"\"\"Accumulate |STFT| for range bins 10-30 → spectrogram magnitude + doppler axis.\"\"\"\n",
    "    nperseg  = TIME_WINDOW\n",
    "    noverlap = int(round(nperseg * OVERLAP_FRAC))\n",
    "    nfft     = PAD_FACTOR * nperseg\n",
    "\n",
    "    S_accum = None\n",
    "    for r in range(9, 30):  # bins 10-30  (0-based 9-29)\n",
    "        _, _, S = signal.spectrogram(\n",
    "            mti[r, :],\n",
    "            fs            = prf,\n",
    "            window        = \"hann\",\n",
    "            nperseg       = nperseg,\n",
    "            noverlap      = noverlap,\n",
    "            nfft          = nfft,\n",
    "            mode          = \"complex\",\n",
    "            return_onesided=False,\n",
    "        )\n",
    "        S = np.fft.fftshift(S, axes=0)\n",
    "        S_accum = np.abs(S) if S_accum is None else S_accum + np.abs(S)\n",
    "\n",
    "    doppler = np.fft.fftshift(np.fft.fftfreq(nfft, d=1 / prf))\n",
    "    return S_accum, doppler\n",
    "\n",
    "\n",
    "def binary_mask(db_img, thresh_db):\n",
    "    return db_img >= (db_img.max() + thresh_db)\n",
    "\n",
    "\n",
    "def pseudo_zernike(img, radius=20, degree=4):\n",
    "    if not _HAS_MAHOTAS:\n",
    "        return 0.0\n",
    "    size   = max(img.shape)\n",
    "    padder = [(0, size - img.shape[0]), (0, size - img.shape[1])]\n",
    "    img_n  = (np.pad(img, padder) - img.min()) / (img.ptp() + 1e-12)\n",
    "    return float(np.mean(np.abs(mh.features.zernike_moments(img_n, radius, degree=degree))))\n",
    "\n",
    "\n",
    "def extract_features(mti, fc, Tsweep):\n",
    "    prf                = 1.0 / Tsweep\n",
    "    S, doppler         = stft_mag(mti, prf)\n",
    "    S2                 = S**2\n",
    "    flat               = S2.ravel()\n",
    "\n",
    "    p                  = flat / (flat.sum() + 1e-12)\n",
    "    mean_entropy       = float(-(p * np.log(p + 1e-12)).sum())\n",
    "    mean_power         = float(flat.mean())\n",
    "    variance           = float(flat.var())\n",
    "    stddev             = float(math.sqrt(variance))\n",
    "\n",
    "    v_axis             = doppler * 3e8 / (2 * fc)\n",
    "    vmax               = float(v_axis[np.unravel_index(S.argmax(), S.shape)[0]])\n",
    "    amp_density        = binary_mask(20 * np.log10(S + 1e-12), DENSITY_THR_DB).mean()\n",
    "    kurtosis_val       = float(stats.kurtosis(flat, fisher=False))\n",
    "    z_moment           = pseudo_zernike(S)\n",
    "\n",
    "    pw_sweep           = S2.sum(axis=0)\n",
    "    acf                = signal.correlate(pw_sweep, pw_sweep, mode=\"full\")[len(pw_sweep)-1 :]\n",
    "    periodicity        = float(acf[1:].max() / (acf[0] + 1e-12))\n",
    "\n",
    "    torso_mask         = np.abs(v_axis) <= TORSO_V_MAX\n",
    "    mean_torso_power   = float(S2[torso_mask, :].mean())\n",
    "\n",
    "    pos_power          = S2[v_axis > 0, :].sum()\n",
    "    neg_power          = S2[v_axis < 0, :].sum()\n",
    "    pos_neg_ratio      = float(pos_power / (neg_power + 1e-12))\n",
    "\n",
    "    weights            = S2.sum(axis=1)\n",
    "    doppler_offset     = float((v_axis * weights).sum() / (weights.sum() + 1e-12))\n",
    "\n",
    "    row_db             = 20 * np.log10(S2.mean(axis=1) + 1e-12)\n",
    "    mask               = binary_mask(row_db, DENSITY_THR_DB)\n",
    "    if mask.any():\n",
    "        idx            = np.where(mask)[0]\n",
    "        main_lobe_width = float(v_axis[idx.max()] - v_axis[idx.min()])\n",
    "    else:\n",
    "        main_lobe_width = 0.0\n",
    "        \n",
    "\n",
    "    return dict(\n",
    "        mean_entropy       = mean_entropy,\n",
    "        mean_power         = mean_power,\n",
    "        variance           = variance,\n",
    "        stddev             = stddev,\n",
    "        max_vel            = vmax,\n",
    "        amp_density        = amp_density,\n",
    "        kurtosis           = kurtosis_val,\n",
    "        zernike_moment     = z_moment,\n",
    "        periodicity        = periodicity,\n",
    "        mean_torso_power   = mean_torso_power,\n",
    "        pos_neg_ratio      = pos_neg_ratio,\n",
    "        doppler_offset     = doppler_offset,\n",
    "        main_lobe_width    = main_lobe_width,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "def process_one(path: Path):\n",
    "    \"\"\"Worker wrapper: returns dict ready for CSV OR raises.\"\"\"\n",
    "    fc, Tsweep, mti = read_dat(path)\n",
    "    feats           = extract_features(mti, fc, Tsweep)\n",
    "    fid             = path.stem\n",
    "    feats.update(\n",
    "        file_id  = fid,\n",
    "        activity = ACTIVITY_MAP.get(fid[0], \"unknown\"),\n",
    "        path     = str(path),\n",
    "    )\n",
    "    return feats\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  MAIN — run workers & stream to CSV\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    all_files = sorted(DATASETS_ROOT.rglob(\"*.dat\"))\n",
    "    n_files   = len(all_files)\n",
    "    if not n_files:\n",
    "        print(f\"No .dat files found under {DATASETS_ROOT.resolve()}\")\n",
    "        return\n",
    "\n",
    "    # prepare CSV (append if exists, else create with header)\n",
    "    csv_exists = os.path.exists(CSV_PATH)\n",
    "    csv_file   = open(CSV_PATH, \"a\", newline=\"\")\n",
    "    writer     = csv.DictWriter(csv_file, FIELDNAMES)\n",
    "    if not csv_exists:\n",
    "        writer.writeheader()\n",
    "\n",
    "    max_workers = min(12, cpu_count())    # 12 logical procs on your PC\n",
    "    print(f\"• Processing {n_files} files with {max_workers} workers …\")\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as pool:\n",
    "        future_to_path = {pool.submit(process_one, p): p for p in all_files}\n",
    "\n",
    "        for i, fut in enumerate(as_completed(future_to_path), 1):\n",
    "            p = future_to_path[fut]\n",
    "            try:\n",
    "                row = fut.result()\n",
    "                writer.writerow(row)\n",
    "                csv_file.flush()          # ensure on-disk immediately\n",
    "                print(f\"✓ [{i:>4}/{n_files}] {row['file_id']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ [{i:>4}/{n_files}] {p.name}: {e}\")\n",
    "\n",
    "    csv_file.close()\n",
    "    print(f\"\\n✅  Done.  All features appended to  {CSV_PATH}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
