{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b85417f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file 1/1754: 1P36A01R01.dat\n",
      "Reading DAT file...\n",
      "Extracting features...\n",
      "Adding metadata...\n",
      "Writing to CSV...\n",
      "✓ [   1/1754] 1P36A01R01\n",
      "\n",
      "Processing file 2/1754: 1P36A01R02.dat\n",
      "Reading DAT file...\n",
      "Extracting features...\n",
      "Adding metadata...\n",
      "Writing to CSV...\n",
      "✓ [   2/1754] 1P36A01R02\n",
      "\n",
      "Processing file 3/1754: 1P36A01R03.dat\n",
      "Reading DAT file...\n",
      "Extracting features...\n",
      "Adding metadata...\n",
      "Writing to CSV...\n",
      "✓ [   3/1754] 1P36A01R03\n",
      "\n",
      "Processing file 4/1754: 1P37A01R01.dat\n",
      "Reading DAT file...\n",
      "Extracting features...\n",
      "Adding metadata...\n",
      "Writing to CSV...\n",
      "✓ [   4/1754] 1P37A01R01\n",
      "\n",
      "Processing file 5/1754: 1P37A01R02.dat\n",
      "Reading DAT file...\n",
      "Extracting features...\n",
      "Adding metadata...\n",
      "Writing to CSV...\n",
      "✓ [   5/1754] 1P37A01R02\n",
      "\n",
      "Processing file 6/1754: 1P37A01R03.dat\n",
      "Reading DAT file...\n",
      "Extracting features...\n",
      "Adding metadata...\n",
      "Writing to CSV...\n",
      "✓ [   6/1754] 1P37A01R03\n",
      "\n",
      "Processing file 7/1754: 1P38A01R01.dat\n",
      "Reading DAT file...\n",
      "Extracting features...\n",
      "Adding metadata...\n",
      "Writing to CSV...\n",
      "✓ [   7/1754] 1P38A01R01\n",
      "\n",
      "Processing file 8/1754: 1P38A01R02.dat\n",
      "Reading DAT file...\n",
      "Extracting features...\n",
      "\n",
      "⚠️  Processing interrupted by user\n",
      "\n",
      "✅  Features saved to INSHEP_features.csv\n"
     ]
    }
   ],
   "source": [
    "import os, math, warnings, csv\n",
    "from pathlib import Path\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal, stats\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage.util import img_as_ubyte\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  optional pseudo-Zernike moments  (needs  pip install mahotas)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "try:\n",
    "    import mahotas as mh\n",
    "    _HAS_MAHOTAS = True\n",
    "except ImportError:\n",
    "    warnings.warn(\"⚠️  mahotas not found – pseudo-Zernike moments will be 0.\")\n",
    "    _HAS_MAHOTAS = False\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  GLOBAL CONSTANTS\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "DATASETS_ROOT   = DATASETS_ROOT = Path(\"..\") / \"datasets\"\n",
    "CSV_PATH        = \"INSHEP_features.csv\"\n",
    "\n",
    "ACTIVITY_MAP = {\n",
    "    \"1\": \"walking\",\n",
    "    \"2\": \"sitting_down\",\n",
    "    \"3\": \"standing_up\",\n",
    "    \"4\": \"pick_object\",\n",
    "    \"5\": \"drink_water\",\n",
    "    \"6\": \"fall\",\n",
    "}\n",
    "\n",
    "TIME_WINDOW     = 200\n",
    "OVERLAP_FRAC    = 0.95\n",
    "PAD_FACTOR      = 4\n",
    "BUTTER_N        = 4\n",
    "BUTTER_CUT      = 0.0075         # high-pass cut-off (fraction of Nyquist)\n",
    "TORSO_V_MAX     = 0.25           # ± m/s\n",
    "DENSITY_THR_DB  = -3             # dB down from peak for masks\n",
    "\n",
    "FIELDNAMES = [\n",
    "    \"file_id\", \"activity\", \"path\",\n",
    "    \"mean_entropy\", \"mean_power\", \"variance\", \"stddev\",\n",
    "    \"max_vel\", \"amp_density\", \"kurtosis\", \"zernike_moment\",\n",
    "    \"periodicity\", \"mean_torso_power\", \"pos_neg_ratio\",\n",
    "    \"doppler_offset\", \"main_lobe_width\",\"auto_correlation\",\n",
    "    \"envelope_width\", \"limb_asymmetry\", \"limb_power\",\n",
    "    \"limb_smoothness\", \"clean_kurtosis\",\n",
    "    \"motion_duration\", \"doppler_peak_velocity\", \"doppler_symmetry_index\",\n",
    "    \"cepstral_entropy\", \"range_bin_span\", \"doppler_bandwidth\",\n",
    "    \"skew_val\",\n",
    "    \"contrast\", \"dissimilarity\", \"homogeneity\", \"energy\",\n",
    "    \"correlation\", \"ASM\"\n",
    "    ]\n",
    "\n",
    "def iq_correction(raw_data):\n",
    "    \"\"\"\n",
    "    Perform I/Q correction on complex radar data\n",
    "    \n",
    "    Args:\n",
    "        raw_data (np.ndarray): Complex IQ data\n",
    "        window_size (int): Window size for moving average\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Corrected complex IQ data\n",
    "    \"\"\"\n",
    "    # Split into I and Q components\n",
    "    i_data = np.real(raw_data)\n",
    "    q_data = np.imag(raw_data)\n",
    "    \n",
    "    # DC offset removal\n",
    "    i_dc = np.mean(i_data)\n",
    "    q_dc = np.mean(q_data)\n",
    "    i_data = i_data - i_dc\n",
    "    q_data = q_data - q_dc\n",
    "    \n",
    "    # Amplitude correction\n",
    "    i_amp = np.sqrt(np.mean(i_data**2))\n",
    "    q_amp = np.sqrt(np.mean(q_data**2))\n",
    "    amp_correction = np.sqrt(i_amp * q_amp)\n",
    "    i_data = i_data * (amp_correction / i_amp)\n",
    "    q_data = q_data * (amp_correction / q_amp)\n",
    "    \n",
    "    # Phase imbalance correction\n",
    "    iq_corr = np.mean(i_data * q_data)\n",
    "    phase_error = np.arcsin(iq_corr / (i_amp * q_amp))\n",
    "    q_data_corr = q_data * np.cos(phase_error) - i_data * np.sin(phase_error)\n",
    "    \n",
    "    return i_data + 1j * q_data_corr\n",
    "\n",
    "def read_dat(path: Path):\n",
    "    \"\"\"Load one *.dat file and return fc [Hz], Tsweep [s], MTI-filtered range-time matrix.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = [ln.strip() for ln in f]\n",
    "\n",
    "    fc, Tsweep_ms, NTS, Bw = map(float, lines[:4])\n",
    "    Tsweep = Tsweep_ms * 1e-3\n",
    "    NTS    = int(NTS)\n",
    "    raw    = np.array([complex(s.replace(\"i\", \"j\")) for s in lines[4:]])\n",
    "\n",
    "    # Apply I/Q correction to raw data\n",
    "    raw_corrected = iq_correction(raw)\n",
    "\n",
    "    n_chirps = raw_corrected.size // NTS\n",
    "    time_mat = raw_corrected[: n_chirps * NTS].reshape((NTS, n_chirps), order=\"F\")\n",
    "\n",
    "    rng_fft  = np.fft.fftshift(np.fft.fft(time_mat, axis=0), axes=0)\n",
    "    rng_half = rng_fft[NTS // 2 :, :]\n",
    "\n",
    "    b, a     = signal.butter(BUTTER_N, BUTTER_CUT, \"high\")\n",
    "    mti      = signal.lfilter(b, a, rng_half, axis=1)\n",
    "\n",
    "    return fc, Tsweep, mti[1:, :]     # skip leakage bin\n",
    "\n",
    "\n",
    "def kalman_filter_1d(observed, dt, process_noise, measurement_noise):\n",
    "    A = np.array([[1, dt], [0, 1]])\n",
    "    B = np.array([[0.5 * dt**2], [dt]])\n",
    "    C = np.array([[1, 0]])\n",
    "    Q = process_noise * np.array([[dt**4/4, dt**3/2], [dt**3/2, dt**2]])\n",
    "    R = measurement_noise ** 2\n",
    "\n",
    "    x = np.array([[observed[0]], [0]])  # initial state: position + velocity\n",
    "    P = np.eye(2)\n",
    "    filtered = []\n",
    "\n",
    "    for z in observed:\n",
    "        # Predict\n",
    "        x = A @ x\n",
    "        P = A @ P @ A.T + Q\n",
    "\n",
    "        # Update\n",
    "        K = P @ C.T @ np.linalg.inv(C @ P @ C.T + R)\n",
    "        x = x + K @ (z - C @ x)\n",
    "        P = (np.eye(2) - K @ C) @ P\n",
    "\n",
    "        filtered.append(x[0, 0])\n",
    "    \n",
    "    return np.array(filtered)\n",
    "def remove_torso_from_spectrogram(Sxx, velocity_axis, torso_velocity_trace, bandwidth=0.4):\n",
    "    \"\"\"\n",
    "    Removes the torso signature from the spectrogram and centers all motion around zero velocity\n",
    "    for better limb motion analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Sxx : 2D array\n",
    "        Input spectrogram\n",
    "    velocity_axis : array\n",
    "        Velocity axis values\n",
    "    torso_velocity_trace : array\n",
    "        Estimated torso velocity over time\n",
    "    bandwidth : float\n",
    "        Width of the suppression band around zero velocity in m/s\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Sxx_centered : 2D array\n",
    "        Spectrogram with torso removed and motion centered around zero velocity\n",
    "    \"\"\"\n",
    "    Sxx_centered = np.zeros_like(Sxx)\n",
    "    zero_idx = np.argmin(np.abs(velocity_axis - 0))\n",
    "\n",
    "    for i, torso_vel in enumerate(torso_velocity_trace):\n",
    "        # Shift to center torso at 0\n",
    "        shift_idx = np.argmin(np.abs(velocity_axis - torso_vel))\n",
    "        shift_amount = zero_idx - shift_idx\n",
    "        shifted_col = np.roll(Sxx[:, i], shift=shift_amount)\n",
    "\n",
    "        # Apply suppression mask around 0 velocity\n",
    "        suppress_mask = np.abs(velocity_axis) <= bandwidth\n",
    "        shifted_col[suppress_mask] = 0\n",
    "\n",
    "        # Store the centered version without rolling back\n",
    "        Sxx_centered[:, i] = shifted_col\n",
    "\n",
    "    return Sxx_centered\n",
    "\n",
    "def detect_envelope(Sxx_dB, velocity_axis, threshold_dB):\n",
    "    n_bins, n_frames = Sxx_dB.shape\n",
    "    upper_envelope = np.full(n_frames, np.nan)\n",
    "    lower_envelope = np.full(n_frames, np.nan)\n",
    "\n",
    "    for i in range(n_frames):\n",
    "        above_thresh = np.where(Sxx_dB[:, i] > threshold_dB)[0]\n",
    "        if len(above_thresh) > 0:\n",
    "            lower_envelope[i] = velocity_axis[above_thresh[0]]\n",
    "            upper_envelope[i] = velocity_axis[above_thresh[-1]]\n",
    "\n",
    "    return lower_envelope, upper_envelope\n",
    "\n",
    "\n",
    "def stft_mag(mti, prf):\n",
    "    \"\"\"Accumulate |STFT| for range bins 10-30 → spectrogram magnitude + doppler axis.\"\"\"\n",
    "    nperseg  = TIME_WINDOW\n",
    "    noverlap = int(round(nperseg * OVERLAP_FRAC))\n",
    "    nfft     = PAD_FACTOR * nperseg\n",
    "\n",
    "    S_accum = None\n",
    "    for r in range(9, 30):  # bins 10-30  (0-based 9-29)\n",
    "        _, _, S = signal.spectrogram(\n",
    "            mti[r, :],\n",
    "            fs            = prf,\n",
    "            window        = \"hann\",\n",
    "            nperseg       = nperseg,\n",
    "            noverlap      = noverlap,\n",
    "            nfft          = nfft,\n",
    "            mode          = \"complex\",\n",
    "            return_onesided=False,\n",
    "        )\n",
    "        S = np.fft.fftshift(S, axes=0)\n",
    "        S_accum = np.abs(S) if S_accum is None else S_accum + np.abs(S)\n",
    "\n",
    "    doppler = np.fft.fftshift(np.fft.fftfreq(nfft, d=1 / prf))\n",
    "    return S_accum, doppler\n",
    "\n",
    "\n",
    "def binary_mask(db_img, thresh_db):\n",
    "    return db_img >= (db_img.max() + thresh_db)\n",
    "\n",
    "\n",
    "def pseudo_zernike(img, radius=20, degree=4):\n",
    "    if not _HAS_MAHOTAS:\n",
    "        return 0.0\n",
    "    size   = max(img.shape)\n",
    "    padder = [(0, size - img.shape[0]), (0, size - img.shape[1])]\n",
    "    img_n  = (np.pad(img, padder) - img.min()) / (img.ptp() + 1e-12)\n",
    "    return float(np.mean(np.abs(mh.features.zernike_moments(img_n, radius, degree=degree))))\n",
    "\n",
    "# Then update the function to use the new names:\n",
    "def extract_glcm_features(spectrogram_image):\n",
    "    spectrogram_image = np.abs(spectrogram_image)\n",
    "    spectrogram_image = (spectrogram_image - spectrogram_image.min()) / (spectrogram_image.max() - spectrogram_image.min())\n",
    "    spectrogram_image = img_as_ubyte(spectrogram_image)\n",
    "\n",
    "    glcm = graycomatrix(\n",
    "        spectrogram_image,\n",
    "        distances=[1, 3],\n",
    "        angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
    "        levels=256,\n",
    "        symmetric=True,\n",
    "        normed=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    features = {\n",
    "        'contrast': graycoprops(glcm, 'contrast').mean(),\n",
    "        'dissimilarity': graycoprops(glcm, 'dissimilarity').mean(),\n",
    "        'homogeneity': graycoprops(glcm, 'homogeneity').mean(),\n",
    "        'energy': graycoprops(glcm, 'energy').mean(),\n",
    "        'correlation': graycoprops(glcm, 'correlation').mean(),\n",
    "        'ASM': graycoprops(glcm, 'ASM').mean()\n",
    "    }\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_features(mti, fc, Tsweep):\n",
    "    prf                = 1.0 / Tsweep\n",
    "    S, doppler         = stft_mag(mti, prf)\n",
    "    S2                 = S**2\n",
    "    flat               = S2.ravel()\n",
    "\n",
    "    p                  = flat / (flat.sum() + 1e-12)\n",
    "    mean_entropy       = float(-(p * np.log(p + 1e-12)).sum())\n",
    "    mean_power         = float(flat.mean())\n",
    "    variance           = float(flat.var())\n",
    "    stddev             = float(math.sqrt(variance))\n",
    "\n",
    "    v_axis             = doppler * 3e8 / (2 * fc)\n",
    "    vmax               = float(v_axis[np.unravel_index(S.argmax(), S.shape)[0]])\n",
    "    amp_density        = binary_mask(20 * np.log10(S + 1e-12), DENSITY_THR_DB).mean()\n",
    "    kurtosis_val       = float(stats.kurtosis(flat, fisher=False))\n",
    "    z_moment           = pseudo_zernike(S)\n",
    "\n",
    "    pw_sweep           = S2.sum(axis=0)\n",
    "    acf                = signal.correlate(pw_sweep, pw_sweep, mode=\"full\")[len(pw_sweep)-1 :]\n",
    "    periodicity        = float(acf[1:].max() / (acf[0] + 1e-12))\n",
    "\n",
    "    torso_mask         = np.abs(v_axis) <= TORSO_V_MAX\n",
    "    mean_torso_power   = float(S2[torso_mask, :].mean())\n",
    "\n",
    "    pos_power          = S2[v_axis > 0, :].sum()\n",
    "    neg_power          = S2[v_axis < 0, :].sum()\n",
    "    pos_neg_ratio      = float(pos_power / (neg_power + 1e-12))\n",
    "\n",
    "    weights            = S2.sum(axis=1)\n",
    "    doppler_offset     = float((v_axis * weights).sum() / (weights.sum() + 1e-12))\n",
    "\n",
    "    row_db             = 20 * np.log10(S2.mean(axis=1) + 1e-12)\n",
    "    mask               = binary_mask(row_db, DENSITY_THR_DB)\n",
    "    if mask.any():\n",
    "        idx            = np.where(mask)[0]\n",
    "        main_lobe_width = float(v_axis[idx.max()] - v_axis[idx.min()])\n",
    "    else:\n",
    "        main_lobe_width = 0.0\n",
    "\n",
    "    # auto-correlation\n",
    "    pw_sweep           = S2.sum(axis=0)\n",
    "    acf                = signal.correlate(pw_sweep, pw_sweep, mode=\"full\")[len(pw_sweep)-1 :]\n",
    "    periodicity        = float(acf[1:].max() / (acf[0] + 1e-12))\n",
    "    \n",
    "    auto_correlation   = float(acf[1] / (acf[0] + 1e-12))  # First lag autocorrelation\n",
    "\n",
    "    # Square the spectrogram for further processing\n",
    "    S2 = S**2\n",
    "\n",
    "    # Estimate torso velocity trace\n",
    "    raw_idx = np.argmax(S, axis=0)\n",
    "    raw_torso_v = v_axis[raw_idx]\n",
    "\n",
    "    kalman_torso_v = kalman_filter_1d(raw_torso_v, dt=(1 / prf), process_noise=10000.0, measurement_noise=0.1)\n",
    "\n",
    "    # Remove torso\n",
    "    S2_torso_removed = remove_torso_from_spectrogram(S2, v_axis, kalman_torso_v)\n",
    "\n",
    "    # Convert to dB\n",
    "    S2_dB_torso_removed = 20 * np.log10(S2_torso_removed + 1e-12)\n",
    "\n",
    "    # Envelope detection\n",
    "    threshold = S2_dB_torso_removed.min() + 20\n",
    "    lower_env, upper_env = detect_envelope(S2_dB_torso_removed, v_axis, threshold)\n",
    "\n",
    "    # Envelope-based features\n",
    "    env_width      = np.nanmean(upper_env - lower_env)\n",
    "    env_asymmetry  = float((upper_env - lower_env).mean())\n",
    "    smoothness     = float(np.std(np.gradient(upper_env - lower_env)))\n",
    "    limb_power     = float(S2_torso_removed[(v_axis[:, None] >= lower_env) & (v_axis[:, None] <= upper_env)].sum())\n",
    "    kurtosis_clean = float(stats.kurtosis(S2_torso_removed.ravel(), fisher=False))\n",
    "\n",
    "    \n",
    "    # Motion Duration: Duration of signal above a threshold in the time domain.\n",
    "    nperseg = TIME_WINDOW\n",
    "    noverlap = int(round(nperseg * OVERLAP_FRAC))\n",
    "    time_step = (nperseg - noverlap) / prf\n",
    "    motion_thresh = pw_sweep.max() * (10**(DENSITY_THR_DB / 10))\n",
    "    motion_duration = float((pw_sweep >= motion_thresh).sum() * time_step)\n",
    "\n",
    "    # Average Doppler Spectrum features\n",
    "    avg_doppler_spectrum = S2.mean(axis=1)\n",
    "\n",
    "    # Doppler Peak Velocity: Velocity at the peak of the time-averaged Doppler spectrum.\n",
    "    doppler_peak_velocity = float(v_axis[avg_doppler_spectrum.argmax()])\n",
    "\n",
    "    # Doppler Symmetry Index: Normalized difference between positive and negative Doppler power.\n",
    "    pos_mask = v_axis > 0\n",
    "    neg_mask = v_axis < 0\n",
    "    pos_power_avg = avg_doppler_spectrum[pos_mask].sum()\n",
    "    neg_power_avg = avg_doppler_spectrum[neg_mask].sum()\n",
    "    doppler_symmetry_index = float((pos_power_avg - neg_power_avg) / (pos_power_avg + neg_power_avg + 1e-12))\n",
    "\n",
    "    # Cepstral Entropy: Entropy of the power cepstrum of the average Doppler spectrum.\n",
    "    log_spec = np.log(avg_doppler_spectrum + 1e-12)\n",
    "    cepstrum = np.abs(np.fft.irfft(log_spec))**2\n",
    "    p_cep = cepstrum / (cepstrum.sum() + 1e-12)\n",
    "    cepstral_entropy = float(-(p_cep * np.log(p_cep + 1e-12)).sum())\n",
    "\n",
    "    # Range Bin Span: Spread of the signal across range bins.\n",
    "    range_power = np.sum(np.abs(mti)**2, axis=1)\n",
    "    range_thresh = range_power.max() * (10**(DENSITY_THR_DB / 10))\n",
    "    active_bins_mask = range_power >= range_thresh\n",
    "    if active_bins_mask.any():\n",
    "        active_indices = np.where(active_bins_mask)[0]\n",
    "        range_bin_span = float(active_indices.max() - active_indices.min())\n",
    "    else:\n",
    "        range_bin_span = 0.0\n",
    "\n",
    "    # Doppler Bandwidth: Power-weighted standard deviation of the Doppler velocity.\n",
    "    doppler_variance = (weights * (v_axis - doppler_offset)**2).sum() / (weights.sum() + 1e-12)\n",
    "    doppler_bandwidth = float(np.sqrt(doppler_variance))\n",
    "\n",
    "    # Skewness of the Doppler spectrum\n",
    "    skew_val = float(stats.skew(S2.mean(axis=1)))       \n",
    "\n",
    "    return dict(\n",
    "        mean_entropy       = mean_entropy,\n",
    "        mean_power         = mean_power,\n",
    "        variance           = variance,\n",
    "        stddev             = stddev,\n",
    "        max_vel            = vmax,\n",
    "        amp_density        = amp_density,\n",
    "        kurtosis           = kurtosis_val,\n",
    "        zernike_moment     = z_moment,\n",
    "        periodicity        = periodicity,\n",
    "        mean_torso_power   = mean_torso_power,\n",
    "        pos_neg_ratio      = pos_neg_ratio,\n",
    "        doppler_offset     = doppler_offset,\n",
    "        main_lobe_width    = main_lobe_width,\n",
    "        auto_correlation   = auto_correlation,\n",
    "        envelope_width     = env_width,\n",
    "        limb_asymmetry     = env_asymmetry,\n",
    "        limb_power         = limb_power,\n",
    "        limb_smoothness    = smoothness,\n",
    "        clean_kurtosis     = kurtosis_clean,\n",
    "        motion_duration        = motion_duration,\n",
    "        doppler_peak_velocity  = doppler_peak_velocity,\n",
    "        doppler_symmetry_index = doppler_symmetry_index,\n",
    "        cepstral_entropy       = cepstral_entropy,\n",
    "        range_bin_span         = range_bin_span,\n",
    "        doppler_bandwidth      = doppler_bandwidth,\n",
    "        skew_val            = skew_val,\n",
    "        contrast           = extract_glcm_features(S2)[ 'contrast' ],\n",
    "        dissimilarity      = extract_glcm_features(S2)[ 'dissimilarity' ],\n",
    "        homogeneity        = extract_glcm_features(S2)[ 'homogeneity' ],\n",
    "        energy             = extract_glcm_features(S2)[ 'energy' ],\n",
    "        correlation        = extract_glcm_features(S2)[ 'correlation' ],\n",
    "        ASM                = extract_glcm_features(S2)[ 'ASM' ],\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "def process_one(path: Path):\n",
    "    \"\"\"Worker wrapper: returns dict ready for CSV OR raises.\"\"\"\n",
    "    fc, Tsweep, mti = read_dat(path)\n",
    "    feats           = extract_features(mti, fc, Tsweep)\n",
    "    fid             = path.stem\n",
    "    feats.update(\n",
    "        file_id  = fid,\n",
    "        activity = ACTIVITY_MAP.get(fid[0], \"unknown\"),\n",
    "        path     = str(path),\n",
    "    )\n",
    "    return feats\n",
    "\n",
    "def process_batch(files, writer, batch_start, total_files):\n",
    "    \"\"\"Process a batch of files with detailed error handling\"\"\"\n",
    "    max_workers = min(4, cpu_count())  # Reduced from 12 to 4\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as pool:\n",
    "        future_to_path = {pool.submit(process_one, p): p for p in files}\n",
    "        \n",
    "        for i, fut in enumerate(as_completed(future_to_path), 1):\n",
    "            p = future_to_path[fut]\n",
    "            try:\n",
    "                row = fut.result()\n",
    "                writer.writerow(row)\n",
    "                global_i = batch_start + i\n",
    "                print(f\"✓ [{global_i:>4}/{total_files}] {row['file_id']}\")\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                print(f\"✗ [{batch_start+i:>4}/{total_files}] {p.name}\")\n",
    "                print(f\"Error: {e.__class__.__name__}: {e}\")\n",
    "                print(\"Traceback:\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "def main():\n",
    "    all_files = sorted(DATASETS_ROOT.rglob(\"*.dat\"))\n",
    "    n_files = len(all_files)\n",
    "    if not n_files:\n",
    "        print(f\"No .dat files found under {DATASETS_ROOT.resolve()}\")\n",
    "        return\n",
    "\n",
    "    # prepare CSV (append if exists, else create with header)\n",
    "    csv_exists = os.path.exists(CSV_PATH)\n",
    "    csv_file = open(CSV_PATH, \"w\", newline=\"\")\n",
    "    writer = csv.DictWriter(csv_file, FIELDNAMES)\n",
    "    if not csv_exists:\n",
    "        writer.writeheader()\n",
    "\n",
    "    try:\n",
    "        # Process one file at a time\n",
    "        for i, file_path in enumerate(all_files, 1):\n",
    "            try:\n",
    "                print(f\"\\nProcessing file {i}/{n_files}: {file_path.name}\")\n",
    "                # Add detailed logging\n",
    "                print(\"Reading DAT file...\")\n",
    "                fc, Tsweep, mti = read_dat(file_path)\n",
    "                \n",
    "                print(\"Extracting features...\")\n",
    "                feats = extract_features(mti, fc, Tsweep)\n",
    "                \n",
    "                print(\"Adding metadata...\")\n",
    "                fid = file_path.stem\n",
    "                feats.update(\n",
    "                    file_id=fid,\n",
    "                    activity=ACTIVITY_MAP.get(fid[0], \"unknown\"),\n",
    "                    path=str(file_path)\n",
    "                )\n",
    "                \n",
    "                print(\"Writing to CSV...\")\n",
    "                writer.writerow(feats)\n",
    "                csv_file.flush()\n",
    "                print(f\"✓ [{i:>4}/{n_files}] {fid}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ [{i:>4}/{n_files}] {file_path.name}\")\n",
    "                print(f\"Error: {e.__class__.__name__}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️  Processing interrupted by user\")\n",
    "    finally:\n",
    "        csv_file.close()\n",
    "        print(f\"\\n✅  Features saved to {CSV_PATH}\") \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ca7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
