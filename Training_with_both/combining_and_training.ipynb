{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1413a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "INSHEP = pd.read_csv(r'C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassificationWithRadar\\Training_with_our_features\\INSHEP_features.csv')\n",
    "SPECTROGRAM = pd.read_csv(r'C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassificationWithRadar\\Training_with_transfer_learning\\spectrogram_features_google_vite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "586db8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>path</th>\n",
       "      <th>mean_entropy</th>\n",
       "      <th>mean_power</th>\n",
       "      <th>variance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>max_vel</th>\n",
       "      <th>amp_density</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_torso_power</th>\n",
       "      <th>pos_neg_ratio</th>\n",
       "      <th>doppler_offset</th>\n",
       "      <th>main_lobe_width</th>\n",
       "      <th>motion_duration</th>\n",
       "      <th>doppler_peak_velocity</th>\n",
       "      <th>doppler_symmetry_index</th>\n",
       "      <th>cepstral_entropy</th>\n",
       "      <th>range_bin_span</th>\n",
       "      <th>doppler_bandwidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1P36A01R02</td>\n",
       "      <td>walking</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>9.025276</td>\n",
       "      <td>1.959870e+07</td>\n",
       "      <td>6.906348e+16</td>\n",
       "      <td>2.627993e+08</td>\n",
       "      <td>1.099138</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>721.726235</td>\n",
       "      <td>...</td>\n",
       "      <td>7.179115e+07</td>\n",
       "      <td>1.530323</td>\n",
       "      <td>0.279423</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.002155</td>\n",
       "      <td>0.209587</td>\n",
       "      <td>0.940833</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.836027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1P38A01R03</td>\n",
       "      <td>walking</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>9.258551</td>\n",
       "      <td>2.104924e+07</td>\n",
       "      <td>5.631026e+16</td>\n",
       "      <td>2.372978e+08</td>\n",
       "      <td>0.840517</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>433.210982</td>\n",
       "      <td>...</td>\n",
       "      <td>6.311644e+07</td>\n",
       "      <td>1.251952</td>\n",
       "      <td>0.112505</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.111882</td>\n",
       "      <td>0.999645</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.915421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1P38A01R01</td>\n",
       "      <td>walking</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>9.140249</td>\n",
       "      <td>2.620588e+07</td>\n",
       "      <td>9.122603e+16</td>\n",
       "      <td>3.020365e+08</td>\n",
       "      <td>0.808190</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>421.248890</td>\n",
       "      <td>...</td>\n",
       "      <td>1.433929e+08</td>\n",
       "      <td>1.574890</td>\n",
       "      <td>0.182513</td>\n",
       "      <td>0.193966</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.840517</td>\n",
       "      <td>0.223268</td>\n",
       "      <td>1.032866</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.705764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1P37A01R02</td>\n",
       "      <td>walking</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>9.187234</td>\n",
       "      <td>2.411605e+07</td>\n",
       "      <td>7.596749e+16</td>\n",
       "      <td>2.756220e+08</td>\n",
       "      <td>0.840517</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>389.177448</td>\n",
       "      <td>...</td>\n",
       "      <td>1.239555e+08</td>\n",
       "      <td>0.917316</td>\n",
       "      <td>-0.039091</td>\n",
       "      <td>1.907328</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.840517</td>\n",
       "      <td>-0.043125</td>\n",
       "      <td>0.982990</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.738319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1P36A01R03</td>\n",
       "      <td>walking</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>8.914113</td>\n",
       "      <td>1.945387e+07</td>\n",
       "      <td>7.617443e+16</td>\n",
       "      <td>2.759972e+08</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>700.990139</td>\n",
       "      <td>...</td>\n",
       "      <td>6.483077e+07</td>\n",
       "      <td>2.064459</td>\n",
       "      <td>0.364250</td>\n",
       "      <td>0.193966</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>0.347356</td>\n",
       "      <td>0.921222</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.762972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>5P55A05R2</td>\n",
       "      <td>drink_water</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>7.935626</td>\n",
       "      <td>5.718070e+05</td>\n",
       "      <td>7.192478e+13</td>\n",
       "      <td>8.480848e+06</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>640.340498</td>\n",
       "      <td>...</td>\n",
       "      <td>2.337248e+07</td>\n",
       "      <td>0.927633</td>\n",
       "      <td>-0.015329</td>\n",
       "      <td>0.420259</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>-0.037542</td>\n",
       "      <td>0.653246</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.272728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>5P55A05R3</td>\n",
       "      <td>drink_water</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>8.041883</td>\n",
       "      <td>9.940976e+05</td>\n",
       "      <td>1.895267e+14</td>\n",
       "      <td>1.376687e+07</td>\n",
       "      <td>-0.193966</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>528.732149</td>\n",
       "      <td>...</td>\n",
       "      <td>3.984330e+07</td>\n",
       "      <td>1.403761</td>\n",
       "      <td>0.040513</td>\n",
       "      <td>0.387931</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.129310</td>\n",
       "      <td>0.167971</td>\n",
       "      <td>0.877308</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.242563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>5P56A05R1</td>\n",
       "      <td>drink_water</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>7.231323</td>\n",
       "      <td>2.052530e+06</td>\n",
       "      <td>1.951923e+15</td>\n",
       "      <td>4.418057e+07</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>1119.092028</td>\n",
       "      <td>...</td>\n",
       "      <td>8.719889e+07</td>\n",
       "      <td>1.275885</td>\n",
       "      <td>0.032924</td>\n",
       "      <td>0.420259</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.121221</td>\n",
       "      <td>1.052820</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.204817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>5P56A05R2</td>\n",
       "      <td>drink_water</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>7.295752</td>\n",
       "      <td>1.713943e+06</td>\n",
       "      <td>1.355906e+15</td>\n",
       "      <td>3.682262e+07</td>\n",
       "      <td>0.193966</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>1515.745649</td>\n",
       "      <td>...</td>\n",
       "      <td>6.577793e+07</td>\n",
       "      <td>1.193892</td>\n",
       "      <td>0.027241</td>\n",
       "      <td>0.484914</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.193966</td>\n",
       "      <td>0.088378</td>\n",
       "      <td>1.140604</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.228499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>5P56A05R3</td>\n",
       "      <td>drink_water</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>7.325337</td>\n",
       "      <td>1.550412e+06</td>\n",
       "      <td>1.088354e+15</td>\n",
       "      <td>3.299020e+07</td>\n",
       "      <td>0.129310</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>1611.329710</td>\n",
       "      <td>...</td>\n",
       "      <td>7.025300e+07</td>\n",
       "      <td>1.729300</td>\n",
       "      <td>0.041178</td>\n",
       "      <td>0.096983</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.129310</td>\n",
       "      <td>0.267211</td>\n",
       "      <td>1.095919</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.195418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1754 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         file_id     activity  \\\n",
       "0     1P36A01R02      walking   \n",
       "1     1P38A01R03      walking   \n",
       "2     1P38A01R01      walking   \n",
       "3     1P37A01R02      walking   \n",
       "4     1P36A01R03      walking   \n",
       "...          ...          ...   \n",
       "1749   5P55A05R2  drink_water   \n",
       "1750   5P55A05R3  drink_water   \n",
       "1751   5P56A05R1  drink_water   \n",
       "1752   5P56A05R2  drink_water   \n",
       "1753   5P56A05R3  drink_water   \n",
       "\n",
       "                                                   path  mean_entropy  \\\n",
       "0     C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      9.025276   \n",
       "1     C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      9.258551   \n",
       "2     C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      9.140249   \n",
       "3     C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      9.187234   \n",
       "4     C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      8.914113   \n",
       "...                                                 ...           ...   \n",
       "1749  C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      7.935626   \n",
       "1750  C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      8.041883   \n",
       "1751  C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      7.231323   \n",
       "1752  C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      7.295752   \n",
       "1753  C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      7.325337   \n",
       "\n",
       "        mean_power      variance        stddev   max_vel  amp_density  \\\n",
       "0     1.959870e+07  6.906348e+16  2.627993e+08  1.099138     0.000520   \n",
       "1     2.104924e+07  5.631026e+16  2.372978e+08  0.840517     0.000819   \n",
       "2     2.620588e+07  9.122603e+16  3.020365e+08  0.808190     0.000743   \n",
       "3     2.411605e+07  7.596749e+16  2.756220e+08  0.840517     0.000945   \n",
       "4     1.945387e+07  7.617443e+16  2.759972e+08  0.969828     0.000692   \n",
       "...            ...           ...           ...       ...          ...   \n",
       "1749  5.718070e+05  7.192478e+13  8.480848e+06  0.161638     0.000559   \n",
       "1750  9.940976e+05  1.895267e+14  1.376687e+07 -0.193966     0.000572   \n",
       "1751  2.052530e+06  1.951923e+15  4.418057e+07  0.161638     0.000533   \n",
       "1752  1.713943e+06  1.355906e+15  3.682262e+07  0.193966     0.000288   \n",
       "1753  1.550412e+06  1.088354e+15  3.299020e+07  0.129310     0.000265   \n",
       "\n",
       "         kurtosis  ...  mean_torso_power  pos_neg_ratio  doppler_offset  \\\n",
       "0      721.726235  ...      7.179115e+07       1.530323        0.279423   \n",
       "1      433.210982  ...      6.311644e+07       1.251952        0.112505   \n",
       "2      421.248890  ...      1.433929e+08       1.574890        0.182513   \n",
       "3      389.177448  ...      1.239555e+08       0.917316       -0.039091   \n",
       "4      700.990139  ...      6.483077e+07       2.064459        0.364250   \n",
       "...           ...  ...               ...            ...             ...   \n",
       "1749   640.340498  ...      2.337248e+07       0.927633       -0.015329   \n",
       "1750   528.732149  ...      3.984330e+07       1.403761        0.040513   \n",
       "1751  1119.092028  ...      8.719889e+07       1.275885        0.032924   \n",
       "1752  1515.745649  ...      6.577793e+07       1.193892        0.027241   \n",
       "1753  1611.329710  ...      7.025300e+07       1.729300        0.041178   \n",
       "\n",
       "      main_lobe_width  motion_duration  doppler_peak_velocity  \\\n",
       "0            0.258621             1.07               1.002155   \n",
       "1            0.161638             2.30               0.937500   \n",
       "2            0.193966             2.00               0.840517   \n",
       "3            1.907328             2.01               0.840517   \n",
       "4            0.193966             1.37               0.905172   \n",
       "...               ...              ...                    ...   \n",
       "1749         0.420259             0.42               0.161638   \n",
       "1750         0.387931             0.59               0.129310   \n",
       "1751         0.420259             0.46               0.161638   \n",
       "1752         0.484914             0.26               0.193966   \n",
       "1753         0.096983             0.22               0.129310   \n",
       "\n",
       "      doppler_symmetry_index  cepstral_entropy  range_bin_span  \\\n",
       "0                   0.209587          0.940833             6.0   \n",
       "1                   0.111882          0.999645             6.0   \n",
       "2                   0.223268          1.032866             5.0   \n",
       "3                  -0.043125          0.982990             6.0   \n",
       "4                   0.347356          0.921222             6.0   \n",
       "...                      ...               ...             ...   \n",
       "1749               -0.037542          0.653246             3.0   \n",
       "1750                0.167971          0.877308             2.0   \n",
       "1751                0.121221          1.052820             2.0   \n",
       "1752                0.088378          1.140604             2.0   \n",
       "1753                0.267211          1.095919             2.0   \n",
       "\n",
       "      doppler_bandwidth  \n",
       "0              0.836027  \n",
       "1              0.915421  \n",
       "2              0.705764  \n",
       "3              0.738319  \n",
       "4              0.762972  \n",
       "...                 ...  \n",
       "1749           0.272728  \n",
       "1750           0.242563  \n",
       "1751           0.204817  \n",
       "1752           0.228499  \n",
       "1753           0.195418  \n",
       "\n",
       "[1754 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INSHEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f162ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, extract the folder name from the Windows-style path\n",
    "INSHEP['folder'] = INSHEP['path'].str.extract(r'datasets\\\\([^\\\\]+)\\\\')\n",
    "\n",
    "# Then, build the full image path\n",
    "INSHEP['image_path'] = 'spectrograms/' + INSHEP['folder'] + '/' + INSHEP['file_id'] + '_spectrogram.png'\n",
    "\n",
    "# Optional: drop the helper 'folder' column\n",
    "INSHEP.drop(columns=['folder'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d904fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>path</th>\n",
       "      <th>mean_entropy</th>\n",
       "      <th>mean_power</th>\n",
       "      <th>variance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>max_vel</th>\n",
       "      <th>amp_density</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_neg_ratio</th>\n",
       "      <th>doppler_offset</th>\n",
       "      <th>main_lobe_width</th>\n",
       "      <th>motion_duration</th>\n",
       "      <th>doppler_peak_velocity</th>\n",
       "      <th>doppler_symmetry_index</th>\n",
       "      <th>cepstral_entropy</th>\n",
       "      <th>range_bin_span</th>\n",
       "      <th>doppler_bandwidth</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1P36A01R02</td>\n",
       "      <td>walking</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>9.025276</td>\n",
       "      <td>1.959870e+07</td>\n",
       "      <td>6.906348e+16</td>\n",
       "      <td>2.627993e+08</td>\n",
       "      <td>1.099138</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>721.726235</td>\n",
       "      <td>...</td>\n",
       "      <td>1.530323</td>\n",
       "      <td>0.279423</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.002155</td>\n",
       "      <td>0.209587</td>\n",
       "      <td>0.940833</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.836027</td>\n",
       "      <td>spectrograms/1 December 2017 Dataset/1P36A01R0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1P38A01R03</td>\n",
       "      <td>walking</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>9.258551</td>\n",
       "      <td>2.104924e+07</td>\n",
       "      <td>5.631026e+16</td>\n",
       "      <td>2.372978e+08</td>\n",
       "      <td>0.840517</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>433.210982</td>\n",
       "      <td>...</td>\n",
       "      <td>1.251952</td>\n",
       "      <td>0.112505</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.111882</td>\n",
       "      <td>0.999645</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.915421</td>\n",
       "      <td>spectrograms/1 December 2017 Dataset/1P38A01R0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1P38A01R01</td>\n",
       "      <td>walking</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>9.140249</td>\n",
       "      <td>2.620588e+07</td>\n",
       "      <td>9.122603e+16</td>\n",
       "      <td>3.020365e+08</td>\n",
       "      <td>0.808190</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>421.248890</td>\n",
       "      <td>...</td>\n",
       "      <td>1.574890</td>\n",
       "      <td>0.182513</td>\n",
       "      <td>0.193966</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.840517</td>\n",
       "      <td>0.223268</td>\n",
       "      <td>1.032866</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.705764</td>\n",
       "      <td>spectrograms/1 December 2017 Dataset/1P38A01R0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1P37A01R02</td>\n",
       "      <td>walking</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>9.187234</td>\n",
       "      <td>2.411605e+07</td>\n",
       "      <td>7.596749e+16</td>\n",
       "      <td>2.756220e+08</td>\n",
       "      <td>0.840517</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>389.177448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917316</td>\n",
       "      <td>-0.039091</td>\n",
       "      <td>1.907328</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.840517</td>\n",
       "      <td>-0.043125</td>\n",
       "      <td>0.982990</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.738319</td>\n",
       "      <td>spectrograms/1 December 2017 Dataset/1P37A01R0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1P36A01R03</td>\n",
       "      <td>walking</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>8.914113</td>\n",
       "      <td>1.945387e+07</td>\n",
       "      <td>7.617443e+16</td>\n",
       "      <td>2.759972e+08</td>\n",
       "      <td>0.969828</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>700.990139</td>\n",
       "      <td>...</td>\n",
       "      <td>2.064459</td>\n",
       "      <td>0.364250</td>\n",
       "      <td>0.193966</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>0.347356</td>\n",
       "      <td>0.921222</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.762972</td>\n",
       "      <td>spectrograms/1 December 2017 Dataset/1P36A01R0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>5P55A05R2</td>\n",
       "      <td>drink_water</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>7.935626</td>\n",
       "      <td>5.718070e+05</td>\n",
       "      <td>7.192478e+13</td>\n",
       "      <td>8.480848e+06</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>640.340498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927633</td>\n",
       "      <td>-0.015329</td>\n",
       "      <td>0.420259</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>-0.037542</td>\n",
       "      <td>0.653246</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.272728</td>\n",
       "      <td>spectrograms/7 March 2019 West Cumbria Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>5P55A05R3</td>\n",
       "      <td>drink_water</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>8.041883</td>\n",
       "      <td>9.940976e+05</td>\n",
       "      <td>1.895267e+14</td>\n",
       "      <td>1.376687e+07</td>\n",
       "      <td>-0.193966</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>528.732149</td>\n",
       "      <td>...</td>\n",
       "      <td>1.403761</td>\n",
       "      <td>0.040513</td>\n",
       "      <td>0.387931</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.129310</td>\n",
       "      <td>0.167971</td>\n",
       "      <td>0.877308</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.242563</td>\n",
       "      <td>spectrograms/7 March 2019 West Cumbria Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>5P56A05R1</td>\n",
       "      <td>drink_water</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>7.231323</td>\n",
       "      <td>2.052530e+06</td>\n",
       "      <td>1.951923e+15</td>\n",
       "      <td>4.418057e+07</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>1119.092028</td>\n",
       "      <td>...</td>\n",
       "      <td>1.275885</td>\n",
       "      <td>0.032924</td>\n",
       "      <td>0.420259</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.121221</td>\n",
       "      <td>1.052820</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.204817</td>\n",
       "      <td>spectrograms/7 March 2019 West Cumbria Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>5P56A05R2</td>\n",
       "      <td>drink_water</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>7.295752</td>\n",
       "      <td>1.713943e+06</td>\n",
       "      <td>1.355906e+15</td>\n",
       "      <td>3.682262e+07</td>\n",
       "      <td>0.193966</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>1515.745649</td>\n",
       "      <td>...</td>\n",
       "      <td>1.193892</td>\n",
       "      <td>0.027241</td>\n",
       "      <td>0.484914</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.193966</td>\n",
       "      <td>0.088378</td>\n",
       "      <td>1.140604</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.228499</td>\n",
       "      <td>spectrograms/7 March 2019 West Cumbria Dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>5P56A05R3</td>\n",
       "      <td>drink_water</td>\n",
       "      <td>C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...</td>\n",
       "      <td>7.325337</td>\n",
       "      <td>1.550412e+06</td>\n",
       "      <td>1.088354e+15</td>\n",
       "      <td>3.299020e+07</td>\n",
       "      <td>0.129310</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>1611.329710</td>\n",
       "      <td>...</td>\n",
       "      <td>1.729300</td>\n",
       "      <td>0.041178</td>\n",
       "      <td>0.096983</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.129310</td>\n",
       "      <td>0.267211</td>\n",
       "      <td>1.095919</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.195418</td>\n",
       "      <td>spectrograms/Sample_data_preprocessing+label_e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1754 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         file_id     activity  \\\n",
       "0     1P36A01R02      walking   \n",
       "1     1P38A01R03      walking   \n",
       "2     1P38A01R01      walking   \n",
       "3     1P37A01R02      walking   \n",
       "4     1P36A01R03      walking   \n",
       "...          ...          ...   \n",
       "1749   5P55A05R2  drink_water   \n",
       "1750   5P55A05R3  drink_water   \n",
       "1751   5P56A05R1  drink_water   \n",
       "1752   5P56A05R2  drink_water   \n",
       "1753   5P56A05R3  drink_water   \n",
       "\n",
       "                                                   path  mean_entropy  \\\n",
       "0     C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      9.025276   \n",
       "1     C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      9.258551   \n",
       "2     C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      9.140249   \n",
       "3     C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      9.187234   \n",
       "4     C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      8.914113   \n",
       "...                                                 ...           ...   \n",
       "1749  C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      7.935626   \n",
       "1750  C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      8.041883   \n",
       "1751  C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      7.231323   \n",
       "1752  C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      7.295752   \n",
       "1753  C:\\Users\\Adnane\\Desktop\\Radar\\ObjectClassifica...      7.325337   \n",
       "\n",
       "        mean_power      variance        stddev   max_vel  amp_density  \\\n",
       "0     1.959870e+07  6.906348e+16  2.627993e+08  1.099138     0.000520   \n",
       "1     2.104924e+07  5.631026e+16  2.372978e+08  0.840517     0.000819   \n",
       "2     2.620588e+07  9.122603e+16  3.020365e+08  0.808190     0.000743   \n",
       "3     2.411605e+07  7.596749e+16  2.756220e+08  0.840517     0.000945   \n",
       "4     1.945387e+07  7.617443e+16  2.759972e+08  0.969828     0.000692   \n",
       "...            ...           ...           ...       ...          ...   \n",
       "1749  5.718070e+05  7.192478e+13  8.480848e+06  0.161638     0.000559   \n",
       "1750  9.940976e+05  1.895267e+14  1.376687e+07 -0.193966     0.000572   \n",
       "1751  2.052530e+06  1.951923e+15  4.418057e+07  0.161638     0.000533   \n",
       "1752  1.713943e+06  1.355906e+15  3.682262e+07  0.193966     0.000288   \n",
       "1753  1.550412e+06  1.088354e+15  3.299020e+07  0.129310     0.000265   \n",
       "\n",
       "         kurtosis  ...  pos_neg_ratio  doppler_offset  main_lobe_width  \\\n",
       "0      721.726235  ...       1.530323        0.279423         0.258621   \n",
       "1      433.210982  ...       1.251952        0.112505         0.161638   \n",
       "2      421.248890  ...       1.574890        0.182513         0.193966   \n",
       "3      389.177448  ...       0.917316       -0.039091         1.907328   \n",
       "4      700.990139  ...       2.064459        0.364250         0.193966   \n",
       "...           ...  ...            ...             ...              ...   \n",
       "1749   640.340498  ...       0.927633       -0.015329         0.420259   \n",
       "1750   528.732149  ...       1.403761        0.040513         0.387931   \n",
       "1751  1119.092028  ...       1.275885        0.032924         0.420259   \n",
       "1752  1515.745649  ...       1.193892        0.027241         0.484914   \n",
       "1753  1611.329710  ...       1.729300        0.041178         0.096983   \n",
       "\n",
       "      motion_duration  doppler_peak_velocity  doppler_symmetry_index  \\\n",
       "0                1.07               1.002155                0.209587   \n",
       "1                2.30               0.937500                0.111882   \n",
       "2                2.00               0.840517                0.223268   \n",
       "3                2.01               0.840517               -0.043125   \n",
       "4                1.37               0.905172                0.347356   \n",
       "...               ...                    ...                     ...   \n",
       "1749             0.42               0.161638               -0.037542   \n",
       "1750             0.59               0.129310                0.167971   \n",
       "1751             0.46               0.161638                0.121221   \n",
       "1752             0.26               0.193966                0.088378   \n",
       "1753             0.22               0.129310                0.267211   \n",
       "\n",
       "      cepstral_entropy  range_bin_span  doppler_bandwidth  \\\n",
       "0             0.940833             6.0           0.836027   \n",
       "1             0.999645             6.0           0.915421   \n",
       "2             1.032866             5.0           0.705764   \n",
       "3             0.982990             6.0           0.738319   \n",
       "4             0.921222             6.0           0.762972   \n",
       "...                ...             ...                ...   \n",
       "1749          0.653246             3.0           0.272728   \n",
       "1750          0.877308             2.0           0.242563   \n",
       "1751          1.052820             2.0           0.204817   \n",
       "1752          1.140604             2.0           0.228499   \n",
       "1753          1.095919             2.0           0.195418   \n",
       "\n",
       "                                             image_path  \n",
       "0     spectrograms/1 December 2017 Dataset/1P36A01R0...  \n",
       "1     spectrograms/1 December 2017 Dataset/1P38A01R0...  \n",
       "2     spectrograms/1 December 2017 Dataset/1P38A01R0...  \n",
       "3     spectrograms/1 December 2017 Dataset/1P37A01R0...  \n",
       "4     spectrograms/1 December 2017 Dataset/1P36A01R0...  \n",
       "...                                                 ...  \n",
       "1749  spectrograms/7 March 2019 West Cumbria Dataset...  \n",
       "1750  spectrograms/7 March 2019 West Cumbria Dataset...  \n",
       "1751  spectrograms/7 March 2019 West Cumbria Dataset...  \n",
       "1752  spectrograms/7 March 2019 West Cumbria Dataset...  \n",
       "1753  spectrograms/Sample_data_preprocessing+label_e...  \n",
       "\n",
       "[1754 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INSHEP\n",
    "#display where path is duplicated and order by path\n",
    "INSHEP[INSHEP.duplicated(subset=['path'], keep=False)].sort_values(by='path')\n",
    "\n",
    "#remove duplicates based on 'path' column, keeping the first occurrence\n",
    "INSHEP = INSHEP.drop_duplicates(subset=['path'], keep='first')\n",
    "INSHEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c0ff5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = SPECTROGRAM.merge(INSHEP, on='image_path', how='inner')  # or 'left'/'right'/'outer' as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb4d667e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_torso_power</th>\n",
       "      <th>pos_neg_ratio</th>\n",
       "      <th>doppler_offset</th>\n",
       "      <th>main_lobe_width</th>\n",
       "      <th>motion_duration</th>\n",
       "      <th>doppler_peak_velocity</th>\n",
       "      <th>doppler_symmetry_index</th>\n",
       "      <th>cepstral_entropy</th>\n",
       "      <th>range_bin_span</th>\n",
       "      <th>doppler_bandwidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/3P66A03R2_spe...</td>\n",
       "      <td>-0.048340</td>\n",
       "      <td>0.055817</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>-0.373291</td>\n",
       "      <td>-0.008018</td>\n",
       "      <td>-0.236084</td>\n",
       "      <td>0.068909</td>\n",
       "      <td>0.140747</td>\n",
       "      <td>-0.217529</td>\n",
       "      <td>...</td>\n",
       "      <td>6.303265e+07</td>\n",
       "      <td>0.151130</td>\n",
       "      <td>-0.231972</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.193966</td>\n",
       "      <td>-0.737423</td>\n",
       "      <td>1.083437</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.236668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/1P68A01R1_spe...</td>\n",
       "      <td>0.034607</td>\n",
       "      <td>-0.034241</td>\n",
       "      <td>-0.101257</td>\n",
       "      <td>-0.314209</td>\n",
       "      <td>-0.255371</td>\n",
       "      <td>-0.125732</td>\n",
       "      <td>0.016068</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.256104</td>\n",
       "      <td>...</td>\n",
       "      <td>1.235783e+07</td>\n",
       "      <td>0.558474</td>\n",
       "      <td>-0.234554</td>\n",
       "      <td>0.420259</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.872845</td>\n",
       "      <td>-0.283306</td>\n",
       "      <td>0.909129</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.832668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/5P60A05R1_spe...</td>\n",
       "      <td>-0.086121</td>\n",
       "      <td>0.089172</td>\n",
       "      <td>-0.174683</td>\n",
       "      <td>-0.426758</td>\n",
       "      <td>-0.082092</td>\n",
       "      <td>-0.114990</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.198975</td>\n",
       "      <td>-0.243286</td>\n",
       "      <td>...</td>\n",
       "      <td>1.745773e+06</td>\n",
       "      <td>1.047543</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.323276</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.129310</td>\n",
       "      <td>0.023219</td>\n",
       "      <td>0.410934</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.762034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/3P65A03R3_spe...</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>0.034851</td>\n",
       "      <td>-0.234253</td>\n",
       "      <td>-0.427246</td>\n",
       "      <td>-0.048126</td>\n",
       "      <td>-0.329834</td>\n",
       "      <td>0.031830</td>\n",
       "      <td>0.111877</td>\n",
       "      <td>-0.276123</td>\n",
       "      <td>...</td>\n",
       "      <td>1.134535e+08</td>\n",
       "      <td>0.047521</td>\n",
       "      <td>-0.251409</td>\n",
       "      <td>0.129310</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.161638</td>\n",
       "      <td>-0.909270</td>\n",
       "      <td>1.122153</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.204580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spectrograms/4 July 2018 Dataset/1P62A01R2_spe...</td>\n",
       "      <td>0.058044</td>\n",
       "      <td>-0.131958</td>\n",
       "      <td>-0.123047</td>\n",
       "      <td>-0.252197</td>\n",
       "      <td>-0.213013</td>\n",
       "      <td>-0.140991</td>\n",
       "      <td>0.030991</td>\n",
       "      <td>-0.042145</td>\n",
       "      <td>-0.204224</td>\n",
       "      <td>...</td>\n",
       "      <td>9.166222e+06</td>\n",
       "      <td>1.839353</td>\n",
       "      <td>0.272753</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>0.295614</td>\n",
       "      <td>0.826051</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.963742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/6P56A06R0...</td>\n",
       "      <td>-0.045807</td>\n",
       "      <td>-0.061523</td>\n",
       "      <td>-0.045197</td>\n",
       "      <td>-0.329102</td>\n",
       "      <td>-0.037079</td>\n",
       "      <td>-0.083801</td>\n",
       "      <td>-0.027954</td>\n",
       "      <td>0.187988</td>\n",
       "      <td>-0.169800</td>\n",
       "      <td>...</td>\n",
       "      <td>3.642903e+06</td>\n",
       "      <td>0.016558</td>\n",
       "      <td>-0.585832</td>\n",
       "      <td>0.193966</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.517241</td>\n",
       "      <td>-0.967423</td>\n",
       "      <td>1.140569</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.293774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/1P42A01R0...</td>\n",
       "      <td>0.025467</td>\n",
       "      <td>-0.013214</td>\n",
       "      <td>-0.195801</td>\n",
       "      <td>-0.453125</td>\n",
       "      <td>-0.275391</td>\n",
       "      <td>-0.149902</td>\n",
       "      <td>-0.021835</td>\n",
       "      <td>0.052521</td>\n",
       "      <td>-0.262939</td>\n",
       "      <td>...</td>\n",
       "      <td>1.031794e+08</td>\n",
       "      <td>0.648905</td>\n",
       "      <td>-0.094934</td>\n",
       "      <td>1.745690</td>\n",
       "      <td>2.90</td>\n",
       "      <td>-0.743534</td>\n",
       "      <td>-0.212926</td>\n",
       "      <td>0.960106</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.736956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/6P40A06R0...</td>\n",
       "      <td>-0.044037</td>\n",
       "      <td>-0.016708</td>\n",
       "      <td>-0.088562</td>\n",
       "      <td>-0.362061</td>\n",
       "      <td>-0.119751</td>\n",
       "      <td>-0.167114</td>\n",
       "      <td>-0.026672</td>\n",
       "      <td>0.211792</td>\n",
       "      <td>-0.144775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.974422e+07</td>\n",
       "      <td>0.014511</td>\n",
       "      <td>-0.433081</td>\n",
       "      <td>0.129310</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.355603</td>\n",
       "      <td>-0.971394</td>\n",
       "      <td>1.054593</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.330681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>spectrograms/1 December 2017 Dataset/2P56A02R0...</td>\n",
       "      <td>0.064026</td>\n",
       "      <td>-0.040649</td>\n",
       "      <td>-0.171021</td>\n",
       "      <td>-0.335693</td>\n",
       "      <td>-0.171387</td>\n",
       "      <td>-0.112244</td>\n",
       "      <td>-0.003649</td>\n",
       "      <td>0.106323</td>\n",
       "      <td>-0.143188</td>\n",
       "      <td>...</td>\n",
       "      <td>1.168200e+08</td>\n",
       "      <td>15.373221</td>\n",
       "      <td>0.264797</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.877849</td>\n",
       "      <td>1.310697</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.270219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>spectrograms/Sample_data_preprocessing+label_e...</td>\n",
       "      <td>0.089233</td>\n",
       "      <td>0.015251</td>\n",
       "      <td>-0.126831</td>\n",
       "      <td>-0.284668</td>\n",
       "      <td>-0.037140</td>\n",
       "      <td>-0.150513</td>\n",
       "      <td>0.018188</td>\n",
       "      <td>0.124084</td>\n",
       "      <td>-0.143188</td>\n",
       "      <td>...</td>\n",
       "      <td>7.025300e+07</td>\n",
       "      <td>1.729300</td>\n",
       "      <td>0.041178</td>\n",
       "      <td>0.096983</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.129310</td>\n",
       "      <td>0.267211</td>\n",
       "      <td>1.095919</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.195418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1754 rows × 791 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             image_path  feature_0  feature_1  \\\n",
       "0     spectrograms/4 July 2018 Dataset/3P66A03R2_spe...  -0.048340   0.055817   \n",
       "1     spectrograms/4 July 2018 Dataset/1P68A01R1_spe...   0.034607  -0.034241   \n",
       "2     spectrograms/4 July 2018 Dataset/5P60A05R1_spe...  -0.086121   0.089172   \n",
       "3     spectrograms/4 July 2018 Dataset/3P65A03R3_spe...  -0.034821   0.034851   \n",
       "4     spectrograms/4 July 2018 Dataset/1P62A01R2_spe...   0.058044  -0.131958   \n",
       "...                                                 ...        ...        ...   \n",
       "1749  spectrograms/1 December 2017 Dataset/6P56A06R0...  -0.045807  -0.061523   \n",
       "1750  spectrograms/1 December 2017 Dataset/1P42A01R0...   0.025467  -0.013214   \n",
       "1751  spectrograms/1 December 2017 Dataset/6P40A06R0...  -0.044037  -0.016708   \n",
       "1752  spectrograms/1 December 2017 Dataset/2P56A02R0...   0.064026  -0.040649   \n",
       "1753  spectrograms/Sample_data_preprocessing+label_e...   0.089233   0.015251   \n",
       "\n",
       "      feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  \\\n",
       "0     -0.145508  -0.373291  -0.008018  -0.236084   0.068909   0.140747   \n",
       "1     -0.101257  -0.314209  -0.255371  -0.125732   0.016068  -0.000095   \n",
       "2     -0.174683  -0.426758  -0.082092  -0.114990  -0.052063   0.198975   \n",
       "3     -0.234253  -0.427246  -0.048126  -0.329834   0.031830   0.111877   \n",
       "4     -0.123047  -0.252197  -0.213013  -0.140991   0.030991  -0.042145   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1749  -0.045197  -0.329102  -0.037079  -0.083801  -0.027954   0.187988   \n",
       "1750  -0.195801  -0.453125  -0.275391  -0.149902  -0.021835   0.052521   \n",
       "1751  -0.088562  -0.362061  -0.119751  -0.167114  -0.026672   0.211792   \n",
       "1752  -0.171021  -0.335693  -0.171387  -0.112244  -0.003649   0.106323   \n",
       "1753  -0.126831  -0.284668  -0.037140  -0.150513   0.018188   0.124084   \n",
       "\n",
       "      feature_8  ...  mean_torso_power  pos_neg_ratio  doppler_offset  \\\n",
       "0     -0.217529  ...      6.303265e+07       0.151130       -0.231972   \n",
       "1     -0.256104  ...      1.235783e+07       0.558474       -0.234554   \n",
       "2     -0.243286  ...      1.745773e+06       1.047543        0.001466   \n",
       "3     -0.276123  ...      1.134535e+08       0.047521       -0.251409   \n",
       "4     -0.204224  ...      9.166222e+06       1.839353        0.272753   \n",
       "...         ...  ...               ...            ...             ...   \n",
       "1749  -0.169800  ...      3.642903e+06       0.016558       -0.585832   \n",
       "1750  -0.262939  ...      1.031794e+08       0.648905       -0.094934   \n",
       "1751  -0.144775  ...      1.974422e+07       0.014511       -0.433081   \n",
       "1752  -0.143188  ...      1.168200e+08      15.373221        0.264797   \n",
       "1753  -0.143188  ...      7.025300e+07       1.729300        0.041178   \n",
       "\n",
       "      main_lobe_width  motion_duration  doppler_peak_velocity  \\\n",
       "0            0.161638             0.31              -0.193966   \n",
       "1            0.420259             0.53              -0.872845   \n",
       "2            0.323276             0.05               0.129310   \n",
       "3            0.129310             0.13              -0.161638   \n",
       "4            0.258621             0.93               0.905172   \n",
       "...               ...              ...                    ...   \n",
       "1749         0.193966             0.14              -0.517241   \n",
       "1750         1.745690             2.90              -0.743534   \n",
       "1751         0.129310             0.31              -0.355603   \n",
       "1752         0.161638             0.25               0.161638   \n",
       "1753         0.096983             0.22               0.129310   \n",
       "\n",
       "      doppler_symmetry_index  cepstral_entropy  range_bin_span  \\\n",
       "0                  -0.737423          1.083437             6.0   \n",
       "1                  -0.283306          0.909129            12.0   \n",
       "2                   0.023219          0.410934             8.0   \n",
       "3                  -0.909270          1.122153             6.0   \n",
       "4                   0.295614          0.826051            10.0   \n",
       "...                      ...               ...             ...   \n",
       "1749               -0.967423          1.140569             5.0   \n",
       "1750               -0.212926          0.960106             6.0   \n",
       "1751               -0.971394          1.054593             6.0   \n",
       "1752                0.877849          1.310697             5.0   \n",
       "1753                0.267211          1.095919             2.0   \n",
       "\n",
       "      doppler_bandwidth  \n",
       "0              0.236668  \n",
       "1              0.832668  \n",
       "2              0.762034  \n",
       "3              0.204580  \n",
       "4              0.963742  \n",
       "...                 ...  \n",
       "1749           0.293774  \n",
       "1750           0.736956  \n",
       "1751           0.330681  \n",
       "1752           0.270219  \n",
       "1753           0.195418  \n",
       "\n",
       "[1754 rows x 791 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d022ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the activity column like this \n",
    "ACTIVITY_MAP = {\n",
    "    \"walking\": 1,\n",
    "    \"sitting_down\": 2,\n",
    "    \"standing_up\": 3,\n",
    "    \"pick_object\": 4,\n",
    "    \"drink_water\": 5,\n",
    "    \"fall\": 6,\n",
    "}\n",
    "merged_df['activity'] = merged_df['activity'].map(ACTIVITY_MAP)\n",
    "\n",
    "merged_df=merged_df.drop(columns=['path', 'file_id', 'image_path'])  # drop unnecessary columns\n",
    "#split in train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged_df.drop(columns=['activity']),\n",
    "                                                    merged_df['activity'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=merged_df['activity'])\n",
    "#validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                  y_train,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=42,\n",
    "                                                  stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb8eb62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     walking       1.00      0.98      0.99        62\n",
      "sitting_down       0.95      0.97      0.96        63\n",
      " standing_up       0.92      0.95      0.94        62\n",
      " pick_object       0.73      0.77      0.75        62\n",
      " drink_water       0.79      0.74      0.77        62\n",
      "        fall       0.97      0.93      0.95        40\n",
      "\n",
      "    accuracy                           0.89       351\n",
      "   macro avg       0.89      0.89      0.89       351\n",
      "weighted avg       0.89      0.89      0.89       351\n",
      "\n",
      "Accuracy: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "#randomforest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#evaluate the model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=ACTIVITY_MAP.keys()))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed4c195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.98      0.99        62\n",
      "           2       0.95      0.97      0.96        63\n",
      "           3       0.92      0.95      0.94        62\n",
      "           4       0.73      0.77      0.75        62\n",
      "           5       0.79      0.74      0.77        62\n",
      "           6       0.97      0.93      0.95        40\n",
      "\n",
      "    accuracy                           0.89       351\n",
      "   macro avg       0.89      0.89      0.89       351\n",
      "weighted avg       0.89      0.89      0.89       351\n",
      "\n",
      "Accuracy with scaled features: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "#standardize the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "#fit the model again with scaled features\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "#evaluate the model again\n",
    "y_pred_scaled = rf.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred_scaled))\n",
    "print(\"Accuracy with scaled features:\", accuracy_score(y_test, y_pred_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ff3f219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.98      0.97        62\n",
      "           2       0.69      0.76      0.72        63\n",
      "           3       0.65      0.77      0.71        62\n",
      "           4       0.57      0.74      0.65        62\n",
      "           5       0.69      0.44      0.53        62\n",
      "           6       1.00      0.60      0.75        40\n",
      "\n",
      "    accuracy                           0.72       351\n",
      "   macro avg       0.76      0.72      0.72       351\n",
      "weighted avg       0.74      0.72      0.72       351\n",
      "\n",
      "Accuracy with PCA features: 0.7236467236467237\n"
     ]
    }
   ],
   "source": [
    "#PCA for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=768)  # retain 95% of variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "#fit the model again with PCA features\n",
    "rf.fit(X_train_pca, y_train)\n",
    "#evaluate the model again\n",
    "y_pred_pca = rf.predict(X_test_pca)\n",
    "print(classification_report(y_test, y_pred_pca))\n",
    "print(\"Accuracy with PCA features:\", accuracy_score(y_test, y_pred_pca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a0a7259",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RFECV\n\u001b[0;32m      3\u001b[0m selector \u001b[38;5;241m=\u001b[39m RFECV(estimator\u001b[38;5;241m=\u001b[39mrf, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m selector \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Get the selected features\u001b[39;00m\n\u001b[0;32m      6\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mcolumns[selector\u001b[38;5;241m.\u001b[39msupport_]\n",
      "File \u001b[1;32mc:\\Users\\Adnane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Adnane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:777\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    774\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    775\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[1;32m--> 777\u001b[0m scores_features \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m scores, step_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mscores_features)\n\u001b[0;32m    783\u001b[0m step_n_features_rev \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(step_n_features[\u001b[38;5;241m0\u001b[39m])[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Adnane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Adnane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Adnane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adnane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#leave one out feature search\n",
    "from sklearn.feature_selection import RFECV\n",
    "selector = RFECV(estimator=rf, step=1, cv=5, scoring='accuracy', verbose = 4, n_jobs=-1)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "# Get the selected features\n",
    "selected_features = X_train.columns[selector.support_]\n",
    "print(\"Selected features:\", selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04581883",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'selected_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#only keep selected features\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#Selected features: ['feature_2', 'feature_8', 'feature_13', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_27', 'feature_29', 'feature_34', 'feature_40', 'feature_41', 'feature_50', 'feature_61', 'feature_62', 'feature_64', 'feature_66', 'feature_67', 'feature_72', 'feature_79', 'feature_82', 'feature_84', 'feature_87', 'feature_89', 'feature_90', 'feature_92', 'feature_108', 'feature_114', 'feature_116', 'feature_120', 'feature_127', 'feature_128', 'feature_132', 'feature_136', 'feature_137', 'feature_138', 'feature_141', 'feature_150', 'feature_151', 'feature_157', 'feature_159', 'feature_164', 'feature_165', 'feature_166', 'feature_170', 'feature_172', 'feature_176', 'feature_180', 'feature_182', 'feature_184', 'feature_185', 'feature_186', 'feature_190', 'feature_192', 'feature_198', 'feature_204', 'feature_205', 'feature_210', 'feature_213', 'feature_214', 'feature_221', 'feature_225', 'feature_229', 'feature_233', 'feature_234', 'feature_240', 'feature_243', 'feature_254', 'feature_256', 'feature_257', 'feature_262', 'feature_266', 'feature_267', 'feature_269', 'feature_270', 'feature_275', 'feature_276', 'feature_278', 'feature_279', 'feature_284', 'feature_288', 'feature_293', 'feature_297', 'feature_298', 'feature_301', 'feature_306', 'feature_308', 'feature_309', 'feature_311', 'feature_314', 'feature_315', 'feature_319', 'feature_324', 'feature_328', 'feature_331', 'feature_334', 'feature_336', 'feature_337', 'feature_340', 'feature_342', 'feature_344', 'feature_353', 'feature_356', 'feature_357', 'feature_367', 'feature_371', 'feature_375', 'feature_377', 'feature_380', 'feature_381', 'feature_383', 'feature_385', 'feature_387', 'feature_389', 'feature_390', 'feature_395', 'feature_396', 'feature_407', 'feature_408', 'feature_409', 'feature_413', 'feature_415', 'feature_418', 'feature_426', 'feature_427', 'feature_430', 'feature_432', 'feature_441', 'feature_444', 'feature_448', 'feature_452', 'feature_462', 'feature_465', 'feature_466', 'feature_467', 'feature_477', 'feature_479', 'feature_483', 'feature_486', 'feature_489', 'feature_492', 'feature_497', 'feature_498', 'feature_500', 'feature_504', 'feature_512', 'feature_515', 'feature_521', 'feature_524', 'feature_527', 'feature_538', 'feature_541', 'feature_544', 'feature_547', 'feature_550', 'feature_552', 'feature_556', 'feature_557', 'feature_558', 'feature_564', 'feature_565', 'feature_570', 'feature_574', 'feature_579', 'feature_580', 'feature_581', 'feature_582', 'feature_584', 'feature_587', 'feature_589', 'feature_596', 'feature_597', 'feature_605', 'feature_614', 'feature_617', 'feature_620', 'feature_624', 'feature_629', 'feature_630', 'feature_639', 'feature_650', 'feature_651', 'feature_654', 'feature_656', 'feature_665', 'feature_667', 'feature_668', 'feature_671', 'feature_673', 'feature_679', 'feature_685', 'feature_690', 'feature_695', 'feature_705', 'feature_712', 'feature_714', 'feature_718', 'feature_719', 'feature_724', 'feature_732', 'feature_735', 'feature_737', 'feature_741', 'feature_747', 'feature_748', 'feature_753', 'feature_756', 'feature_760', 'feature_765', 'feature_766', 'mean_entropy', 'mean_power', 'variance', 'stddev', 'max_vel', 'periodicity', 'mean_torso_power', 'pos_neg_ratio', 'doppler_offset', 'main_lobe_width', 'doppler_peak_velocity', 'doppler_symmetry_index', 'cepstral_entropy', 'doppler_bandwidth']\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train[\u001b[43mselected_features\u001b[49m]\n\u001b[0;32m      5\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test[selected_features]\n\u001b[0;32m      6\u001b[0m X_val \u001b[38;5;241m=\u001b[39m X_val[selected_features]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'selected_features' is not defined"
     ]
    }
   ],
   "source": [
    "#only keep selected features\n",
    "#Selected features: ['feature_2', 'feature_8', 'feature_13', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_27', 'feature_29', 'feature_34', 'feature_40', 'feature_41', 'feature_50', 'feature_61', 'feature_62', 'feature_64', 'feature_66', 'feature_67', 'feature_72', 'feature_79', 'feature_82', 'feature_84', 'feature_87', 'feature_89', 'feature_90', 'feature_92', 'feature_108', 'feature_114', 'feature_116', 'feature_120', 'feature_127', 'feature_128', 'feature_132', 'feature_136', 'feature_137', 'feature_138', 'feature_141', 'feature_150', 'feature_151', 'feature_157', 'feature_159', 'feature_164', 'feature_165', 'feature_166', 'feature_170', 'feature_172', 'feature_176', 'feature_180', 'feature_182', 'feature_184', 'feature_185', 'feature_186', 'feature_190', 'feature_192', 'feature_198', 'feature_204', 'feature_205', 'feature_210', 'feature_213', 'feature_214', 'feature_221', 'feature_225', 'feature_229', 'feature_233', 'feature_234', 'feature_240', 'feature_243', 'feature_254', 'feature_256', 'feature_257', 'feature_262', 'feature_266', 'feature_267', 'feature_269', 'feature_270', 'feature_275', 'feature_276', 'feature_278', 'feature_279', 'feature_284', 'feature_288', 'feature_293', 'feature_297', 'feature_298', 'feature_301', 'feature_306', 'feature_308', 'feature_309', 'feature_311', 'feature_314', 'feature_315', 'feature_319', 'feature_324', 'feature_328', 'feature_331', 'feature_334', 'feature_336', 'feature_337', 'feature_340', 'feature_342', 'feature_344', 'feature_353', 'feature_356', 'feature_357', 'feature_367', 'feature_371', 'feature_375', 'feature_377', 'feature_380', 'feature_381', 'feature_383', 'feature_385', 'feature_387', 'feature_389', 'feature_390', 'feature_395', 'feature_396', 'feature_407', 'feature_408', 'feature_409', 'feature_413', 'feature_415', 'feature_418', 'feature_426', 'feature_427', 'feature_430', 'feature_432', 'feature_441', 'feature_444', 'feature_448', 'feature_452', 'feature_462', 'feature_465', 'feature_466', 'feature_467', 'feature_477', 'feature_479', 'feature_483', 'feature_486', 'feature_489', 'feature_492', 'feature_497', 'feature_498', 'feature_500', 'feature_504', 'feature_512', 'feature_515', 'feature_521', 'feature_524', 'feature_527', 'feature_538', 'feature_541', 'feature_544', 'feature_547', 'feature_550', 'feature_552', 'feature_556', 'feature_557', 'feature_558', 'feature_564', 'feature_565', 'feature_570', 'feature_574', 'feature_579', 'feature_580', 'feature_581', 'feature_582', 'feature_584', 'feature_587', 'feature_589', 'feature_596', 'feature_597', 'feature_605', 'feature_614', 'feature_617', 'feature_620', 'feature_624', 'feature_629', 'feature_630', 'feature_639', 'feature_650', 'feature_651', 'feature_654', 'feature_656', 'feature_665', 'feature_667', 'feature_668', 'feature_671', 'feature_673', 'feature_679', 'feature_685', 'feature_690', 'feature_695', 'feature_705', 'feature_712', 'feature_714', 'feature_718', 'feature_719', 'feature_724', 'feature_732', 'feature_735', 'feature_737', 'feature_741', 'feature_747', 'feature_748', 'feature_753', 'feature_756', 'feature_760', 'feature_765', 'feature_766', 'mean_entropy', 'mean_power', 'variance', 'stddev', 'max_vel', 'periodicity', 'mean_torso_power', 'pos_neg_ratio', 'doppler_offset', 'main_lobe_width', 'doppler_peak_velocity', 'doppler_symmetry_index', 'cepstral_entropy', 'doppler_bandwidth']\n",
    "\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "X_val = X_val[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b8dee6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.98      0.99        62\n",
      "           2       0.95      0.97      0.96        63\n",
      "           3       0.91      0.94      0.92        62\n",
      "           4       0.72      0.77      0.74        62\n",
      "           5       0.79      0.73      0.76        62\n",
      "           6       0.97      0.93      0.95        40\n",
      "\n",
      "    accuracy                           0.88       351\n",
      "   macro avg       0.89      0.89      0.89       351\n",
      "weighted avg       0.88      0.88      0.88       351\n",
      "\n",
      "Accuracy with selected features: 0.8831908831908832\n"
     ]
    }
   ],
   "source": [
    "#random forest classifier with selected features\n",
    "rf.fit(X_train, y_train)\n",
    "#evaluate the model with selected features\n",
    "y_pred_selected = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_selected))\n",
    "print(\"Accuracy with selected features:\", accuracy_score(y_test, y_pred_selected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4f41de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.98      0.99        62\n",
      "           2       0.93      0.98      0.95        63\n",
      "           3       0.97      0.97      0.97        62\n",
      "           4       0.69      0.79      0.74        62\n",
      "           5       0.78      0.65      0.71        62\n",
      "           6       0.97      0.95      0.96        40\n",
      "\n",
      "    accuracy                           0.88       351\n",
      "   macro avg       0.89      0.89      0.89       351\n",
      "weighted avg       0.89      0.88      0.88       351\n",
      "\n",
      "MLP Accuracy: 0.8831908831908832\n"
     ]
    }
   ],
   "source": [
    "#MLP classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "#fit the model\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "#evaluate the model\n",
    "y_pred_mlp = mlp.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred_mlp))\n",
    "print(\"MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b25e54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_train_tensor: tensor([0, 1, 2, 3, 4, 5])\n",
      "Unique values in y_val_tensor: tensor([0, 1, 2, 3, 4, 5])\n",
      "Unique values in y_test_tensor: tensor([0, 1, 2, 3, 4, 5])\n",
      "num_classes: 6\n",
      "X_train_tensor has NaNs: tensor(False)\n",
      "X_train_tensor has Infs: tensor(False)\n",
      "X_val_tensor has NaNs: tensor(False)\n",
      "X_val_tensor has Infs: tensor(False)\n",
      "X_test_tensor has NaNs: tensor(False)\n",
      "X_test_tensor has Infs: tensor(False)\n",
      "Using device: cuda\n",
      "Epoch [1/5000], Loss: 1.1320, Val Loss: 0.6731\n",
      "Epoch [2/5000], Loss: 0.6543, Val Loss: 0.5355\n",
      "Epoch [3/5000], Loss: 0.5524, Val Loss: 0.4800\n",
      "Epoch [4/5000], Loss: 0.4963, Val Loss: 0.4581\n",
      "Epoch [5/5000], Loss: 0.4494, Val Loss: 0.4516\n",
      "Epoch [6/5000], Loss: 0.4242, Val Loss: 0.4367\n",
      "Epoch [7/5000], Loss: 0.3950, Val Loss: 0.4580\n",
      "Epoch [8/5000], Loss: 0.3736, Val Loss: 0.4286\n",
      "Epoch [9/5000], Loss: 0.3287, Val Loss: 0.4390\n",
      "Epoch [10/5000], Loss: 0.3333, Val Loss: 0.4255\n",
      "Epoch [11/5000], Loss: 0.3216, Val Loss: 0.4572\n",
      "Epoch [12/5000], Loss: 0.3122, Val Loss: 0.4296\n",
      "Epoch [13/5000], Loss: 0.2846, Val Loss: 0.4468\n",
      "Epoch [14/5000], Loss: 0.2769, Val Loss: 0.4293\n",
      "Epoch [15/5000], Loss: 0.2631, Val Loss: 0.4468\n",
      "Epoch [16/5000], Loss: 0.2539, Val Loss: 0.4322\n",
      "Epoch [17/5000], Loss: 0.2320, Val Loss: 0.4492\n",
      "Epoch [18/5000], Loss: 0.2501, Val Loss: 0.4684\n",
      "Epoch [19/5000], Loss: 0.2391, Val Loss: 0.4803\n",
      "Epoch [20/5000], Loss: 0.2175, Val Loss: 0.4845\n",
      "Epoch [21/5000], Loss: 0.2148, Val Loss: 0.4500\n",
      "Epoch [22/5000], Loss: 0.2099, Val Loss: 0.4771\n",
      "Epoch [23/5000], Loss: 0.2169, Val Loss: 0.4743\n",
      "Epoch [24/5000], Loss: 0.2166, Val Loss: 0.4935\n",
      "Epoch [25/5000], Loss: 0.2184, Val Loss: 0.4747\n",
      "Epoch [26/5000], Loss: 0.1996, Val Loss: 0.5000\n",
      "Epoch [27/5000], Loss: 0.1936, Val Loss: 0.5068\n",
      "Epoch [28/5000], Loss: 0.1786, Val Loss: 0.4971\n",
      "Epoch [29/5000], Loss: 0.1707, Val Loss: 0.5019\n",
      "Epoch [30/5000], Loss: 0.1697, Val Loss: 0.5117\n",
      "Epoch [31/5000], Loss: 0.1703, Val Loss: 0.5268\n",
      "Epoch [32/5000], Loss: 0.1668, Val Loss: 0.5111\n",
      "Epoch [33/5000], Loss: 0.1679, Val Loss: 0.5415\n",
      "Epoch [34/5000], Loss: 0.1351, Val Loss: 0.5268\n",
      "Epoch [35/5000], Loss: 0.1382, Val Loss: 0.5325\n",
      "Epoch [36/5000], Loss: 0.1678, Val Loss: 0.5178\n",
      "Epoch [37/5000], Loss: 0.1386, Val Loss: 0.5440\n",
      "Epoch [38/5000], Loss: 0.1449, Val Loss: 0.5523\n",
      "Epoch [39/5000], Loss: 0.1255, Val Loss: 0.5765\n",
      "Epoch [40/5000], Loss: 0.1200, Val Loss: 0.5933\n",
      "Epoch [41/5000], Loss: 0.1177, Val Loss: 0.5976\n",
      "Epoch [42/5000], Loss: 0.1361, Val Loss: 0.5870\n",
      "Epoch [43/5000], Loss: 0.1107, Val Loss: 0.5979\n",
      "Epoch [44/5000], Loss: 0.1144, Val Loss: 0.5985\n",
      "Epoch [45/5000], Loss: 0.1184, Val Loss: 0.5989\n",
      "Epoch [46/5000], Loss: 0.1158, Val Loss: 0.6089\n",
      "Epoch [47/5000], Loss: 0.1292, Val Loss: 0.6167\n",
      "Epoch [48/5000], Loss: 0.1170, Val Loss: 0.5950\n",
      "Epoch [49/5000], Loss: 0.0962, Val Loss: 0.5945\n",
      "Epoch [50/5000], Loss: 0.1120, Val Loss: 0.6145\n",
      "Epoch [51/5000], Loss: 0.1087, Val Loss: 0.6278\n",
      "Epoch [52/5000], Loss: 0.1009, Val Loss: 0.6645\n",
      "Epoch [53/5000], Loss: 0.1126, Val Loss: 0.6446\n",
      "Epoch [54/5000], Loss: 0.0933, Val Loss: 0.6214\n",
      "Epoch [55/5000], Loss: 0.0970, Val Loss: 0.6448\n",
      "Epoch [56/5000], Loss: 0.0952, Val Loss: 0.6438\n",
      "Epoch [57/5000], Loss: 0.0994, Val Loss: 0.6524\n",
      "Epoch [58/5000], Loss: 0.0869, Val Loss: 0.6794\n",
      "Epoch [59/5000], Loss: 0.0805, Val Loss: 0.7328\n",
      "Epoch [60/5000], Loss: 0.0918, Val Loss: 0.7296\n",
      "Early stopping triggered\n",
      "Loading best model from best_model.pth\n",
      "Test Loss: 0.3886, Test Accuracy: 0.8490\n",
      "Final model state saved to final_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\AppData\\Local\\Temp\\ipykernel_24760\\3199207767.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd # Assuming X_train_pca is a pandas DataFrame\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Encode labels to start from 0 using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Fit on all unique labels across all sets to ensure consistency\n",
    "le.fit(np.unique(np.concatenate([y_train, y_val, y_test])))\n",
    "\n",
    "# Transform all label sets\n",
    "y_train_tensor = torch.tensor(le.transform(y_train), dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(le.transform(y_val), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(le.transform(y_test), dtype=torch.long)\n",
    "\n",
    "# Define number of classes\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Debug: Check label ranges\n",
    "print(\"Unique values in y_train_tensor:\", torch.unique(y_train_tensor))\n",
    "print(\"Unique values in y_val_tensor:\", torch.unique(y_val_tensor))\n",
    "print(\"Unique values in y_test_tensor:\", torch.unique(y_test_tensor))\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "# Assertions for label range (already present, good practice)\n",
    "assert torch.all(y_train_tensor >= 0) and torch.all(y_train_tensor < num_classes), \\\n",
    "    f\"y_train_tensor has out-of-range labels. Min: {y_train_tensor.min()}, Max: {y_train_tensor.max()}, Expected range [0, {num_classes-1}]\"\n",
    "assert torch.all(y_val_tensor >= 0) and torch.all(y_val_tensor < num_classes), \\\n",
    "    f\"y_val_tensor has out-of-range labels. Min: {y_val_tensor.min()}, Max: {y_val_tensor.max()}, Expected range [0, {num_classes-1}]\"\n",
    "assert torch.all(y_test_tensor >= 0) and torch.all(y_test_tensor < num_classes), \\\n",
    "    f\"y_test_tensor has out-of-range labels. Min: {y_test_tensor.min()}, Max: {y_test_tensor.max()}, Expected range [0, {num_classes-1}]\"\n",
    "\n",
    "\n",
    "# Convert PCA features to PyTorch tensors\n",
    "# Use directly if X_pca is a numpy array\n",
    "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_pca, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "\n",
    "# Check for NaNs/Infs in input tensors\n",
    "print(\"X_train_tensor has NaNs:\", torch.isnan(X_train_tensor).any())\n",
    "print(\"X_train_tensor has Infs:\", torch.isinf(X_train_tensor).any())\n",
    "print(\"X_val_tensor has NaNs:\", torch.isnan(X_val_tensor).any())\n",
    "print(\"X_val_tensor has Infs:\", torch.isinf(X_val_tensor).any())\n",
    "print(\"X_test_tensor has NaNs:\", torch.isnan(X_test_tensor).any())\n",
    "print(\"X_test_tensor has Infs:\", torch.isinf(X_test_tensor).any())\n",
    "\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x) # No activation here, as CrossEntropyLoss expects logits\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = X_train_pca.shape[1]\n",
    "model = SimpleNN(input_size, num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 5000\n",
    "patience = 50\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "best_model_path = 'best_model.pth' # Define path for saving best model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)  # Save best model\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Load best model for evaluation\n",
    "print(f\"Loading best model from {best_model_path}\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "with torch.no_grad():\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(X_test_tensor)\n",
    "test_accuracy = test_correct / len(X_test_tensor)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Save final model (optional, often you'd just use the best_model.pth for deployment)\n",
    "final_model_path = 'final_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model state saved to {final_model_path}\")\n",
    "\n",
    "# Load model for inference (example)\n",
    "# loaded_model = SimpleNN(input_size, num_classes)\n",
    "# loaded_model.load_state_dict(torch.load('final_model.pth'))\n",
    "# loaded_model.to(device) # Don't forget to move it to device if you want to use it on GPU\n",
    "# loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c12ee722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "|   iter    |  target   | batch_... | dropou... | layer1... | layer2... | layer3... | learni... | optimi... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-0.2676  \u001b[39m | \u001b[39m201.8    \u001b[39m | \u001b[39m0.8556   \u001b[39m | \u001b[39m1.133e+03\u001b[39m | \u001b[39m932.4    \u001b[39m | \u001b[39m266.7    \u001b[39m | \u001b[39m0.0156   \u001b[39m | \u001b[39m0.1162   \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m-0.4348  \u001b[39m | \u001b[39m445.6    \u001b[39m | \u001b[39m0.541    \u001b[39m | \u001b[39m1.097e+03\u001b[39m | \u001b[39m62.96    \u001b[39m | \u001b[39m1.491e+03\u001b[39m | \u001b[39m0.08324  \u001b[39m | \u001b[39m0.4247   \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m-0.3299  \u001b[39m | \u001b[39m106.2    \u001b[39m | \u001b[39m0.1651   \u001b[39m | \u001b[39m489.6    \u001b[39m | \u001b[39m821.2    \u001b[39m | \u001b[39m681.6    \u001b[39m | \u001b[39m0.02912  \u001b[39m | \u001b[39m1.224    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-0.3424  \u001b[39m | \u001b[39m85.19    \u001b[39m | \u001b[39m0.2629   \u001b[39m | \u001b[39m583.0    \u001b[39m | \u001b[39m717.9    \u001b[39m | \u001b[39m1.213e+03\u001b[39m | \u001b[39m0.01997  \u001b[39m | \u001b[39m1.028    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-0.3834  \u001b[39m | \u001b[39m309.8    \u001b[39m | \u001b[39m0.04181  \u001b[39m | \u001b[39m945.7    \u001b[39m | \u001b[39m288.5    \u001b[39m | \u001b[39m129.8    \u001b[39m | \u001b[39m0.09489  \u001b[39m | \u001b[39m1.931    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m-0.4683  \u001b[39m | \u001b[39m417.0    \u001b[39m | \u001b[39m0.2742   \u001b[39m | \u001b[39m178.9    \u001b[39m | \u001b[39m1.061e+03\u001b[39m | \u001b[39m694.0    \u001b[39m | \u001b[39m0.0122   \u001b[39m | \u001b[39m0.9904   \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-0.3017  \u001b[39m | \u001b[39m33.06    \u001b[39m | \u001b[39m0.8184   \u001b[39m | \u001b[39m421.2    \u001b[39m | \u001b[39m1.028e+03\u001b[39m | \u001b[39m500.8    \u001b[39m | \u001b[39m0.05201  \u001b[39m | \u001b[39m1.093    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m-0.3522  \u001b[39m | \u001b[39m107.7    \u001b[39m | \u001b[39m0.8726   \u001b[39m | \u001b[39m1.198e+03\u001b[39m | \u001b[39m1.445e+03\u001b[39m | \u001b[39m1.378e+03\u001b[39m | \u001b[39m0.05979  \u001b[39m | \u001b[39m1.844    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m-0.3237  \u001b[39m | \u001b[39m59.89    \u001b[39m | \u001b[39m0.1764   \u001b[39m | \u001b[39m100.0    \u001b[39m | \u001b[39m521.3    \u001b[39m | \u001b[39m616.6    \u001b[39m | \u001b[39m0.02713  \u001b[39m | \u001b[39m1.657    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m-0.3099  \u001b[39m | \u001b[39m192.9    \u001b[39m | \u001b[39m0.2528   \u001b[39m | \u001b[39m848.2    \u001b[39m | \u001b[39m244.0    \u001b[39m | \u001b[39m1.239e+03\u001b[39m | \u001b[39m0.007455 \u001b[39m | \u001b[39m1.974    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m-0.354   \u001b[39m | \u001b[39m172.9    \u001b[39m | \u001b[39m0.2642   \u001b[39m | \u001b[39m1.024e+03\u001b[39m | \u001b[39m910.8    \u001b[39m | \u001b[39m314.5    \u001b[39m | \u001b[39m0.04099  \u001b[39m | \u001b[39m1.352    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m-0.4796  \u001b[39m | \u001b[39m47.93    \u001b[39m | \u001b[39m0.09807  \u001b[39m | \u001b[39m1.38e+03 \u001b[39m | \u001b[39m1.31e+03 \u001b[39m | \u001b[39m1.163e+03\u001b[39m | \u001b[39m0.02789  \u001b[39m | \u001b[39m0.9956   \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m-1.789   \u001b[39m | \u001b[39m160.2    \u001b[39m | \u001b[39m0.007358 \u001b[39m | \u001b[39m1.171e+03\u001b[39m | \u001b[39m953.3    \u001b[39m | \u001b[39m231.6    \u001b[39m | \u001b[39m1e-08    \u001b[39m | \u001b[39m1.935    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m-0.6112  \u001b[39m | \u001b[39m511.3    \u001b[39m | \u001b[39m0.04605  \u001b[39m | \u001b[39m1.209e+03\u001b[39m | \u001b[39m1.148e+03\u001b[39m | \u001b[39m1.224e+03\u001b[39m | \u001b[39m0.03493  \u001b[39m | \u001b[39m0.9288   \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m-0.3008  \u001b[39m | \u001b[39m291.8    \u001b[39m | \u001b[39m0.8134   \u001b[39m | \u001b[39m1.027e+03\u001b[39m | \u001b[39m378.7    \u001b[39m | \u001b[39m1.485e+03\u001b[39m | \u001b[39m0.03961  \u001b[39m | \u001b[39m0.05533  \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m-0.5626  \u001b[39m | \u001b[39m417.3    \u001b[39m | \u001b[39m0.1735   \u001b[39m | \u001b[39m1.312e+03\u001b[39m | \u001b[39m1.011e+03\u001b[39m | \u001b[39m1.029e+03\u001b[39m | \u001b[39m0.00512  \u001b[39m | \u001b[39m0.8376   \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m-0.3492  \u001b[39m | \u001b[39m368.0    \u001b[39m | \u001b[39m0.5683   \u001b[39m | \u001b[39m1.238e+03\u001b[39m | \u001b[39m1.046e+03\u001b[39m | \u001b[39m1.303e+03\u001b[39m | \u001b[39m0.02156  \u001b[39m | \u001b[39m1.92     \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m-0.2843  \u001b[39m | \u001b[39m75.08    \u001b[39m | \u001b[39m0.8553   \u001b[39m | \u001b[39m1.488e+03\u001b[39m | \u001b[39m1.466e+03\u001b[39m | \u001b[39m1.423e+03\u001b[39m | \u001b[39m0.02364  \u001b[39m | \u001b[39m0.8428   \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m-0.5241  \u001b[39m | \u001b[39m500.2    \u001b[39m | \u001b[39m0.6299   \u001b[39m | \u001b[39m1.39e+03 \u001b[39m | \u001b[39m700.8    \u001b[39m | \u001b[39m645.6    \u001b[39m | \u001b[39m0.02404  \u001b[39m | \u001b[39m0.3581   \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m-0.3301  \u001b[39m | \u001b[39m228.4    \u001b[39m | \u001b[39m0.2945   \u001b[39m | \u001b[39m810.3    \u001b[39m | \u001b[39m1.508e+03\u001b[39m | \u001b[39m57.32    \u001b[39m | \u001b[39m0.08906  \u001b[39m | \u001b[39m1.039    \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m-0.3153  \u001b[39m | \u001b[39m19.21    \u001b[39m | \u001b[39m0.4347   \u001b[39m | \u001b[39m469.5    \u001b[39m | \u001b[39m913.9    \u001b[39m | \u001b[39m800.4    \u001b[39m | \u001b[39m0.01597  \u001b[39m | \u001b[39m1.933    \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m-0.2936  \u001b[39m | \u001b[39m401.7    \u001b[39m | \u001b[39m0.6429   \u001b[39m | \u001b[39m932.2    \u001b[39m | \u001b[39m71.1     \u001b[39m | \u001b[39m696.9    \u001b[39m | \u001b[39m0.05694  \u001b[39m | \u001b[39m1.351    \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m-0.9429  \u001b[39m | \u001b[39m183.3    \u001b[39m | \u001b[39m0.8693   \u001b[39m | \u001b[39m174.3    \u001b[39m | \u001b[39m741.2    \u001b[39m | \u001b[39m1.495e+03\u001b[39m | \u001b[39m0.05826  \u001b[39m | \u001b[39m1.565    \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m-0.2984  \u001b[39m | \u001b[39m261.1    \u001b[39m | \u001b[39m0.7266   \u001b[39m | \u001b[39m859.6    \u001b[39m | \u001b[39m1.465e+03\u001b[39m | \u001b[39m740.9    \u001b[39m | \u001b[39m0.09406  \u001b[39m | \u001b[39m0.6294   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m-0.3419  \u001b[39m | \u001b[39m386.2    \u001b[39m | \u001b[39m0.3046   \u001b[39m | \u001b[39m713.7    \u001b[39m | \u001b[39m1.019e+03\u001b[39m | \u001b[39m973.2    \u001b[39m | \u001b[39m0.02051  \u001b[39m | \u001b[39m1.495    \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m-0.3386  \u001b[39m | \u001b[39m481.5    \u001b[39m | \u001b[39m0.7116   \u001b[39m | \u001b[39m414.3    \u001b[39m | \u001b[39m1.077e+03\u001b[39m | \u001b[39m364.2    \u001b[39m | \u001b[39m0.06485  \u001b[39m | \u001b[39m1.473    \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m-0.3341  \u001b[39m | \u001b[39m94.71    \u001b[39m | \u001b[39m0.251    \u001b[39m | \u001b[39m572.5    \u001b[39m | \u001b[39m714.0    \u001b[39m | \u001b[39m1.215e+03\u001b[39m | \u001b[39m0.06712  \u001b[39m | \u001b[39m1.618    \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m-0.2825  \u001b[39m | \u001b[39m395.2    \u001b[39m | \u001b[39m0.6286   \u001b[39m | \u001b[39m924.7    \u001b[39m | \u001b[39m57.66    \u001b[39m | \u001b[39m689.9    \u001b[39m | \u001b[39m0.04418  \u001b[39m | \u001b[39m1.634    \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m-0.3343  \u001b[39m | \u001b[39m431.6    \u001b[39m | \u001b[39m0.5334   \u001b[39m | \u001b[39m594.6    \u001b[39m | \u001b[39m450.8    \u001b[39m | \u001b[39m1.515e+03\u001b[39m | \u001b[39m0.04177  \u001b[39m | \u001b[39m1.508    \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m-0.315   \u001b[39m | \u001b[39m179.2    \u001b[39m | \u001b[39m0.6511   \u001b[39m | \u001b[39m547.7    \u001b[39m | \u001b[39m948.5    \u001b[39m | \u001b[39m1.148e+03\u001b[39m | \u001b[39m0.00229  \u001b[39m | \u001b[39m0.3943   \u001b[39m |\n",
      "=============================================================================================================\n",
      "Best Hyperparameters: {'batch_size': 201.7718989482918, 'dropout_rate': 0.8556428757689246, 'layer1_size': 1132.9188884843531, 'layer2_size': 932.382360232343, 'layer3_size': 266.6520352254245, 'learning_rate': 0.015599460473675063, 'optimizer_idx': 0.11616722433639892}\n",
      "Epoch [1/5000], Loss: 1.8697, Val Loss: 14.7125\n",
      "Epoch [2/5000], Loss: 1.5296, Val Loss: 10.3946\n",
      "Epoch [3/5000], Loss: 1.2318, Val Loss: 2.7523\n",
      "Epoch [4/5000], Loss: 1.0798, Val Loss: 1.2401\n",
      "Epoch [5/5000], Loss: 1.0454, Val Loss: 1.0235\n",
      "Epoch [6/5000], Loss: 0.9869, Val Loss: 0.9769\n",
      "Epoch [7/5000], Loss: 0.9556, Val Loss: 0.9293\n",
      "Epoch [8/5000], Loss: 0.8860, Val Loss: 0.8717\n",
      "Epoch [9/5000], Loss: 0.8656, Val Loss: 0.8135\n",
      "Epoch [10/5000], Loss: 0.8003, Val Loss: 0.7477\n",
      "Epoch [11/5000], Loss: 0.8133, Val Loss: 0.6813\n",
      "Epoch [12/5000], Loss: 0.7517, Val Loss: 0.6103\n",
      "Epoch [13/5000], Loss: 0.6546, Val Loss: 0.5501\n",
      "Epoch [14/5000], Loss: 0.6320, Val Loss: 0.4814\n",
      "Epoch [15/5000], Loss: 0.5782, Val Loss: 0.4271\n",
      "Epoch [16/5000], Loss: 0.5149, Val Loss: 0.3810\n",
      "Epoch [17/5000], Loss: 0.4905, Val Loss: 0.3480\n",
      "Epoch [18/5000], Loss: 0.4498, Val Loss: 0.3349\n",
      "Epoch [19/5000], Loss: 0.4210, Val Loss: 0.3254\n",
      "Epoch [20/5000], Loss: 0.4097, Val Loss: 0.3176\n",
      "Epoch [21/5000], Loss: 0.3823, Val Loss: 0.3057\n",
      "Epoch [22/5000], Loss: 0.3717, Val Loss: 0.2975\n",
      "Epoch [23/5000], Loss: 0.3244, Val Loss: 0.2947\n",
      "Epoch [24/5000], Loss: 0.3358, Val Loss: 0.2866\n",
      "Epoch [25/5000], Loss: 0.3029, Val Loss: 0.2771\n",
      "Epoch [26/5000], Loss: 0.2771, Val Loss: 0.2789\n",
      "Epoch [27/5000], Loss: 0.2742, Val Loss: 0.2823\n",
      "Epoch [28/5000], Loss: 0.2825, Val Loss: 0.2817\n",
      "Epoch [29/5000], Loss: 0.2468, Val Loss: 0.2839\n",
      "Epoch [30/5000], Loss: 0.2613, Val Loss: 0.2950\n",
      "Epoch [31/5000], Loss: 0.2626, Val Loss: 0.3065\n",
      "Epoch [32/5000], Loss: 0.2245, Val Loss: 0.3013\n",
      "Epoch [33/5000], Loss: 0.2419, Val Loss: 0.3037\n",
      "Epoch [34/5000], Loss: 0.2408, Val Loss: 0.3043\n",
      "Epoch [35/5000], Loss: 0.2115, Val Loss: 0.2988\n",
      "Epoch [36/5000], Loss: 0.2222, Val Loss: 0.2818\n",
      "Epoch [37/5000], Loss: 0.2104, Val Loss: 0.2749\n",
      "Epoch [38/5000], Loss: 0.2169, Val Loss: 0.2905\n",
      "Epoch [39/5000], Loss: 0.1839, Val Loss: 0.3062\n",
      "Epoch [40/5000], Loss: 0.1988, Val Loss: 0.3050\n",
      "Epoch [41/5000], Loss: 0.2122, Val Loss: 0.3005\n",
      "Epoch [42/5000], Loss: 0.1621, Val Loss: 0.2993\n",
      "Epoch [43/5000], Loss: 0.1740, Val Loss: 0.3056\n",
      "Epoch [44/5000], Loss: 0.1519, Val Loss: 0.3266\n",
      "Epoch [45/5000], Loss: 0.1756, Val Loss: 0.3329\n",
      "Epoch [46/5000], Loss: 0.1460, Val Loss: 0.3313\n",
      "Epoch [47/5000], Loss: 0.1384, Val Loss: 0.3289\n",
      "Epoch [48/5000], Loss: 0.1394, Val Loss: 0.3284\n",
      "Epoch [49/5000], Loss: 0.1332, Val Loss: 0.3325\n",
      "Epoch [50/5000], Loss: 0.1490, Val Loss: 0.3369\n",
      "Epoch [51/5000], Loss: 0.1330, Val Loss: 0.3458\n",
      "Epoch [52/5000], Loss: 0.1168, Val Loss: 0.3566\n",
      "Epoch [53/5000], Loss: 0.1438, Val Loss: 0.3593\n",
      "Epoch [54/5000], Loss: 0.1322, Val Loss: 0.3617\n",
      "Epoch [55/5000], Loss: 0.1434, Val Loss: 0.3556\n",
      "Epoch [56/5000], Loss: 0.1391, Val Loss: 0.3471\n",
      "Epoch [57/5000], Loss: 0.1261, Val Loss: 0.3351\n",
      "Epoch [58/5000], Loss: 0.1275, Val Loss: 0.3316\n",
      "Epoch [59/5000], Loss: 0.1007, Val Loss: 0.3290\n",
      "Epoch [60/5000], Loss: 0.1219, Val Loss: 0.3364\n",
      "Epoch [61/5000], Loss: 0.1127, Val Loss: 0.3487\n",
      "Epoch [62/5000], Loss: 0.0928, Val Loss: 0.3624\n",
      "Epoch [63/5000], Loss: 0.1092, Val Loss: 0.3696\n",
      "Epoch [64/5000], Loss: 0.1260, Val Loss: 0.3726\n",
      "Epoch [65/5000], Loss: 0.0787, Val Loss: 0.3797\n",
      "Epoch [66/5000], Loss: 0.0983, Val Loss: 0.3830\n",
      "Epoch [67/5000], Loss: 0.1204, Val Loss: 0.3726\n",
      "Epoch [68/5000], Loss: 0.1057, Val Loss: 0.3784\n",
      "Epoch [69/5000], Loss: 0.1076, Val Loss: 0.3854\n",
      "Epoch [70/5000], Loss: 0.1010, Val Loss: 0.3905\n",
      "Epoch [71/5000], Loss: 0.0972, Val Loss: 0.3861\n",
      "Epoch [72/5000], Loss: 0.0966, Val Loss: 0.3920\n",
      "Epoch [73/5000], Loss: 0.0870, Val Loss: 0.3976\n",
      "Epoch [74/5000], Loss: 0.0869, Val Loss: 0.4057\n",
      "Epoch [75/5000], Loss: 0.0986, Val Loss: 0.4182\n",
      "Epoch [76/5000], Loss: 0.0887, Val Loss: 0.4296\n",
      "Epoch [77/5000], Loss: 0.0807, Val Loss: 0.4405\n",
      "Epoch [78/5000], Loss: 0.1118, Val Loss: 0.4439\n",
      "Epoch [79/5000], Loss: 0.0763, Val Loss: 0.4491\n",
      "Epoch [80/5000], Loss: 0.0733, Val Loss: 0.4436\n",
      "Epoch [81/5000], Loss: 0.0764, Val Loss: 0.4373\n",
      "Epoch [82/5000], Loss: 0.1089, Val Loss: 0.4454\n",
      "Epoch [83/5000], Loss: 0.0997, Val Loss: 0.4475\n",
      "Epoch [84/5000], Loss: 0.0852, Val Loss: 0.4474\n",
      "Epoch [85/5000], Loss: 0.1161, Val Loss: 0.4499\n",
      "Epoch [86/5000], Loss: 0.0787, Val Loss: 0.4447\n",
      "Epoch [87/5000], Loss: 0.0809, Val Loss: 0.4449\n",
      "Early stopping triggered\n",
      "Test Loss: 0.2854, Test Accuracy: 0.9145\n",
      "Final optimized model saved to final_model_optimized.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\AppData\\Local\\Temp\\ipykernel_27116\\3769445838.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  final_model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Assuming X_train_pca, y_train, X_val_pca, y_val, X_test_pca, y_test are already defined as pandas DataFrames or numpy arrays\n",
    "\n",
    "# Encode labels to start from 0 using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(np.unique(np.concatenate([y_train, y_val, y_test])))\n",
    "y_train_tensor = torch.tensor(le.transform(y_train), dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(le.transform(y_val), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(le.transform(y_test), dtype=torch.long)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Convert PCA features to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_pca, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "\n",
    "# Check for NaNs/Infs in input tensors\n",
    "assert not torch.isnan(X_train_tensor).any(), \"X_train_tensor has NaNs\"\n",
    "assert not torch.isinf(X_train_tensor).any(), \"X_train_tensor has Infs\"\n",
    "assert not torch.isnan(X_val_tensor).any(), \"X_val_tensor has NaNs\"\n",
    "assert not torch.isinf(X_val_tensor).any(), \"X_val_tensor has Infs\"\n",
    "assert not torch.isnan(X_test_tensor).any(), \"X_test_tensor has NaNs\"\n",
    "assert not torch.isinf(X_test_tensor).any(), \"X_test_tensor has Infs\"\n",
    "\n",
    "# Create DataLoader for training and validation sets (batch size will be set dynamically)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the neural network architecture\n",
    "class ImprovedNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, layer1_size, layer2_size, layer3_size, dropout_rate):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, layer1_size)\n",
    "        self.bn1 = nn.BatchNorm1d(layer1_size)\n",
    "        self.fc2 = nn.Linear(layer1_size, layer2_size)\n",
    "        self.bn2 = nn.BatchNorm1d(layer2_size)\n",
    "        self.fc3 = nn.Linear(layer2_size, layer3_size)\n",
    "        self.bn3 = nn.BatchNorm1d(layer3_size)\n",
    "        self.fc4 = nn.Linear(layer3_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(layer1_size, layer2_size, layer3_size, dropout_rate, learning_rate, batch_size, optimizer_idx):\n",
    "    # Map optimizer index to actual optimizer\n",
    "    optimizers = [optim.Adam, optim.SGD, optim.RMSprop]\n",
    "    optimizer_class = optimizers[int(optimizer_idx)]\n",
    "\n",
    "    # Create DataLoader with current batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=int(batch_size), shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=int(batch_size), shuffle=False)  # drop_last not needed here\n",
    "\n",
    "    # Initialize model\n",
    "    model = ImprovedNN(input_size=X_train_pca.shape[1], num_classes=num_classes,\n",
    "                       layer1_size=int(layer1_size), layer2_size=int(layer2_size), layer3_size=int(layer3_size),\n",
    "                       dropout_rate=dropout_rate)\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    num_epochs = 5000\n",
    "    patience = 50\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            break\n",
    "\n",
    "    return -best_val_loss  # Return negative validation loss for maximization\n",
    "\n",
    "# Define the search space for Bayesian optimization\n",
    "pbounds = {\n",
    "    'layer1_size': (32, 1536),      # Number of neurons in the first layer\n",
    "    'layer2_size': (32, 1536),      # Number of neurons in the second layer\n",
    "    'layer3_size': (32, 1536),      # Number of neurons in the third layer\n",
    "    'dropout_rate': (0.0, 0.9),    # Dropout probability\n",
    "    'learning_rate': (1e-8, 1e-1), # Learning rate for the optimizer\n",
    "    'batch_size': (16, 512),       # Batch size for training\n",
    "    'optimizer_idx': (0, 2)        # 0: Adam, 1: SGD, 2: RMSprop\n",
    "}\n",
    "\n",
    "# Initialize Bayesian Optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_and_evaluate,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Run optimization with 10 trials (5 initial points + 5 optimization steps)\n",
    "optimizer.maximize(\n",
    "    init_points=10,  # Number of random initial points\n",
    "    n_iter=20,       # Number of optimization steps\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = optimizer.max['params']\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "train_loader = DataLoader(train_dataset, batch_size=int(best_params['batch_size']), shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=int(best_params['batch_size']), shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=int(best_params['batch_size']))\n",
    "\n",
    "final_model = ImprovedNN(\n",
    "    input_size=X_train_pca.shape[1],\n",
    "    num_classes=num_classes,\n",
    "    layer1_size=int(best_params['layer1_size']),\n",
    "    layer2_size=int(best_params['layer2_size']),\n",
    "    layer3_size=int(best_params['layer3_size']),\n",
    "    dropout_rate=best_params['dropout_rate']\n",
    ")\n",
    "final_model.to(device)\n",
    "\n",
    "optimizers = [optim.Adam, optim.SGD, optim.RMSprop]\n",
    "final_optimizer = optimizers[int(best_params['optimizer_idx'])](final_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "# Training loop for the final model\n",
    "num_epochs = 5000\n",
    "patience = 50\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "best_model_path = 'best_model_optimized.pth'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    final_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        final_optimizer.zero_grad()\n",
    "        outputs = final_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        final_optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation step\n",
    "    final_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = final_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(final_model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "final_model.load_state_dict(torch.load(best_model_path))\n",
    "final_model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = final_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(X_test_tensor)\n",
    "test_accuracy = test_correct / len(X_test_tensor)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Save the final optimized model\n",
    "final_model_path = 'final_model_optimized.pth'\n",
    "torch.save(final_model.state_dict(), final_model_path)\n",
    "print(f\"Final optimized model saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57cd038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using optimized hyperparameters:\n",
      "Batch Size: 202\n",
      "Dropout Rate: 0.8556428757689246\n",
      "Layer Sizes: 1133, 932, 267\n",
      "Learning Rate: 0.015599460473675063\n",
      "Optimizer: Adam\n",
      "Using device: cuda\n",
      "\n",
      "Starting training with early stopping (patience=70)...\n",
      "Maximum epochs: 5000\n",
      "Epoch [1/5000], Train Loss: 1.8352, Val Loss: 23.2995, Val Acc: 0.1779\n",
      "New best validation loss: 23.2995 (saved model)\n",
      "Epoch [2/5000], Train Loss: 1.4263, Val Loss: 8.3480, Val Acc: 0.2064\n",
      "New best validation loss: 8.3480 (saved model)\n",
      "Epoch [3/5000], Train Loss: 1.1882, Val Loss: 2.2140, Val Acc: 0.4342\n",
      "New best validation loss: 2.2140 (saved model)\n",
      "Epoch [4/5000], Train Loss: 1.1043, Val Loss: 1.2291, Val Acc: 0.6370\n",
      "New best validation loss: 1.2291 (saved model)\n",
      "Epoch [5/5000], Train Loss: 1.0198, Val Loss: 1.0092, Val Acc: 0.6512\n",
      "New best validation loss: 1.0092 (saved model)\n",
      "Epoch [6/5000], Train Loss: 0.9500, Val Loss: 0.8878, Val Acc: 0.7117\n",
      "New best validation loss: 0.8878 (saved model)\n",
      "Epoch [7/5000], Train Loss: 0.9194, Val Loss: 0.8176, Val Acc: 0.7651\n",
      "New best validation loss: 0.8176 (saved model)\n",
      "Epoch [8/5000], Train Loss: 0.8720, Val Loss: 0.7607, Val Acc: 0.7651\n",
      "New best validation loss: 0.7607 (saved model)\n",
      "Epoch [9/5000], Train Loss: 0.8537, Val Loss: 0.7095, Val Acc: 0.7829\n",
      "New best validation loss: 0.7095 (saved model)\n",
      "Epoch [10/5000], Train Loss: 0.7810, Val Loss: 0.6574, Val Acc: 0.7758\n",
      "New best validation loss: 0.6574 (saved model)\n",
      "Epoch [11/5000], Train Loss: 0.7344, Val Loss: 0.5949, Val Acc: 0.7829\n",
      "New best validation loss: 0.5949 (saved model)\n",
      "Epoch [12/5000], Train Loss: 0.7020, Val Loss: 0.5352, Val Acc: 0.7829\n",
      "New best validation loss: 0.5352 (saved model)\n",
      "Epoch [13/5000], Train Loss: 0.6080, Val Loss: 0.4763, Val Acc: 0.8221\n",
      "New best validation loss: 0.4763 (saved model)\n",
      "Epoch [14/5000], Train Loss: 0.5831, Val Loss: 0.4191, Val Acc: 0.8577\n",
      "New best validation loss: 0.4191 (saved model)\n",
      "Epoch [15/5000], Train Loss: 0.5557, Val Loss: 0.3829, Val Acc: 0.8648\n",
      "New best validation loss: 0.3829 (saved model)\n",
      "Epoch [16/5000], Train Loss: 0.4910, Val Loss: 0.3682, Val Acc: 0.8434\n",
      "New best validation loss: 0.3682 (saved model)\n",
      "Epoch [17/5000], Train Loss: 0.4769, Val Loss: 0.3526, Val Acc: 0.8505\n",
      "New best validation loss: 0.3526 (saved model)\n",
      "Epoch [18/5000], Train Loss: 0.4564, Val Loss: 0.3322, Val Acc: 0.8754\n",
      "New best validation loss: 0.3322 (saved model)\n",
      "Epoch [20/5000], Train Loss: 0.4299, Val Loss: 0.3193, Val Acc: 0.8577\n",
      "New best validation loss: 0.3193 (saved model)\n",
      "Epoch [21/5000], Train Loss: 0.3814, Val Loss: 0.2959, Val Acc: 0.8897\n",
      "New best validation loss: 0.2959 (saved model)\n",
      "Epoch [22/5000], Train Loss: 0.3283, Val Loss: 0.2950, Val Acc: 0.8683\n",
      "New best validation loss: 0.2950 (saved model)\n",
      "Epoch [24/5000], Train Loss: 0.2838, Val Loss: 0.2923, Val Acc: 0.8861\n",
      "New best validation loss: 0.2923 (saved model)\n",
      "Epoch [34/5000], Train Loss: 0.2344, Val Loss: 0.2775, Val Acc: 0.8932\n",
      "New best validation loss: 0.2775 (saved model)\n",
      "Epoch [35/5000], Train Loss: 0.1966, Val Loss: 0.2718, Val Acc: 0.8826\n",
      "New best validation loss: 0.2718 (saved model)\n",
      "Epoch [100/5000], Train Loss: 0.0785, Val Loss: 0.4684, Val Acc: 0.8968\n",
      "Early stopping triggered after 105 epochs\n",
      "\n",
      "Loading best model from best_model_optimized.pth\n",
      "\n",
      "=== Final Results ===\n",
      "Best Validation Loss: 0.2718\n",
      "Test Loss: 0.3246\n",
      "Test Accuracy: 0.8889\n",
      "Final optimized model saved to final_model_optimized.pth\n",
      "\n",
      "Model Summary:\n",
      "Total parameters: 2,183,548\n",
      "Trainable parameters: 2,183,548\n",
      "Model architecture: 768 → 1133 → 932 → 267 → 6\n",
      "Test Accuracy: 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\AppData\\Local\\Temp\\ipykernel_27116\\3770819139.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Note: Original code had NO random seeds set - using same approach for exact reproduction\n",
    "# The only seed was random_state=42 in BayesianOptimization (which we're not using here)\n",
    "\n",
    "# Assuming X_train_pca, y_train, X_val_pca, y_val, X_test_pca, y_test are already defined as pandas DataFrames or numpy arrays\n",
    "\n",
    "# Encode labels to start from 0 using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(np.unique(np.concatenate([y_train, y_val, y_test])))\n",
    "y_train_tensor = torch.tensor(le.transform(y_train), dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(le.transform(y_val), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(le.transform(y_test), dtype=torch.long)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Convert PCA features to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_pca, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "\n",
    "# Check for NaNs/Infs in input tensors\n",
    "assert not torch.isnan(X_train_tensor).any(), \"X_train_tensor has NaNs\"\n",
    "assert not torch.isinf(X_train_tensor).any(), \"X_train_tensor has Infs\"\n",
    "assert not torch.isnan(X_val_tensor).any(), \"X_val_tensor has NaNs\"\n",
    "assert not torch.isinf(X_val_tensor).any(), \"X_val_tensor has Infs\"\n",
    "assert not torch.isnan(X_test_tensor).any(), \"X_test_tensor has NaNs\"\n",
    "assert not torch.isinf(X_test_tensor).any(), \"X_test_tensor has Infs\"\n",
    "\n",
    "# Optimized hyperparameters from Bayesian optimization\n",
    "BATCH_SIZE = 202  # Rounded from 201.7718989482918\n",
    "DROPOUT_RATE = 0.8556428757689246\n",
    "LAYER1_SIZE = 1133  # Rounded from 1132.9188884843531\n",
    "LAYER2_SIZE = 932   # Rounded from 932.382360232343\n",
    "LAYER3_SIZE = 267   # Rounded from 266.6520352254245\n",
    "LEARNING_RATE = 0.015599460473675063\n",
    "OPTIMIZER_IDX = 0   # Rounded from 0.11616722433639892 (Adam optimizer)\n",
    "\n",
    "print(\"Using optimized hyperparameters:\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Dropout Rate: {DROPOUT_RATE}\")\n",
    "print(f\"Layer Sizes: {LAYER1_SIZE}, {LAYER2_SIZE}, {LAYER3_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "\n",
    "# Create DataLoaders with optimized batch size\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the neural network architecture\n",
    "class ImprovedNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, layer1_size, layer2_size, layer3_size, dropout_rate):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, layer1_size)\n",
    "        self.bn1 = nn.BatchNorm1d(layer1_size)\n",
    "        self.fc2 = nn.Linear(layer1_size, layer2_size)\n",
    "        self.bn2 = nn.BatchNorm1d(layer2_size)\n",
    "        self.fc3 = nn.Linear(layer2_size, layer3_size)\n",
    "        self.bn3 = nn.BatchNorm1d(layer3_size)\n",
    "        self.fc4 = nn.Linear(layer3_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model with optimized hyperparameters\n",
    "model = ImprovedNN(\n",
    "    input_size=X_train_pca.shape[1],\n",
    "    num_classes=num_classes,\n",
    "    layer1_size=LAYER1_SIZE,\n",
    "    layer2_size=LAYER2_SIZE,\n",
    "    layer3_size=LAYER3_SIZE,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 5000\n",
    "patience = 70\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "best_model_path = 'best_model_optimized.pth'\n",
    "\n",
    "print(f\"\\nStarting training with early stopping (patience={patience})...\")\n",
    "print(f\"Maximum epochs: {num_epochs}\")\n",
    "\n",
    "# Training loop with early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = val_correct / len(val_loader.dataset)\n",
    "    \n",
    "    # Print progress every 100 epochs or when validation improves\n",
    "    if (epoch + 1) % 100 == 0 or val_loss < best_val_loss:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'New best validation loss: {val_loss:.4f} (saved model)')\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "# Load the best model and evaluate on test set\n",
    "print(f\"\\nLoading best model from {best_model_path}\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "# Test evaluation\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Store predictions and labels for further analysis if needed\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_loss /= len(X_test_tensor)\n",
    "test_accuracy = test_correct / len(X_test_tensor)\n",
    "\n",
    "print(f'\\n=== Final Results ===')\n",
    "print(f'Best Validation Loss: {best_val_loss:.4f}')\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Save the final optimized model\n",
    "final_model_path = 'final_model_optimized.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final optimized model saved to {final_model_path}\")\n",
    "\n",
    "# Optional: Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model architecture: {X_train_pca.shape[1]} → {LAYER1_SIZE} → {LAYER2_SIZE} → {LAYER3_SIZE} → {num_classes}\")\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
